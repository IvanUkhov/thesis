Dynamic steady-state analysis addresses the shortcomings of static steady-state
analysis in one particular but important context. It tackles the scenario where
the power consumption follows a periodic pattern. In this case, after a
sufficiently long time, the system will not reach a static steady state but
instead a dynamic steady state: temperature will starts to exhibit a periodic
patter following the periodic patter of power. Then the goal of the analysis is
to find the periodic temperature profile \mq, called the dynamic steady-state
temperature profile (\up{DSSTP}), that corresponds to a given periodic power
profile \mp.

In the case of applications that exhibit periodic or close to periodic behavior,
the \up{DSSP} is of particular importance. Any design optimization has to be
performed such that the efficiency and reliability of the system at hand are
maximized considering not a relatively short transient time interval at the
system's start but the context in which the system is to function over a long
period of time, which is exactly the system's dynamic steady state.

A typical design task, for which the \up{DSSTP} is of central importance, is
temperature-aware reliability optimization. The impact of temperature on the
lifetime of electronic systems is well known \cite{srinivasan2004, coskun2006,
jedec2010, xiang2010}. The failure mechanisms commonly considered are
electromigration, time-dependent dielectric breakdown, and thermal cycling,
which are directly driven by temperature \cite{jedec2010}. What is important in
this context is that not only the average and maximum temperature have a huge
impact on the lifetime of the chip but also the amplitude and frequency of
temperature oscillations. Thus, efficient reliability optimization depends on
the availability of the actual \up{DSSTP}.

\subsection{Prior Work}
\slab{dynamic-steady-state-prior}

Let us elaborate on the techniques that have been applied in the literature in
order to compute the \up{DSSTP} as a prerequisite for reliability optimization.

A straightforward approximation of the \up{DSSTP} can be obtained by running a
temperature simulator over one or more successive periods of the application
until it is assumed that a sufficiently accurate approximation of the dynamic
steady state has been attained \cite{srinivasan2004}. The typical simulator of
choice is HotSpot \cite{skadron2003}, which is the state-of-the-art for
system-level temperature analysis \cite{srinivasan2004, liao2005, coskun2006,
liu2007, huang2009, xiang2010, thiele2011}. The simulator performs transient
temperature analysis by solving \eref{temperature-model-original} numerically
via the Runge--Kutta fourth-order method \cite{press2007}.

\inputfigure{dynamic-steady-state-prior}
The number of iterations required to reach the \up{DSSTP} depends on the thermal
characteristics of the platform. In order to illustrate this aspect, consider an
application with a period of 0.5~s running on five hypothetical platforms with
die areas of 1--25~mm\textsuperscript{2}. Let us simulate 50 successive periods
of the application via HotSpot with its default settings and compare the
resulting approximations in each period with the actual \up{DSSTP} using the
normalized root-mean-square error (\up{NRMSE}). The comparison is shown in
\fref{iterative-transient-error}. It can be observed that the number of
successive periods over which transient analysis has to be performed in order to
achieve a satisfactory level of accuracy is significant for the majority of the
configurations, which entails large computation times. For instance, for a
9-mm\textsuperscript{2} die, even after 15 iterations, the \up{NRMSE} is still
close to 20\%. Using the analytical approach to transient analysis presented in
\sref{transient-analysis}, the calculation can be sped up; however, the large
number of iterations still keeps the computation cost unreasonably high as we
shall illustrate in \sref{dynamic-steady-state-results}. Moreover, this
approximation technique provides no guarantees on the resulting accuracy since
there is no a reliable metric for measuring the proximity to the actual
solution, that is, to the actual \up{DSSTP}.

Another crude but fast approximation of the \up{DSSTP} is proposed in
\cite{huang2009}. It forgoes transient analysis all together and resides to
static steady-state analysis instead. To elaborate, it is assumed that, in each
time interval wherein the power consumption is constant, the system
instantaneously reaches a static steady state. The result of this procedure is a
stepwise temperature curve where each step corresponds to the steady-state
temperature that would be reached if the corresponding power was applied for a
sufficiently long time.

\inputfigure{static-steady-state-example}
An example of such an approximation along with the corresponding \up{DSSTP} for
an application with 10 tasks and a period of 0.1~s is given in
\fref{static-steady-state-example}. The die area is 25~mm\textsuperscript{2} in
this case. The reduced accuracy of this technique is due to the mismatch between
the actual temperature within each interval and the static steady-state
temperature. The inaccuracy depends on the thermal characteristics of the
platform and on the application itself. In order to illustrate this concern, let
us simulate five applications with periods of 0.01--1~s running on five
platforms with die areas of 1--25~mm\textsuperscript{2} and assess the resulting
profiles. The errors relative to the actual \up{DSSTP}s are shown in
\fref{static-steady-state-error}. It can be seen that, for example, for a die
area of 10~mm\textsuperscript{2} and an application period of 100~ms, the
\up{NRMSE} of this approximation technique is close to 40\%.

To conclude, the state-of-the-art techniques for dynamic steady-state
temperature analysis are slow and inaccurate. This state of affairs makes them
difficult and dangerous to be utilized for the purpose of design optimization.

\subsection{Our Solution}
\slab{dynamic-steady-state-present}

In this subsection, we are to formalize the problem of dynamic steady-state
temperature analysis and to develop an exact and, moreover, computationally
efficient solution to this problem, eliminating the issues of the
state-of-the-art solutions discussed in the previous subsection.

Consider the temperature model in \eref{temperature-model} and the corresponding
recurrence in \eref{recurrence}. The key characteristic of a dynamic steady
state is as follows:
\begin{equation} \elab{boundary-condition}
  \vs_0 = \vs_\ns.
\end{equation}
In words, the above condition means that, once the steady state has been
reached, the system starts to arrive at its initial state at the end of each
iteration, which is what makes it periodic. Then, using \eref{recurrence}, the
\up{DSSTP} can be computed by solving the following system of linear equations:
\[
  \begin{cases}
    \vs_1 - \m{E} \vs_\ns & = \m{F}_1 \vp_1 \\
    \dots \\
    \vs_\ns - \m{E}_\ns \vs_{\ns - 1} & = \m{F}_\ns \vp_\ns
  \end{cases}
\]
where the first equation enforces the boundary condition in
\eref{boundary-condition}. In order to get the big picture, the system can be
rewritten as follows:
\begin{equation} \elab{steady-state-system}
  \resizebox{0.89\linewidth}{!}{
    $\underbrace{\left[
      \begin{array}{rrrrrrr}
        \m{I}  &  0     & 0      & \cdots & 0      & 0      & -\m{E} \\
        -\m{E} & \m{I}  & 0      & \cdots & 0      & 0      & 0      \\
        0      & -\m{E} & \m{I}  & \cdots & 0      & 0      & 0      \\
        \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
        0      & 0      & 0      & \cdots & \m{I}  & 0      & 0      \\
        0      & 0      & 0      & \cdots & -\m{E} & \m{I}  & 0      \\
        0      & 0      & 0      & \cdots & 0      & -\m{E} & \m{I}
      \end{array}
    \right]}_{\displaystyle \mathbb{A}} \underbrace{\left[
      \begin{array}{l}
        \vs_1         \\
        \vs_2         \\
        \vs_3         \\
        \cdots        \\
        \vs_{\ns - 2} \\
        \vs_{\ns - 1} \\
        \vs_\ns
      \end{array}
    \right]}_{\displaystyle \mathbb{X}} = \underbrace{\left[
      \begin{array}{l}
        \m{F} \vp_1         \\
        \m{F} \vp_2         \\
        \m{F} \vp_3         \\
        \cdots              \\
        \m{F} \vp_{\ns - 2} \\
        \m{F} \vp_{\ns - 1} \\
        \m{F} \vp_\ns
      \end{array}
    \right]}_{\displaystyle \mathbb{B}}$
  }
\end{equation}
where $\mathbb{A}$ is an $\nn \ns \times \nn \ns$ matrix, and $\mathbb{X}$ and
$\mathbb{B}$ are $\nn \ns$-element vectors.

The most direct way to solve the system in \eref{steady-state-system} is to use
a dense solver such as the \up{LU} decomposition \cite{press2007}. However,
since $\mathbb{A}$ is a sparse matrix, a more appropriate approach is to employ
a sparse solver such as the unsymmetric multifrontal method \cite{davis2004}.
The computational complexity of such solutions is proportional to $\ns^3 \nn^3$
\cite{press2007} where $\nn$ is the number of nodes in the thermal \up{RC}
circuit, and $\ns$ is the number of samples in the power profile. The problem is
that the system to solve can be extremely large, in particular due to $\ns$. In
such cases, direct solvers are prohibitively slow and require a large amount of
memory, and, therefore, we do not discuss them any further.

The overall matrix of the system in \eref{steady-state-system} is, in fact, a
block Toeplitz matrix. To be more specific, the matrix is a block-circulant
matrix where each block row vector is rotated one block element to the right
relative to the preceding block row vector. This leads to a wide range of
possible techniques to solve the system, e.g., the fast Fourier transform (FFT)
\cite{mazancourt1983} that we include in our experiments in
\sref{dynamic-steady-state-results}.

Another possible technique is iterative methods for solving systems of linear
equations (e.g., Jacobi, Gauss--Seidel, Successive Overrelaxation)
\cite{press2007}. These methods are designed to overcome problems of direct
solvers and, consequently, they are applicable for very large systems. However,
the most important issue with these methods is their convergence. In our
experiments we did not observe any advantages of using these methods compared to
the others considered in this paper. Therefore, they are excluded from the
discussion.

In this section we propose a fast approach to solve the system in
\eref{steady-state-system}. The approach consists of an auxiliary transformation
(\sref{substitution}) and the actual solution
(\sref{dynamic-steady-state-present}).

The major problem with straight-forward techniques is that (1) the sparseness of
the matrix is not taken into account and/or (2) its specific structure is
totally ignored, resulting in inefficiency and inaccuracy of the computations.
Using direct dense and sparse solvers, for example, requires a computation time
proportional to $N_n^3 N_s^3$ \cite{press2007}. Our proposed technique considers
both features and delivers solutions in time proportional to $N_s N_n^3$ while
operating only on a few $N_n \times N_n$ matrices. It is important that the
dependency on $N_s$ (the number of steps in the power profile), which is by far
dominating ($N_s \gg N_n$), is linear.

Observing the structure of the matrix in \eref{steady-state-system}, non-zero
elements are located only on the block diagonal, on one subdiagonal just above
the block diagonal, and on one subdiagonal in the left bottom corner. The block
diagonal is composed of $N_n \times N_n$ matrices while all elements of the
subdiagonals are equal to $-1$. Linear systems with the same structure arise in
boundary value problems for ODEs where a technique to solve them is to form a
so-called condensed equation (CE), or condensed system \cite{stoer2002}.

In the recurrence given by \eref{recurrence} we denote $\m{Q}_i = \tm{B}_i \:
\v{P}_i$:
\begin{align}
  & \tv{T}_{i + 1} = \tm{K}_i \: \tv{T}_i + \m{Q}_i, \; i = \range{0}{N_s - 1} \elab{ce-recurrent} \\
  & \tv{T}_0 = \tv{T}_{N_s} \nonumber
\end{align}
Performing the iterative repetition of \eref{ce-recurrent} leads to:
\begin{equation} \elab{y-recurrent}
  \tv{T}_i = \prod_{j = 0}^{i - 1} \tm{K}_j \: \tv{T}_0 + \m{W}_{i - 1}, \; i = \range{1}{N_s}
\end{equation}
where $\m{W}_i$ are defined as follows:
\begin{equation}
  \m{W}_0 = \m{Q}_0 \hspace{15pt} \m{W}_i = \tm{K}_i \: \m{W}_{i - 1} + \m{Q}_i, \; i = \range{1}{N_s - 1} \elab{p-recurrent}
\end{equation}
We calculate the final vector $\tilde{\v{T}}_{N_s}$ using \eref{y-recurrent} and
\eref{p-recurrent}:
\[
  \tilde{\v{T}}_{N_s} = \prod_{j = 0}^{N_s - 1} \tilde{\m{K}}_j \: \tilde{\v{T}}_0 + \m{W}_{N_s - 1}
\]
Taking into account the boundary condition given by \eref{boundary-condition},
we obtain the following system of linear equations:
\begin{equation} \elab{core-system}
  (\m{I} - \prod_{j = 0}^{N_s - 1} \tm{K}_j) \: \tv{T}_0 = \m{W}_{N_s - 1}
\end{equation}
We recall that $\tm{K}_i$ is the matrix exponential given by
\eref{matrix-exponential}; therefore, the following simplification holds:
\[
  \prod_{j = i}^l \tm{K}_j = \prod_{j = i}^l e^{\tm{A} \Delta t_j} = e^{\tm{A} \sum_{j = i}^l \Delta t_j} = \m{U} e^{\left( \sum_{j = i}^l \Delta t_j \: \m{\Lambda} \right)} \m{U}^T
\]
Consequently:
\[
  \prod_{j = 0}^{N_s - 1} \tm{K}_j = \m{U} \: \diagonal{e^{\tau \lambda_0}}{e^{\tau \lambda_{N_n - 1}}} \: \m{U}^T
\]
where $\tau$ is the application period. Substituting this product into
\eref{core-system}, we obtain the following system:
\[
  (\m{I} - \m{U} \: e^{\tau \m{\Lambda}} \: \m{U}^T) \: \tv{T}_0 = \m{W}_{N_s - 1}
\]
The identity matrix $\m{I}$ can be split into $\m{U} \m{U}^T$, hence:
\begin{equation} \elab{t0}
  \tv{T}_0 = \m{U} \: (\m{I} - e^{\tau \m{\Lambda}})^{-1} \: \m{U}^T \: \m{W}_{N_s - 1} = \m{Z} \: \m{W}_{N_s - 1}
\end{equation}
where:
\[
  \m{Z} = \m{U} \: \diagonal{\frac{1}{1 - e^{\tau \lambda_0}}}{\frac{1}{1 - e^{\tau \lambda_{N_n - 1}}}} \: \m{U}^T
\]
The equation gives the initial solution vector $\tv{T}_0$; the rest of vectors
$\tv{T}_i$ for $i = \range{1}{N_s - 1}$ are successively found from
\eref{ce-recurrent}.

Since the power profile is evenly sampled with the sampling interval $\Delta t$,
the recurrence in \eref{ce-recurrent} turns into:
\[
  \tv{T}_{i+1} = \tm{K} \: \tv{T}_i + \m{Q}_i = \tm{K} \: \tv{T}_i + \tm{B} \: \v{P}_i
\]
where $\tm{K} = e^{\tm{A} \: \Delta t}$ and $\tm{B} = \tm{A}^{-1} ( e^{\tm{A} \:
\Delta t} - \m{I} ) \m{C}^{-\frac{1}{2}}$. Here $\tm{K}$ and $\tm{B}$ are
constants, since they depend only on the matrices $\tm{A}$, $\m{C}$, and
sampling interval $\Delta t$, which is fixed. In this case, the block diagonal
of the matrix $\tilde{\mathbb{A}}$, similar to \eref{steady-state-system}, is
composed of the same repeating block $\tm{K}$ and the recurrent expressions take
the following form:
\begin{align}
  & \m{W}_i = \tm{K} \: \m{W}_{i - 1} + \m{Q}_i, \; i = \range{1}{N_s - 1} \elab{final-p-recurrence} \\
  & \tv{T}_{i + 1} = \tm{K} \: \tv{T}_i + \m{Q}_i, \; i = \range{0}{N_s - 1} \elab{final-y-recurrence}
\end{align}
where $\m{Q}_i = \tm{B} \: \v{P}_i$, $\m{W}_0 = \m{Q}_0$, and $\tv{T}_0$ is
given by \eref{t0}.

The last step of the solution is to return to temperature by performing the
backward substitution opposite to \eref{substitution}:
\[
  \v{T}_i = \m{C}^{-\frac{1}{2}} \: \tv{T}_i, \: i = \range{0}{N_s - 1}
\]

As we see, the auxiliary substitution from \sref{substitution} allows us to
perform the single-time eigenvalue decomposition with orthogonal eigenvectors
that later eases the computational process at several stages. In
\sref{dynamic-steady-state-present} it can be observed that the solution of the
system in \eref{steady-state-system} has been reduced to two successive
recurrences in \eref{final-p-recurrence} and \eref{final-y-recurrence} over
$N_s$ steps in the power profile, which implies a linear complexity on $N_s$
mentioned earlier.

It should be noted that the eigenvalue decomposition along with matrices
$\tm{K}$ and $\tm{B}$ are computed only once for a particular RC thermal circuit
and can be considered as given together with the RC circuit. It has not to be
recalculated when a SSDTP is generated, which significantly decreases the
computation time.

\subsection{Leakage Power}

So far, we have assumed that power is independent of temperature. However, due
to the leakage component, the power dissipation is a strong function of
temperature that cannot be neglected (\sref{power-model}). Two techniques can be
applied to include in our proposed solution temperature-dependent leakage
modeling.

\subsubsection{Iterative Computation}
\slab{iterative-leakage}

In this case, we have an iterative process, depicted in \fref{leakage}, where
the temperature and power profiles are calculated in turns. With each new
temperature profile we update the power profile by computing the leakage power
and adding it to the dynamic power: $\mathbb{P}_i = \mathbb{P}_\text{dyn} +
\mathbb{P}_\text{leak}(\mathbb{T}_i)$. The process continues until the
temperature converges, i.e., the difference between two successive temperature
profiles is below a predefined bound. In our experiments we used $0.5^\circ C$
as the maximal acceptable difference and observed that the number of required
iterations to converge is 4--7.

\subsubsection{Linear Approximation}
\slab{linearized-leakage}

A linear approximation of the leakage power has the following matrix form:
$\v{P}_\text{leak}(\v{T}) = \m{A} \: \v{T}(t) + \v{B}$ where $\m{A}$ is a $N_n
\times N_n$ diagonal matrix of the proportionality and $\v{B}$ is a vector with
$N_n$ elements of the intercept. Both characterize the leakage power for each of
the $N_n$ thermal nodes in the system. It can be seen that the approximation
keeps \eref{temperature-model-original} untouched: $\m{C} \:
\frac{d\v{T}(t)}{dt} + \bar{\m{G}} \: (\v{T}(t) - \v{T}_\ambient) = \bar{\v{P}}$
where $\bar{\m{G}} = \m{G} - \m{A}$ and $\bar{\v{P}} = \v{P}_\text{dyn} + \m{A}
\: \v{T}_\ambient + \v{B}$. Therefore, all solutions proposed in this paper are
perfectly valid with the linearized model. Moreover, in spite of its simplicity,
the model provides a good estimation, as shown in \cite{liu2007}.

In order to evaluate the linearization, we have constructed a number of
hypothetical platforms with 2--32 cores (other parameters are given in
\tref{parameters}) and compared temperature profiles obtained with the
linearization and the exponential model (\sref{power-model}), respectively. For
the later, we use the iterative approach described in \sref{iterative-leakage}.
For the linearization, the power curve fitting with the least squares regression
\cite{press2007} has been employed, targeted at the range between 40 and
$80^\circ C$. From the experiments we have observed that the NRMSE is bounded by
1--2\%, indicating a good accuracy of the linear approximation.
