In this section, we illustrate the importance of temperature analysis for
design-space exploration. More specifically, we consider reliability
optimization and demonstrate the utility of our solution to dynamic steady-state
analysis presented in \sref{dynamic-steady-state-analysis} for mitigating the
fatigue due to thermal cycling \cite{jedec2010}, which is one of the most common
failure mechanism. To this end, we develop a thermal-cycling-aware technique for
scheduling of periodic applications. (In this thesis, mapping is assumed to be a
part of scheduling. Therefore, for each task of the application in question, a
schedule prescribes not only the starting time but also the processor where the
task is supposed to be executed.)

We proceed as follows. In \sref{thermal-cycling-motivation}, the impact of
design decisions on the lifetime of a multiprocessor system is exemplified. The
system and reliability models that we consider are presented in
\sref{system-model} and \sref{reliability-model}, respectively. In
\sref{thermal-cycling-optimization}, we formulate our optimization objective and
describe the optimization technique that we utilize in order to attain this
objective. Lastly, the experimental results are reported in
\sref{thermal-cycling-results}.

\subsection{Motivational Example}
\slab{thermal-cycling-motivation}

\inputfigure{thermal-cycling-task-graph}
\inputfigure{thermal-cycling-motivation}
Consider a periodic application with six tasks denoted T1--T6 and a
heterogeneous platform with two processors denoted P1 and P2. The task graph of
the application is given in \fref{thermal-cycling-task-graph} along with the
execution times for both processors. The period of the application is 60~ms. The
first alternative schedule and the corresponding \up{DSSTP} are shown on the
left-hand side of \fref{thermal-cycling-motivation} where the height of a task
represents its power consumption. It can be seen that P1 is experiencing three
thermal cycles where a thermal cycle is roughly a time interval wherein
temperature starts from a certain value, reaches an extremum, and comes back. If
we move T6 to P2, the number of cycles decreases to two, which can be seen in
the middle of \fref{thermal-cycling-motivation}. If we also swap T2 and T4, the
number of cycles of P1 drops to one, which is depicted on the right-hand side of
\fref{thermal-cycling-motivation}. According to the reliability model that we
shall describe shortly, these two changes improve the lifetime of the system by
around 45\% and 55\%, respectively, relative to the initial schedule.

The example shows that, when exploring the design space, it is important to
take into account the number of temperature fluctuations and their
characteristics. In order to obtain this information, the \up{DSSTP} has to be
computed.

\subsection{System Model}
\slab{system-model}

Consider a heterogeneous multiprocessor platform with \np processors that is to
execute a periodic application with \nt tasks. Processor $i$ is characterized by
a triple $(\numberof{g}_i, f_i, V_i)$ where $\numberof{g}_i$, $f_i$, and $V_i$
are the number of gates \cite{liao2005}, frequency, and supply voltage of the
processor. The application is given as a directed acyclic graph whose vertices
and edges correspond to the tasks and to data dependencies between the tasks,
respectively; see \fref{thermal-cycling-task-graph} for an example. Denote by
$\period$ the period of the application, which is assumed to be equal to the
application's deadline. Furthermore, each pair of a task and a processor is
characterized by a tuple $(\numberof{c}_{ij}, C_{ij})$ where $\numberof{c}_{ij}$
is the number of clock cycles that task $i$ requires for its completion when it
is executed on processor $j$, and $C_{ij}$ is the corresponding effective
switched capacitance. Lastly, the application is assumed to be scheduled using a
list scheduler whose inputs are priorities of the tasks and a mapping of the
tasks onto the processors.

The above information allows one to computer the power consumption of the tasks
according to the power model presented in \sref{power-model}, which will then be
used to evaluate the temperature model presented in \sref{temperature-model}.

\subsection{Reliability Model}
\slab{reliability-model}
\newcommand{\mean}{\mu}
\newcommand{\scale}{\eta}
\newcommand{\shape}{\beta}

The reliability model that we utilize is the one presented in \cite{huang2009,
xiang2010}. The model relies heavily on Weibull distributions, which we overview
in brief now.

A Weibull distribution has two parameters: $\scale$ and $\shape$, which are
called the scale and shape parameters, respectively. Let $T$ be a random
variable that is distributed according to such a distribution, which is denoted
by $T \sim \mathrm{Weibull}(\scale, \shape)$. Then the distribution function
\cite{durrett2010} of $T$ is
\begin{equation} \elab{weibull-distribution}
  F(t) = 1 - \exp\left(-\left(\frac{t}{\scale}\right)^\shape\right);
\end{equation}
the complementary distribution function of $T$ is
\begin{equation} \elab{weibull-reliability}
  R(t) = 1 - F(t) = \exp\left(-\left(\frac{t}{\scale}\right)^\shape\right);
\end{equation}
and the expectation of $T$ is
\begin{equation} \elab{weibull-expectation}
  \mean = \expectation{T} = \scale \: \Gamma\left(1 + \frac{1}{\shape}\right)
\end{equation}
where $\Gamma$ is the gamma function. In this context of reliability analysis,
$T$ represents the time to failure of the component under consideration; $F$
gives the probability of failure before a certain time moment; $R$ gives the
probability of survival up to a certain time moment, and it is called the
reliability function; and $\mean$ corresponds to the mean time to failure
(\up{MTTF}).

It is natural to expect that the distribution of $T$ is different for different
usage conditions, which is not prominent in \eref{weibull-distribution}. In
order to take this into account, the application period $\period$ is split into
\ns time intervals $\{ \dt_i: i = \range{1}{\ns} \}$ so that the conditions that
are relevant to the model stay unchanged within each interval. Let $T_i \sim
\mathrm{Weibull}(\scale_i, \shape_i)$ be the time to failure that the component
would have if interval $i$ was the only interval present and denote by $\mean_i$
the corresponding \up{MTTF} according to \eref{weibull-expectation}. In the case
of temperature-induced failures, we have that $\shape_i = \shape$ for $i =
\range{1}{\ns}$, that is, all the shape parameters are equal. The reason is
that, unlike the scale parameters, the shape parameters are independent of the
operating temperature \cite{chang2006}.

As it is shown in \cite{xiang2010}, in the above scenario, the reliability
function of the component can still be approximated by means of
\eref{weibull-reliability} by letting
\[
  \scale = \frac{\sum_{i = 1}^\ns \dt_i}{\sum_{i = 1}^\ns \frac{\Delta t_i}{\scale_i}}.
\]
Applying \eref{weibull-expectation} at the level of individual cycles, the
parameter is rewritten as
\[
  \scale = \frac{\sum_{i = 1}^\ns \dt_i}{\Gamma\left(1 + \frac{1}{\shape}\right) \sum_{i = 1}^\ns \frac{\Delta t_i}{\mean_i}}.
\]
Note that the reliability model in \eref{weibull-reliability} becomes fully
specified as soon as $\dt_i$ and $\mean_i$ are identified for $i =
\range{1}{\ns}$. This part of the model depends on the particular failure
mechanism considered. In the rest of this subsection, we shall tailor the model
to the thermal-cycling fatigue, which is of interest to us due to its prominent
dependence on temperature oscillations.

In the case of thermal cycling, the time intervals with constant relevant
conditions correspond to thermal cycles. In order to detect them in a given
temperature curve, we utilize the rainflow counting method \cite{xiang2010}. As
a result, a set of \ns thermal cycles is obtained. Each detected cycle is
characterized by a number of properties including its duration, which is the
desired $\dt_i$. Regarding the corresponding $\mean_i$, it can be expressed as
follows:
\[
  \mean_i = \nc_i \dt_i
\]
where $\nc_i$ stands for the mean number of such cycles to failure. This number
is estimated using a modified version of the Coffin--Manson equation with the
Arrhenius term as shown in the following equation \cite{xiang2010, jedec2010}:
\begin{equation} \elab{reliability-mean-cycles}
  \nc = a (\Delta \q - \Delta \q_0)^{-b} \exp\left(\frac{c}{k \q_\maximum}\right)
\end{equation}
where $a$, $b$ (called the Coffin--Manson exponent), and $c$ (called the
activation energy) are empirically determined constants; $k$ is the Boltzmann
constant; $\Delta \q$ is the excursion of the cycle in question; $\Delta \q_0$
is the portion of the temperature excursion that resides in the elastic region,
which does not cause damage; and $\q_\maximum$ is the maximum temperature during
the cycle.

The reliability model of the abstract component under consideration is now fully
specified. The reliability function is the one in \eref{weibull-reliability}
with
\begin{equation} \elab{reliability-scale}
  \scale = \frac{\sum_{i = 1}^\ns \dt_i}{\Gamma\left(1 + \frac{1}{\shape}\right) \sum_{i = 1}^\ns \frac{1}{\nc_i}}
\end{equation}
where $\nc_i$ is as in \eref{reliability-mean-cycles}. Using
\eref{weibull-expectation}, the \up{MTTF} of the
component is then
\begin{equation} \elab{reliability-mean-time}
  \mean = \frac{\sum_{i = 1}^\ns \dt_i}{\sum_{i = 1}^\ns \frac{1}{\nc_i}}.
\end{equation}

In conclusion, it should be noted that the reliability model requires detailed
information about the thermal cycles that the component is exposed to, which can
be obtained by performing dynamic steady-state temperature analysis.

\subsection{Optimization Procedure}
\slab{thermal-cycling-optimization}

Recall that we have a platform with \np processors and a periodic application
with \nt tasks and a period $\period$ (see \sref{system-model}), and that, in
this context, \mq stands for the \up{DSSTP} of the system. The time to failure
of each processor is modeled using the reliability model presented in
\sref{reliability-model}. With a slight abuse of notation, let $\mean_i$ be the
\up{MTTF} of processor $i$ computed according to \eref{reliability-mean-time}.
Our optimization objective is to find a schedule that maximizes
\begin{equation} \elab{thermal-cycling-objective}
  \min_{i = 1}^\np \mean_i
\end{equation}
such that
\begin{align} \elab{thermal-cycling-constraints}
  \begin{split}
    & \max_{i = 1}^\nt t_{\ns_i} \leq \period \text{ and} \\
    & \norm{\mq} \leq \q_\maximum
  \end{split}
\end{align}
where $t_{\ns_i}$ is the completion time of task $i$, and $\norm{\cdot}$ denotes
the uniform norm. The first constraint enforces a deadline on the schedule, and
the second constraint enforces a maximum temperature $\q_\maximum$ on the
resulting \mq.

The optimization is undertaken using a genetic algorithm \cite{schmitz2004}. In
this paradigm inspired by biology, a population of chromosomes, which represent
candidate solutions, is carried through a number of generations in order to
produce a chromosome with the best possible fitness, that is, a solution that
maximizes the objective function. The fitness function in this case is
\eref{thermal-cycling-objective}.

Each chromosome contains $2 \times \nt$ genes. The first \nt genes encode
priorities of the tasks, and the second \nt genes encode a mapping of the tasks
onto the processors; the usage of this information is to be discussed shortly.
The population contains $4 \times \nt$ chromosomes. In each generation, a number
of chromosomes are chosen for breeding by the tournament selection with the
number of competitors proportional to the population size. The chosen
chromosomes undergo the two-point crossover with probability 0.8 and the uniform
mutation with probability 0.01. The evolution mechanism follows the elitism
model where the best chromosome always survives. The stopping condition is the
absence of improvement within 200 successive generations.

The fitness of a chromosome is evaluated in a number of steps. First, the
chromosome is decoded, and the priorities of the tasks and their mapping onto
the processors are fed to the scheduler; see \sref{system-model}. The scheduler
produces a schedule. If the schedule violates the deadline constraint given in
\eref{thermal-cycling-constraints}, the chromosome is penalized proportionally
to the amount of violation and is not further processed. Otherwise, based on the
parameters of the processors and tasks discussed in \sref{system-model}, a power
profile \mp is constructed, and the corresponding \up{DSSTP} \mq is computed
using our technique presented in \sref{dynamic-steady-state-present}. If the
\up{DSSTP} violates the temperature constraint given in
\eref{thermal-cycling-constraints}, the chromosome is penalized proportionally
to the amount of violation and is not further processed. Otherwise, the
\up{MTTF} of each processor is estimated according to
\eref{reliability-mean-time}, and the fitness function given in
\eref{thermal-cycling-objective} is computed.

\subsection{Experimental Results}
\slab{thermal-cycling-results}

In this subsection, we proceed to the optimization itself. We shall first
consider a set of synthetic applications and then study a real-life one. The
general experimental setup is the same as the one described in
\sref{dynamic-steady-state-results}. All the configuration files used in the
experiments are available online at \cite{eslab2011}.

Heterogeneous platforms and periodic applications are generated randomly using
the \up{TGFF} tool \cite{dick1998} in such a way that the execution times of
tasks are uniformly distributed between 1 and 10~ms, and that the static power
accounts for 30--60\% of the total power consumption. The modeling of the
leakage power is based on the linear approximation discussed in
\sref{power-temperature-interplay}. The area of a single processor is set to
4~mm\textsuperscript{2}. The maximum-temperature constraint $\q_\maximum$ in
\eref{thermal-cycling-constraints} is set to \celsius{100}. In
\eref{reliability-mean-cycles}, the Coffin--Manson exponent $b$ is set to 6, the
activation energy $c$ to 0.5, and the elastic region $\Delta \q_0$ to 0
\cite{jedec2010}; the coefficient of proportionality $a$ in
\eref{reliability-mean-cycles} is irrelevant since we are concerned with
relative improvements, which will be explained shortly.

The initial population of chromosomes is initialized partially randomly and
partially based on a temperature-aware heuristic proposed in \cite{xie2006},
which we shall refer to as the initial solution. The heuristic relies on spatial
temperature distributions and tries to minimize the maximum temperature while
satisfying real-time constraints. This solution is also used to decide on the
deadline constraint $\period$ in \eref{thermal-cycling-constraints}: it is set
to the duration of the initial schedule extended by 5\%. Furthermore, the
initial solution serves as a baseline for evaluating the performance of the
solutions delivered by our optimization.

\inputtable{thermal-cycling-processors}
In the first set of experiments, we change the number of processors \np while
keeping the number of tasks \nt per processor constant and equal to 20. For each
problem, we generate 20 random task graphs and compute the average change in the
\up{MTTF} relative to the initial solution. In addition, we calculate the
average change in the energy consumption. The obtained results are reported in
\tref{thermal-cycling-processors}, which also shows the average time taken by
the optimization procedure. It can be seen that our reliability-aware
optimization increases the \up{MTTF} by a factor of 13--40. Even for large
problems---such as those with 16 processors and 320 tasks---a feasible schedule
that significantly improves the lifetime of the system can be found in an
affordable time. Moreover, as shown in \tref{thermal-cycling-processors}, the
optimization does not impact much the energy efficiency.

\inputtable{thermal-cycling-tasks}
In the second set of experiments, we keep the quad-core platform and vary the
number of tasks \nt of the application. As before, for each problem, we generate
20 random task graphs and monitor the changes in the \up{MTTF} and energy
consumption relative to the initial solution. The results can be seen in
\tref{thermal-cycling-tasks}. The observations are similar to those made with
respect to \tref{thermal-cycling-processors}.

\inputtable{thermal-cycling-methods}
The experiments show that our optimization is able to effectively increase the
\up{MTTF} of the system at hand. The efficiency is due to the fast and accurate
approach to dynamic steady-state temperature analysis presented in
\sref{dynamic-steady-state-present}, which is at the heart of the optimization
procedure. Due to its speed, the technique allows a large portion of the design
space to be explored. In order to illustrate this, let us replace, inside the
optimization framework, our method with \one~the one based on iterative
transient analysis with HotSpot and \two~the one based on static steady-state
analysis; see \sref{dynamic-steady-state-prior}. The goal is to compare the
results in \tref{thermal-cycling-tasks} with the results produced by the two
alternative methods when they are given the same amount of time as the one taken
by our method. For each problem, the optimized \up{MTTF} produced by either of
the two approximate methods is re-evaluated by our exact method. The results are
summarized in \tref{thermal-cycling-methods} where \up{MTTF}\textsubscript{i}
and \up{MTTF}\textsubscript{ii} stand for the two alternative methods,
respectively. It can be seen that, for example, the lifetime of the platform
running 160 tasks can be extended by a factor of 18 using our technique whereas
the best solutions found by the other two techniques within the same time frame
are only around 2--5 times better than the initial solution. The reason for the
poor performance of transient analysis with HotSpot is the excessively long
execution time of the \up{DSSTP} calculation. This method allows for a very
shallow investigation of the design space. In the case of static steady-state
analysis, the reason is different: the method is fast but very inaccurate as
discussed and illustrated in \sref{dynamic-steady-state-prior}. The inaccuracy
drives the optimization towards solutions that turn out to be of low quality.

\inputfigure{thermal-cycling-pareto}
The experiments show that the impact of our optimization on the energy
consumption is insignificant. This is not surprising: the optimization searches
toward low-temperature solutions, which are also implicitly the low-leakage
ones. In order to explore this further, let us perform a multi-objective
optimization along the dimensions of energy and reliability, and let us use
\up{NSGA-II} \cite{deb2002}. The resulting Pareto front averaged over 20
applications with 80 tasks deployed on a quad-core platform is displayed in
\fref{thermal-cycling-pareto}. It can be observed that the variation of the
energy change is less than 2\%. This means that the solutions optimized for the
\up{MTTF} have an energy consumption almost identical to those optimized for
energy. At the same time, the difference along the \up{MTTF} is huge. This means
that ignoring the reliability aspect one may end up with a significantly
decreased \up{MTTF} without any significant gain in energy.

Finally, we have applied our optimization technique to a real-life example,
namely the MPEG2 video decoder \cite{ffmpeg2011} that is deployed onto a
dual-core platform. The decoder was analyzed and split into 34 tasks. The
parameters of each task were obtained through a system-level simulation using
MPARM \cite{benini2005}. The deadline is set to 40 $ms$ assuming 25 video frames
per second. The solution found with the proposed method improves the lifetime of
the system by 23.59 times with a 5\% energy saving, compared to the initial
solution. The same optimization was solved using HotSpot and the SSA. The best
found solutions are only 5.37 and 11.50 times better than the initial one,
respectively.

Experiments demonstrate the superiority of the proposed techniques, compared to
the state of the art.

