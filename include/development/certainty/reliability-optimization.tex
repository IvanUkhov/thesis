In this section, we illustrate the importance of temperature analysis in the
context of reliability optimization. More specifically, we demonstrate the
utility of our solution to dynamic steady-state analysis presented in
\sref{dynamic-steady-state-analysis} for mitigating the fatigue due to thermal
cycling \cite{jedec2010}, which is one of the most common failure mechanism. To
this end, we develop a thermal-cycling-aware technique for scheduling of
periodic applications. (In this thesis, mapping is assumed to be a part of
scheduling. Therefore, for each task of the application in question, a schedule
prescribes not only the starting time but also the processor where the task is
supposed to be executed.)

We proceed as follows. In \sref{thermal-cycling-motivation}, the impact of
design decisions on the damage caused by thermal cycling is exemplified. The
system and reliability models that we consider are presented in
\sref{system-model} and \sref{reliability-model}, respectively. The experimental
results are reported in \sref{thermal-cycling-result}.

\subsection{Motivational Example}
\slab{thermal-cycling-motivation}

\inputfigure{thermal-cycling-task-graph}
\inputfigure{thermal-cycling-motivation}
Consider a periodic application with six tasks denoted T1--T6 and a
heterogeneous platform with two processors denoted P1 and P2. The task graph of
the application is given in \fref{thermal-cycling-task-graph} along with the
execution times for both processors. The period of the application is 60~ms. The
first alternative schedule and the corresponding \up{DSSTP} are shown on the
left-hand side of \fref{thermal-cycling-motivation} where the height of a task
represents its power consumption. It can be seen that P1 is experiencing three
thermal cycles. If we move T6 to P2, the number of cycles decreases to two,
which can be seen in the middle of \fref{thermal-cycling-motivation}. If we also
swap T2 and T4, the number of cycles of P1 drops to one, which is depicted on
the right-hand side of \fref{thermal-cycling-motivation}. According to the
reliability model that we shall describe shortly, these two changes improve the
lifetime of the system by around 45\% and 55\%, respectively, relative to the
initial schedule.

The example shows that, when exploring the design space, it is important to
take into account the number of temperature fluctuations and their
characteristics. In order to obtain this information, the \up{DSSTP} has to be
computed.

\subsection{System Model}
\slab{system-model}

Consider a heterogeneous multiprocessor platform with \np processors. Processor
$i$ is characterized by a triple $(\numberof{g}_i, f_i, V_i)$ where
$\numberof{g}_i$, $f_i$, and $V_i$ are the number of gates \cite{liao2005},
frequency, and supply voltage of the processor. Consider also a periodic
application with \nt tasks. The application is given as a directed acyclic graph
$(V, E)$ where $V$ is a set of vertices corresponding to the tasks, and $E$ is a
set of edges representing data dependencies between the tasks. Let $\tau$ be the
period of the application, which is assumed to be equal to the deadline. Each
pair of a task and a processor is characterized by a tuple $(\numberof{c}_{ij},
C_{ij})$ where $\numberof{c}_{ij}$ is the number of clock cycles that task $i$
requires when it is executed on processor $j$, and $C_{ij}$ is the corresponding
effective switched capacitance.

The above information allows one to computer the power consumption of the tasks
according to the power model presented in \sref{power-model}, which will then be
used to evaluate the temperature model presented in \sref{temperature-model}.

\subsection{Reliability Model}
\slab{reliability-model}

The reliability model that we utilize is the one presented in \cite{huang2009,
xiang2010}. The model relies heavily on Weibull distributions, which we shall
overview in brief now.

A Weibull distribution has two parameters: $\eta$ and $\beta$, which are called
the scale and shape parameters, respectively. Let $T$ be a random variable
that is distributed according to such a distribution, which is denoted by $T
\sim \mathrm{Weibull}(\eta, \beta)$. Then the distribution function
\cite{durrett2010} of $T$ is
\begin{equation} \elab{weibull-distribution}
  F_T(t) = 1 - \exp\left\{-\left(\frac{t}{\eta}\right)^\beta\right\};
\end{equation}
the complementary distribution function of $T$ is
\begin{equation} \elab{weibull-survival}
  R_T(t) := 1 - F_T(t) = \exp\left\{-\left(\frac{t}{\eta}\right)^\beta\right\};
\end{equation}
and the expectation of $T$ is
\begin{equation} \elab{weibull-expectation}
  \expectation{T} = \eta \: \Gamma\left(1 + \frac{1}{\beta}\right)
\end{equation}
where $\Gamma$ is the gamma function. In this context of reliability analysis,
$T$ represents the time to failure of the system under consideration; $F_T$
gives the probability of failure before a certain time moment; $R_T$ gives the
probability of survival up to a certain time moment, and it is called the
reliability function; and $\expectation{T}$ corresponds to the mean time to
failure (\up{MTTF}).

It is natural to expect that the distribution of $T$ changes over time since the
thermal conditions change, which is not the case in \eref{weibull-distribution}
yet. In order to take this into account, the period of the application $\tau$ is
split into \nc time intervals $\{ \dt_i: i = \range{1}{\nc} \}$ so that the
conditions that are captured by the model stay constant within each interval.
Let $T_i \sim \mathrm{Weibull}(\eta_i, \beta_i)$ be the time to failure that the
system would have if interval $i$ was the only one causing damage, and denote by
$\theta_i = \expectation{T_i}$ the corresponding \up{MTTF}. Using
\eref{weibull-expectation}, we have
\begin{equation} \elab{weibull-eta-one}
  \eta_i = \frac{\theta_i}{\Gamma\left(1 + \frac{1}{\beta_i}\right)}
\end{equation}
for $i = \range{1}{\nc}$. The exact form of $\{ (\theta_i, \beta_i): i =
\range{1}{\nc} \}$ depends on the particular failure mechanism modeled. In our
case, since we focus on temperature-induced failures, $\beta_i = \beta$ for $i =
\range{1}{\nc}$, that is, these parameters are all equal. The reason is that,
unlike the scale parameter, the shape parameter is independent of the operating
temperature \cite{chang2006}.

As it is shown in \cite{xiang2010}, in the above scenario, the reliability
function can be approximated by means of the following formula:
\[
  R_T(t) = \exp\left\{-\left( \frac{\sum_{i = 1}^\nc \frac{\dt_i}{\eta_i}}{\sum_{i = 1}^\nc \dt_i} t \right)^\beta\right\},
\]
which keeps the general form of the reliability function of Weibull
distributions in \eref{weibull-survival} with the scale parameter equal to
\begin{equation} \elab{weibull-eta-all}
  \eta = \frac{\sum_{i = 1}^\nc \dt_i}{\sum_{i = 1}^\nc \frac{\Delta t_i}{\eta_i}}.
\end{equation}
The \up{MTTF} with respect to all the intervals, which we denote by $\theta$,
can be obtained by combining \eref{weibull-expectation}, \eref{weibull-eta-one},
and \eref{weibull-eta-all}, and it is as follows:
\[
  \theta = \frac{\sum_{i = 1}^\nc \dt_i}{\sum_{i = 1}^\nc \frac{\Delta t_i}{\theta_i}}.
\]

Since we focus on thermal cycling---which is of interest to us due to its
prominent dependence on temperature oscillations---we shall tailor these
parameters accordingly shortly.

As mentioned previously, in order to compute the MTTF, we need to consider the
particular failure mechanism and determine the values $\theta_i$ needed in
\eref{weibull-eta-one}. We focus on the thermal cycling fatigue
(\sref{reliability-model}). Assuming this concrete failure model, the duration
$\Delta t_i$, during which the corresponding scaling parameter $\eta_i$ is
constant \eref{weibull-eta-one}, is exactly a thermal cycle.

When the system is exposed to identical thermal cycles, the number of such
cycles to failure can be estimated using a modified version of the well-known
Coffin-Manson equation with the Arrhenius term \cite{xiang2010, jedec2010}:
\[
  N_c = A (\Delta T - \Delta T_0)^{-b} e^{\frac{E_a}{k T_\text{max}}}
\]
where $A$ is an empirically determined constant, $\Delta T$ is the thermal cycle
excursion, $\Delta T_0$ is the portion of the temperature range in the elastic
region which does not cause damage, $b$ is the Coffin-Manson exponent, which is
also empirically determined, $E_{a}$ is the activation energy, $k$ is the
Boltzmann constant, and $T_\text{max}$ is the maximal temperature during the
thermal cycle. Over the application period, the system undergoes a number of
different thermal cycles each with its own duration $\Delta t_i$ and each cycle
causes its own damage. Therefore, having $N_m$ thermal cycles characterized by
the number of cycles to failure $N_{c\:i}$ and duration $\Delta t_i$, we can
compute $\theta_i$:
\begin{equation} \elab{mttf-cycle}
  \theta_i = N_{c \: i} \; \Delta t_i
\end{equation}
Taking equations \eref{weibull-expectation}, \eref{weibull-eta-one},
\eref{weibull-eta-all}, and \eref{mttf-cycle} together, we obtain the following
expression to estimate the MTTF of one component in the system:
\begin{align}
  \theta = \frac{\tau}{\sum_{i=0}^{N_m - 1} \frac{1}{N_{c \: i}}}
\end{align}
In order to identify thermal cycles in the temperature curve, we follow the
approach given in \cite{xiang2010} where the rainflow counting method is
employed.

Assuming the TC fatigue, the parameters affecting reliability are the amplitude
and number of thermal cycles as well as the maximal temperature. A thermal cycle
is a time interval in which the temperature starts from a certain value and,
after reaching an extremum, returns back.

The mean time to failure (MTTF) of one processing element in the system can be
estimated as the following:
\begin{align} \elab{one-mttf}
  \theta = \frac{\tau}{\sum_{i=0}^{N_m - 1} \frac{1}{N_{c \: i}}}
\end{align}
where $N_m$ is the number of thermal cycles during the application period
$\tau$. $N_{c \: i}$ characterizes the $i$th thermal cycle and is calculated
according to the following expression:
\begin{equation} \elab{cycles-to-failure}
  N_c = A (\Delta T - \Delta T_0)^{-b} e^{\frac{E_a}{k T_\text{max}}}
\end{equation}
where $\Delta T$ is the thermal cycle excursion (the distance between the
minimal and maximal temperatures) and $T_\text{max}$ is the maximal temperature
during the thermal cycle (more details in \sref{reliability-optimization}).

It can be seen that the computation requires the identification of the thermal
cycles with their amplitudes and maximal temperatures. All these are captured by
the SSDTP, which is needed as an input to the reliability optimization.

\subsection{Problem Formulation}

The problem formulation is the following:

Given:
\begin{itemize}

\item A multiprocessor system $\Pi$ (\sref{system-model}).

\item A periodic application $G$ (\sref{system-model}).

\item The floorplan of the chip at the desired level of details, configuration
of the thermal package, and thermal parameters.

\item The parameters of the reliability model (\sref{reliability-model}), i.e.,
the constants $A$, $\Delta T_0$, $b$, $E_a$ (see \elab{cycles-to-failure}).

\end{itemize}

Maximize:
\begin{equation} \elab{fitness-function}
  \mathcal{F} = \min_{i = 0}^{N_p - 1} \theta_i
\end{equation}
such that
\begin{align}
  & t_{\text{end} \: i} \leq \tau, \: \forall i \elab{deadline} \\
  & T_{ij} \leq T_\text{max}, \: \forall i, j \elab{t-max}
\end{align}
where $\theta_i$ is the MTTF of the $i$th processing element given by
\eref{one-mttf}, $t_{\text{end} \: i}$ denotes the end time of the $i$th task,
$\tau$ is the period of the application, and $T_{ij}$ are temperature values in
the SSDTP. \eref{deadline} imposes the application deadline, which we assume to
be equal to the period. \eref{t-max} enforces the constraint on the maximal
temperature in the temperature profile $\mathbb{T} = \{ T_{ij} \}$.

The optimization procedure is based on a genetic algorithm (GA)
\cite{schmitz2004} with the fitness function $\mathcal{F}$ given by
\eref{fitness-function}. The algorithm is outlined in \sref{genetic-algorithm}.

This section contains the derivation of the reliability model discussed in
\sref{reliability-optimization} and the description of the actual optimization
procedure.

\subsection{Optimization Procedure}
\slab{genetic-algorithm}

The optimization procedure is based on a genetic algorithm \cite{schmitz2004}
with the fitness function $\mathcal{F}$ given by \eref{fitness-function}. Each
chromosome is a vector of $2 \times N_t$ elements, where the first half encodes
priorities of the tasks and the second represents a mapping. The population
contains $4 \times N_t$ individuals that are initialized partially randomly and
partially based on the initial temperature-aware solution \cite{xie2006}. In
each generation, a number of individuals, called parents, are chosen for
breeding by the tournament selection with the number of competitors proportional
to the population size. The parents undergo the 2-point crossover with $0.8$
probability and uniform mutation with $0.01$ probability. The evolution
mechanism follows the elitism model where the best individual always survives.
The stopping condition is an absence of improvement within 200 successive
generations.

The fitness of a chromosome, \eref{fitness-function}, is evaluated in a number
of steps. First, the decoded priorities and mapping are given to a list
scheduler that produces schedules for each of the cores. If the application
schedule does not satisfy the deadline, the solution is penalized proportionally
to the delay and is not further evaluated; otherwise, based on the parameters of
the architecture and tasks, a power profile is obtained and the corresponding
SSDTP is computed by our proposed method. If the SSDTP violates the temperature
constraint given by \eref{t-max}, the solution is penalized proportionally to
the amount of violation and not further processed; otherwise, the MTTF of each
core is estimated according to \eref{one-mttf} and the fitness function
$\mathcal{F}$ is computed.

\subsection{Experimental Results}
\slab{thermal-cycling-result}

In this section we evaluate the reliability optimization approach described in
\sref{reliability-optimization}, first with a set of synthetic applications and,
finally, using a real-life example.

The experimental setup is the following. Heterogeneous platforms and periodic
applications are generated randomly \cite{dick1998} in such a way that the
execution time of tasks is uniformly distributed between 1 and 10 $ms$ and the
leakage power accounts for 30--60\% of the total power dissipation\footnote{The
parameters of the applications and platforms (task graphs, floorplans, HotSpot
configurations, etc.) used in our experiments are available online at
\cite{liu2011}.}. The linear leakage model is used in the experiments, since, as
discussed in \sref{power-temperature-interdependence}, it provides a good
approximation. The area of one core is 4 $mm^2$, other parameters of the die and
thermal package are given in \tref{parameters}. The temperature constraint
$T_\text{max}$ (see \eref{t-max}) is set to $100^\circ C$. In
\eref{cycles-to-failure} the Coffin-Manson exponent $b$ is set to 6, the
activation energy $E_a$ to 0.5, and the elastic temperature region $\Delta T_0$
to zero \cite{jedec2010}. The coefficient of proportionality $A$ is not
significant, since we are concerned about the relative improvement.

In each of the experiments, we compare the optimized solution with an initial
temperature-aware solution proposed in \cite{xie2006}. This solution consists of
a task mapping and schedule that captures the spatial temperature behavior and
tries to minimize the peak temperature while satisfying the real-time
constraints. The deadline is set to the duration of the initial schedule
extended by 5\%.

In the first set of experiments, we change the number of cores $N_p$ while
keeping the number of tasks $N_t$ per core constant and equal to 20. For each
problem we have generated 20 random task graphs and found the average
improvement of the MTTF over the initial solution ($\scriptstyle
\text{MTTF}_\times$). We also have measured the change in the consumed energy
($\scriptstyle \text{E}_\times$). The results are given in \tref{mttf-cores}
($t$ indicates the optimization time in seconds). It can be seen that the
reliability-aware optimization dramatically increases the MTTF by 13 up to 40
times. Even for large applications with, e.g., 320 tasks deployed onto 16 cores,
a feasible mapping and schedule that significantly improve the lifetime of the
system can be found in an affordable time. Moreover, our optimization does not
impact the energy efficiency of the system.

For the second set of experiments, we keep the quad-core architecture and vary
the size (number of tasks $N_t$) of the application. The number of randomly
generated task graphs per application size is 20. The average improvement of the
MTTF along with the change in the energy consumption are given in
\tref{mttf-tasks}. The observations are similar to those for the previous set of
experiments.

The above experiments have confirmed that our proposed approach is able to
effectively increase the MTTF of the system. The efficiency of this approach is
due to the fast and accurate SSDTP calculation, which is at the heart of the
optimization, and which, due to its speed, allows a huge portion of the design
space to be explored. In order to prove this, we have replaced, inside our
optimization framework, the proposed SSDTP calculation with the calculation
based on HotSpot and based on the SSA, respectively
(\sref{dynamic-steady-state-prior}). The goal is to compare our results with the
results produced using HotSpot and the SSA, after the same optimization time as
needed with the proposed SSDTP calculation technique. The experimental setup is
the same as for the experiments in \tref{mttf-tasks}. The MTTF obtained with
HotSpot and the SSA is evaluated and compared with the MTTF obtained by our
proposed method. The results are summarized in \tref{mttf-comparison}. For
example, the lifetime of the platform running 160 tasks can be extended by more
than 18 times, compared to the initial solution, using our approach, whereas,
the best solutions found with HotSpot and the SSA, using the same optimization
time, are only 2.02 and 5.33 times better, respectively. The reason for the poor
results with HotSpot is the excessively long execution time of the SSDTP
calculation. This allows for a much less thorough investigation of the solution
space than with our proposed technique. In the case of the SSA, the reason is
different. The SSA is fast but also very inaccurate
(\sref{dynamic-steady-state-prior}). The inaccuracy drives the optimization
towards solutions that turn out to be of low quality.

We have seen that our reliability-targeted optimizations have significantly
increased the MTTF without affecting the energy consumption. This is not
surprising, since our optimization will search towards low temperature
solutions, which implicitly means low leakage. In order to further explore this
aspect, we have performed a multi-objective optimization\footnote{The
multi-objective optimization is based on NSGA-II \cite{deb2002}.} along the
dimensions of energy and reliability. An example of the Pareto front averaged
over 20 applications with 80 tasks deployed onto a quad-core platform is given
in \fref{average-pareto}. It can be observed that the variation of energy
is less than 2\%. This means that solutions optimized for the MTTF have an
energy consumption almost identical to those optimized for energy. At the same
time, the difference along the MTTF is huge. This means that ignoring the
reliability aspect one may end up with a significantly decreased MTTF, without
any significant gain in energy.

Finally, we have applied our optimization technique to a real-life example,
namely the MPEG2 video decoder \cite{ffmpeg2011} that is deployed onto a
dual-core platform. The decoder was analyzed and split into 34 tasks. The
parameters of each task were obtained through a system-level simulation using
MPARM \cite{benini2005}. The deadline is set to 40 $ms$ assuming 25 video frames
per second. The solution found with the proposed method improves the lifetime of
the system by 23.59 times with a 5\% energy saving, compared to the initial
solution. The same optimization was solved using HotSpot and the SSA. The best
found solutions are only 5.37 and 11.50 times better than the initial one,
respectively.

Experiments demonstrate the superiority of the proposed techniques, compared to
the state of the art.

