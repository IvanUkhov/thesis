The key building block of our solutions developed in the subsequent section is
the uncertainty quantification technique presented in this section. The main
task of this technique is the propagation of uncertainty through the system,
that is, from a set of inputs to a set of outputs. Specifically, the inputs are
the uncertain parameters \vu, and the outputs are the quantities that we are
interested in studying. The latter can be, for instance, the energy consumption,
maximum temperature, or temperature profile of the system over a certain period
of time.

Due to the inherent complexity, uncertainty quantification problems are
typically viewed as approximation problems: one first constructs a
computationally efficient surrogate for the stochastic model under consideration
and then studies this computationally efficient representation instead of the
original model. In order to construct such an approximation, we appeal to
spectral methods \cite{maitre2010, janson1997, eldred2008}.

\subsection{Uncertainty Model}

Before we proceed to the construction of light surrogate models, let us first
refine our definition of $\vu = (\u_i)_{i = 1}^\nu$. Each $\u_i$ is a
characteristic of a single transistor (consider, for instance, the effective
channel length), and, therefore, each device in the electrical circuits at hand
can potentially have a different value of this parameter as, in general, the
variability due to process variation is not uniform. Consequently, each $\u_i$
can be viewed as a random process $\u_i: \Omega \times D \to \real$ defined on
an appropriate spatial domain $D \subset \real^2$. Since this work is
system-level oriented, we model each processing element with one variable for
each such random process. More specifically, we let $\u_{ij} = \u_i(\cdot,
\v{r}_j)$ be the random variable representing the $i$th uncertain parameter at
the $j$th processing element where $\v{r}_j$ stands for the spatial location of
the center of the processing element. Therefore, we redefine the
parameterization \vu of the problem at hand as
\[
  \vu = (\u_i)_{i = 1}^{\nu \np}
\]
such that there is a one-to-one correspondence between $\u_i$, $i = 1, 2, \dots,
\nu \np$, and $\u_{ij}$, $i = 1, 2, \dots, \nu$, $j = 1, 2, \dots, \np$. For
instance, in our illustrative application with two process parameters, the total
number of stochastic dimensions is $2 \np$.

\begin{remark}
Some authors prefer to split the variability of a process parameter at a spatial
location into several parts such as wafer-to-wafer, die-to-die, and within-die;
see, for instance, \cite{juan2012}. However, from the mathematical point of
view, it is sufficient to consider just one random variable per location which
is adequately correlated with the other locations of interest.
\end{remark}

A description of \vu is an input to our analysis given by the designer, and we
consider it to be a part of the system specification \spec. A proper (complete,
unambiguous) way to describe a set of random variables is to specify their joint
probability distribution function. In practice, however, such exhaustive
information is often unavailable, in particular, due to the high dimensionality
in the presence of prominent dependencies inherent to the considered problem. A
more realistic assumption is the knowledge of the marginal distributions and
correlation matrix of \vu. Denote by $\{ F_{\u_i} \}_{i = 1}^{\nu \np}$ and
$\correlation{\vu} \in \real^{\nu \np \times \nu \np}$ the marginal distribution
functions and correlation matrix of the uncertain parameters \vu in
\eref{uncertain-parameters}, respectively. Note that the number of distinct
marginals is only \nu since \np components of \vu correspond to the same
uncertain parameter.

\subsection{Parameter Preprocessing}

Our foremost task now is to transform \vu into mutually independent random
variables as independence is essential for the forthcoming mathematical
treatment and practical computations. To this end, an adequate probability
transformation should be undertaken depending on the available information; see
\cite{eldred2008} for an overview. One transformation for which the assumed
knowledge about \vu is sufficient is the Nataf transformation \cite{li2008}.
Denote this transformation by
\[
  \vu = \transform{\vz},
\]
which relates $\nu \np$ dependent random variables, that is, \vu, with $\nz =
\nu \np$ independent random variables
\[
  \vz = (\z_i)_{i = 1}^\nz.
\]
Regardless of the marginals, $\z_i \sim \mathrm{Gaussian}{0, 1}$, $i = 1, 2,
\dots, \nz$, that is, each $\z_i$ has the standard Gaussian distribution. Refer
to \sref{probability-transformation} for further details about the Nataf
transformation.

As we shall discuss later on, the stochastic dimensionality \nz has a
considerable impact on the computational complexity of our framework. Therefore,
an important part of the preprocessing stage is model order reduction. To this
end, we preserve only those stochastic dimensions whose contribution to the
total variance of \vu is the most significant, which is identified by the
eigenvalues of the correlation matrix $\correlation{\vu}$:
\[
  \v{\lambda} = (\lambda_i)_{i = 1}^{\nu \np}, \hspace{1em}
  \norm[1]{\v{\lambda}} = 1,
\]
as it is further discussed in \sref{model-order-reduction}. Without introducing
additional transformations, we let $\transform$ in
\eref{probability-transformation} be augmented with such a reduction procedure
and redefine $\vz \in \real^\nz$ as the reduced independent random variables
where $\nz \leq \nu \np$. We would like to note that this procedure is highly
preferable as it helps to keep \nz moderate, and it is especially advantages
when refining the granularity of the analysis (see \sref{problem-formulation}).

Let us turn to the illustrative application. Recall that we exemplify our
framework considering the effective channel length and gate-oxide thickness with
the notation given in \eref{application-uncertain-parameters}. Both parameters
correspond to Euclidean distances; they take values on bounded intervals of the
positive part of the real line. With this in mind, we model the two process
parameters using the four-parametric family of beta distributions:
\[
  \u_i \sim F_{\u_i} = \mathrm{Beta}(a_i, b_i, c_i, d_i)
\]
where $i = 1, 2, \dots, 2 \np$, $a_i$ and $b_i$ control the shape of the
distributions, and $[c_i, d_i]$ correspond to their supports. Without loss of
generality, we let the two considered process parameters be independent of each
other, and the correlations among those elements of \vu that correspond to the
same process parameter be given by the following correlation function:
\[
  k(\v{r}_i, \v{r}_j) = w \; k_\SE(\v{r}_i, \v{r}_j) + (1 - w) k_\OU(\v{r}_i, \v{r}_j)
\]
where $\v{r}_i \in \real^2$ is the center of the $i$th processing element
relative to the center of the die. The correlation function is a composition of
two kernels:
\begin{align*}
  & k_\SE(\v{r}_i, \v{r}_j) = \exp\left(- \frac{\norm{\v{r}_i - \v{r}_j}^2}{\ell_\SE^2} \right) \text{ and} \\
  & k_\OU(\v{r}_i, \v{r}_j) = \exp\left(- \frac{\absolute{\norm{\v{r}_i} - \norm{\v{r}_j}}}{\ell_\OU} \right),
\end{align*}
which are known as the squared-exponential and Ornstein--Uhlenbeck kernels,
respectively. In the above formulae, $w \in [0, 1]$ is a weight coefficient
balancing the kernels; $\ell_\SE$ and $\ell_\OU > 0$ are so-called length-scale
parameters; and $\norm{\cdot}$ stands for the Euclidean norm in $\real^2$. The
choice of these two kernels is guided by the observations of the correlation
patterns induced by the fabrication process: $k_\SE$ imposes similarities
between those spatial locations that are close to each other, and $k_\OU$
imposes similarities between those locations that are at the same distance from
the center of the die; see, for instance, \cite{friedberg2005} for additional
details. The length-scale parameters $\ell_\SE$ and $\ell_\OU$ control the
extend of these similarities, that is the range wherein the influence of one
point on another is significant.

\subsection{Surrogate Construction}

Let $\g: \Omega \to \real$ be a quantity of interest dependent on \vu. For
convenience, \g is assumed to be one-dimensional, which will be generalized
later on. In order to give a computationally efficient probabilistic
characterization of \g, we utilize nonintrusive spectral decompositions based on
orthogonal polynomials. The corresponding mathematical foundation is outlined in
\sref{spectral-decomposition} and \sref{numerical-integration}, and here we go
directly to the main results obtained in those sections.

\subsubsection{Classical Decomposition}

Assume $\g \in \L{2}(\Omega, \mathcal{F}, \probability)$ (see
\sref{preliminaries}). Then \g can be expanded into the following series:
\[
  \g \approx \chaos{\nz}{\lc}{\g} := \sum_{\multiindex \in
  \multiindices{\nz}{\lc}} \hat{\g}_{\multiindex} \, \phi_{\multiindex}(\vz)
\]
where \lc is the expansion level; $\multiindex = (\alpha_i) \in \natural^\nz$ is
a multi-index; $\multiindices{\nz}{\lc}$ is an index set to be discussed
shortly; and $\phi_{\multiindex}(\vz)$ is an \nz-variate Hermite polynomial
constructed as a product of normalized one-dimensional Hermite polynomials of
orders specified by the corresponding elements of \multiindex.

As discussed in \sref{spectral-decomposition}, each coefficient
$\hat{\g}_{\multiindex}$ in \eref{spectral-decomposition} is an
$\nz$-dimensional integral of the product of \g with $\phi_{\multiindex}$, and
this integral should be computed numerically. To this end, we construct a
quadrature rule and calculate $\hat{\g}_{\multiindex}$ as
\begin{equation} \elab{numerical-integration}
  \hat{\g}_{\multiindex} \approx \quadrature{\nz}{\lq}{\g \,
  \phi_{\multiindex}} := \sum_{i = 1}^\nq \g(\transform{\v{x}_i}) \, \phi_{\multiindex}(\v{x}_i) \, w_i
\end{equation}
where $\lq$ is the quadrature level, and $\{ (\v{x}_i \in \real^\nz, w_i \in
\real) \}_{i = 1}^\nq$ are the points and weights of the quadrature. The
multivariate quadrature operator $\quadrature{\nz}{\lq}$ is based on a set of
univariate operators and is constructed as follows:
\begin{equation} \elab{smolyak-sparse-grid}
  \quadrature{\nz}{\lq} = \bigoplus_{\multiindex \in \multiindices{\nz}{\lq}}
  \Delta_{\alpha_1} \otimes \cdots \otimes \Delta_{\alpha_\nz}.
\end{equation}
The notation used in the above equation is not essential for the present
discussion and is explained in \sref{numerical-integration}. The important
aspect to note is the structure of this operator, namely, the index set
$\multiindices{\nz}{\lq}$, which we shall come back to shortly.

The standard choice of $\multiindices{\nz}{\lc}$ in
\eref{spectral-decomposition} is $\{ \multiindex: \norm[1]{\multiindex} \leq \lc
\}$, which is called an isotropic total-order index set. \emph{Isotropic} refers
to the fact that all dimensions are trimmed identically, and \emph{total-order}
refers to the structure of the corresponding polynomial space. In
\eref{numerical-integration}, $\phi_{\multiindex}$ is a polynomial of total
order at most \lc, and \g is modeled as such a polynomial. Hence, the integrand
in \eref{numerical-integration} is a polynomial of total order at most $2 \lc$.
Having this aspect in mind, one usually constructs a quadrature rule such that
it is exact for polynomials of total order $2 \lc$ \cite{eldred2008}. In this
work, we employ Gaussian quadratures for integration, in which case a quadrature
of level $\lq$ is exact for integrating polynomials of total order $2 \lq + 1$
\cite{heiss2008} (see also \sref{numerical-integration}). Therefore, it is
sufficient to keep \lc and \lq equal. More generally, the index sets
$\multiindices{\nz}{\lc}$ and $\multiindices{\nz}{\lq}$ should be synchronized;
in what follows, we shall denote both by $\multiindices{\nz}{l}$.

Since we are interested in integration with respect to the standard Gaussian
measure over $\real^\nz$, we shall rely on the Gauss--Hermite family of
quadrature rules \cite{maitre2010}, which is a subset of a broader family known
as Gaussian quadratures.

For the Gauss--Hermite quadrature rules in one dimension, we have that $\nq =
\lq + 1$ and the precision is $2 \nq - 1$ \cite{heiss2008} or, equivalently, $2
\lq + 1$, which is a remarkable property of Gaussian quadratures. The resulting
sparse grid is exact for polynomials with a total order up to $2 \lq + 1$, which
is analogous to integration in one dimension.

\subsubsection{Anisotropic Decomposition}

In the context of sparse grids, an important generalization of the construction
in \eref{smolyak-sparse-grid} is the so-called anisotropic Smolyak algorithm
\cite{nobile2008}. The main difference between the isotropic and anisotropic
versions lies in the constraints imposed on $\multiindices{\nz}{l}$. An
anisotropic total-order index set is defined as follows:
\begin{equation} \elab{anisotropic-total-order-index-set}
  \multiindices{\nz}{l} = \left\{ \multiindex: \innerproduct{\v{c}}{\multiindex} \leq l \, \min_i c_i \right\}
\end{equation}
where $\v{c} = (c_i) \in \real^\nz$, $c_i \geq 0$, is a vector assigning
importance coefficients to each dimension, and $\innerproduct{\cdot}{\cdot}$ is
the standard inner product on $\real^\nz$. Equation
\eref{anisotropic-total-order-index-set} plugged into \eref{smolyak-sparse-grid}
results in a sparse grid which is exact for the polynomial space that is
tailored using the same index set.

The above approach allows one to leverage the highly anisotropic behaviors
inherent for many practical problems \cite{nobile2008}. It provides a great
control over the computational time associated with the construction of spectral
decompositions: a carefully chosen importance vector $\v{c}$ in
\eref{anisotropic-total-order-index-set} can significantly reduce the number of
polynomial terms in \eref{spectral-decomposition} and the number of quadrature
points needed in \eref{numerical-integration} to compute the coefficients of
those terms. The question to discuss now is the choice of $\v{c}$. In this
regard, we rely on the variance contributions of the dimensions given by
$\v{\lambda}$ in \eref{dimension-contribution}. Specifically, we let
\begin{equation} \elab{dimension-anisotropy}
  \v{c} = \v{\lambda}^\gamma := (\lambda_i^\gamma)_{i = 1}^\nz
\end{equation}
where $\gamma \in [0, 1]$ is a tuning parameter. The isotropic scenario can be
recovered by setting $\gamma = 0$; the other values of $\gamma$ correspond to
various levels of anisotropy with the maximum attained by setting $\gamma = 1$.

Let us sum up what we have achieved at this point. In order to give a
probabilistic characterization of a quantity of interest, we perform polynomial
expansions as shown in \eref{spectral-decomposition}. The coefficients of such
expansions are evaluated by means of Gaussian quadratures as shown in
\eref{numerical-integration}. The quadratures are constructed using the Smolyak
formula given in \eref{smolyak-sparse-grid}. The index sets used in both
\eref{spectral-decomposition} and \eref{smolyak-sparse-grid} are the one given
in \eref{anisotropic-total-order-index-set} wherein the anisotropic weights are
set according to \eref{dimension-contribution} and \eref{dimension-anisotropy}.

\subsubsection{Efficient Implementation}

The pair of \vz and $\v{c}$ uniquely characterizes the uncertainty
quantification problem at hand. Once they have been identified, and the desired
approximation level $l = \lc = \lq$ has been specified, the corresponding
polynomial basis and quadrature stay the same for all quantities that one might
be interested in studying. This observation is of high importance as a lot of
preparatory work can and should be done only once and then stored for future
uses. In particular, the construction in \eref{spectral-decomposition} can be
reduced to one matrix multiplication with a precomputed matrix, which we shall
demonstrate next.

Let $\nc = \cardinality{\multiindices{\nz}{l}}$ be the cardinality of
$\multiindices{\nz}{l}$, which is also the number of polynomial terms and,
hence, coefficients in \eref{spectral-decomposition}. Assume the multi-indices
contained in $\multiindices{\nz}{l}$ are arranged in a vector
$(\multiindex_i)_{i = 1}^\nc$, which gives a certain ordering. Now, let
\[
  \Pi = \big( \pi_{ij} = \phi_{\multiindex_i}(\v{x}_j) \, w_j \big)_{i = 1, \, j = 1}^{i = \nc, \, j = \nq},
\]
that is, $\pi_{ij}$ is the polynomial corresponding to the $i$th multi-index
evaluated at the $j$th quadrature point and multiplied by the $j$th quadrature
weight. We refer to $\Pi$ as the projection matrix. The coefficients in
\eref{spectral-decomposition} can now be computed as
\[
  \hat{\v{v}} = \Pi \, \v{v}
\]
where
\[
  \hat{\v{v}} = (\hat{\g}_i)_{i = 1}^\nc \hspace{1em} \text{and} \hspace{1em}
  \v{v} = \big(\g(\transform{\v{x}_i}) \big)_{i = 1}^\nq.
\]
It can be seen that \eref{coefficient-evaluation} is a matrix version of
\eref{numerical-integration}. $\Pi$ is the one that should be precomputed. The
pseudocode of the procedure is given in \aref{surrogate-construction} wherein
Algorithm~X stands for the routine that calculates \g for a given \vu. Needless
to say, Algorithm~X is problem specific and has a crucial impact on the
performance of the whole procedure presented in this section: any modeling
errors inherent to this algorithm can propagate to the output of the uncertainty
analysis. Algorithm~X will be further discussed in
\sref{temperature-analysis}--\sref{reliability-optimization}.

\subsection{Post-Processing}

The function given by \eref{spectral-decomposition} is nothing more than a
polynomial; hence, it is easy to interpret and easy to evaluate. Consequently,
having constructed such an expansion, various statistics about \g can be
estimated with little effort. Moreover, \eref{spectral-decomposition} yields
analytical formulae for the expected value and variance of \g solely based on
the coefficients of \eref{spectral-decomposition}:
\[
  \expectation{\g} = \hat{\g}_{\v{0}} \hspace{1em} \text{and} \hspace{1em}
  \variance{\g} = \sum_{\multiindex \in \multiindices{\nz}{l} \setminus \{ \v{0} \}} \hat{\g}_{\multiindex}^2
\]
where $\v{0} = (0)$ is a multi-index with all entries equal to zero. Such
quantities as the cumulative distribution and probability density functions can
be estimated by sampling \eref{spectral-decomposition}; each sample is a trivial
evaluation of a polynomial.

\begin{remark}
When \g is multidimensional, we shall consider it as a row vector with an
appropriate number of elements. Then all the operations with respect to \g, such
as those in \eref{spectral-decomposition}, \eref{numerical-integration}, and
\eref{probabilistic-moments}, should be undertaken elementwise. In
\eref{coefficient-evaluation}, \eref{quantity-evaluation}, and
\aref{surrogate-construction}, $\v{v}$ and $\hat{\v{v}}$ are to be treated as
matrices with \nc rows, and $\g_i$ as a row vector. The output of Algorithm~X is
assumed to be automatically reshaped into a row vector.
\end{remark}

In what follows, we shall apply the probabilistic analysis developed in this
section to a number of concrete problems: temperature analysis
(\sref{temperature-analysis}), reliability analysis
(\sref{reliability-analysis}), and reliability optimization
(\sref{reliability-optimization}).
