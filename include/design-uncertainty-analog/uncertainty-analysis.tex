The key building block of our solutions developed in the subsequent section is
the uncertainty-quantification technique presented in this section. The task of
this technique is to propagate the uncertainty through the system, from a set of
inputs to a set of outputs. The inputs are the uncertain parameters \vu, and the
outputs are the quantities that the designer is interested in studying. The
former can be, for instance, the effective channel length, gate oxide thickness,
and threshold voltage, and the latter can be, for instance, the temperature
profile, energy consumption, and maximum temperature of the system at hand.

\inputfigure{chaos-overview}
The major stages of our general approach are depicted in \fref{chaos-overview}.
At Stage~1, the quantity of interest \g and the uncertain parameters \vu are
specified. The quantity is given as a ``black-box'' function that operates on a
particular outcome of the parameters. In order to evaluate \g, the designer is
implicitly required to specify the system model considered, which also includes
the power and temperature models. At Stage~2, the uncertain parameters \vu are
transformed into independent random variables \vz since independence is a
prerequisite of the subsequent calculations. At Stage~3, a surrogate for \g is
constructed by means of a \ac{PC} expansion. At Stage~4, the computed expansion
is analyzed in order to obtain the desired characteristics of the quantity \g.

Stage~1 is problem specific and will be discussed in the subsequent sections. In
this section, we elaborate on Stage~2, Stage~3, and Stage~4.

\subsection{Probability Transformation}
\slab{chaos-probability-transformation}

Independence is required by \ac{PC} expansions. In general, however, the \nu
individual variables in $\vu: \Omega \to \real^\nu$ are dependent. Therefore,
our foremost task is to transform \vu into mutually independent random variables
in order to fulfill the requirement; see Stage~2 in \fref{chaos-overview}. To
this end, an adequate probability transformation should be undertaken depending
on the available information; see \cite{eldred2008} for an overview. Denote such
a transformation by
\begin{equation} \elab{chaos-transformation}
  \vu = \transform{\vz}
\end{equation}
where $\vz: \Omega \to \real^\nz$ is a random vector with \nz independent
components.

Correlated random variables can be transformed into linearly uncorrelated ones
via the \ac{KL} decomposition given in \eref{karhunen-loeve}. If, in addition,
the correlated variables form a Gaussian vector, the uncorrelated ones are also
mutually independent. In the general case (non-Gaussian), the most prominent
solutions to attain independence are the Rosenblatt \cite{rosenblatt1952} and
Nataf transformations \cite{liu1986}. Rosenblatt's approach is suitable when the
joint distribution function of \vu is known; however, such information is rarely
available. A set of marginal distributions and a correlation matrix are more
likely to be given, which are already sufficient for the Nataf transformation.
See \sref{probability-transformation} for more detail.

Apart from the extraction of the independent variables \vz, an essential
operation at this stage is model order reduction since the number of stochastic
dimensions---that is, the dimensionality of \vz---directly impacts the
complexity of the rest of the computations. This operation is frequently treated
as a part of the \ac{KL} decomposition, which is also covered in
\sref{probability-transformation}.

\subsection{Surrogate Construction}
\slab{chaos-surrogate-construction}

Let $\g: \Omega \to \real$ be a quantity of interest dependent on \vu. For
convenience, \g is assumed to be one-dimensional, which will be generalized
later on. In order to give a computationally efficient probabilistic
characterization of \g, we utilize a nonintrusive spectral decomposition based
on orthogonal polynomials. The corresponding mathematical foundation is outlined
in \sref{polynomial-chaos}.

Assume that \g as a function of \vu belongs to $\L{2}(\Omega, \F,
\probability)$; see \sref{probability-theory}. Then \g can be expanded into the
following series (see Stage~3 in \fref{chaos-overview}):
\begin{equation} \elab{chaos-expansion}
  \g \approx \chaos{\nz}{\lc}{\g} = \sum_{\vi \in \multiindices{\nz}{\lc}} \hat{\g}_{\vi} \psi_{\vi}
\end{equation}
where $\lc \in \natural$ is the level of the expansion; $\vi = (i_k) \in
\natural^\nz$ is a multi-index; $\multiindices{\nz}{\lc}$ is a multi-index set;
and $\{ \psi_{\vi} \}$ are \nz-variate orthonormal polynomials of orders
specified by the corresponding elements of \vi.

It can be seen that the foremost step toward a polynomial expansion is the
choice of a suitable polynomial basis, which is typically made based on the
Askey scheme of orthogonal polynomials \cite{xiu2010}. The step is crucial as
the rate of convergence of \ac{PC} expansions depends on it. Although there are
no rules that guarantee the optimal choice \cite{knio2006}, there are best
practices suggesting that one should be guided by the probability distributions
of the random variables that drive the stochastic system at hand. For instance,
when a random variable follows a beta distribution, the Jacobi basis is worth
being tried first; on the other hand, the Hermite basis is preferable for
Gaussian distributions.

As shown in \eref{chaos-inner-product} and \eref{chaos-projection}, each
coefficient $\hat{\g}_{\vi}$ is an \nz-dimensional integral of the product of \g
and $\psi_{\vi}$. In general, this integral should be computed numerically as
described in \sref{numerical-integration}. Specifically, an adequate
\nz-dimensional quadrature rule $\quadrature{\nz}{\lq}$---which is a set of
\nz-dimensional points accompanied by a set of scalar weights---is to be
utilized. The result is
\begin{equation} \elab{chaos-coefficient}
  \hat{\g}_{\vi} \approx \quadrature{\nz}{\lq}{\g \psi_{\vi}} = \sum_{i = 1}^\nq \g(\transform{\vx_i}) \psi_{\vi}(\vx_i) w_i
\end{equation}
where $\lq \in \natural$ is the level of the quadrature, and $\{ \vx_i \}_{i =
1}^\nq \subset \real^\nz$ and $\{ w_i \}_{i = 1}^\nq \subset \real$ are the
corresponding points and weights, respectively. The operator
$\quadrature{\nz}{\lq}$ is constructed by means of the isotropic Smolyak
algorithm \cite{smolyak1963} as shown in \eref{integration-smolyak}. The
important aspect to note about \eref{integration-smolyak} is the structure of
the operator, namely, the multi-index index set $\multiindices{\nz}{\lq}$, which
we discuss now.

The standard choice of $\multiindices{\nz}{\lc}$ in \eref{chaos-expansion} is
the one in \eref{index-total-order-isotropic}, which is called an isotropic
total-order multi-index set. \emph{Isotropic} refers to constraining all
dimensions identically, and \emph{total-order} refers to the criterion used for
constraining each dimension. Since $\psi_{\vi}$ is a polynomial of total order
at most \lc, and \g is approximated by such a polynomial, the integrand in
\eref{chaos-coefficient} is a polynomial of total order at most $2 \lc$. Then
one usually constructs such a quadrature rule that is exact for polynomials of
total order up to $2 \lc$ \cite{eldred2008}. In the case of Gaussian quadratures
discussed in \sref{numerical-integration}, a quadrature of level \lq is exact
for polynomials of total order up to $2 \lq + 1$ \cite{heiss2008}. Therefore, in
this particular and very common case, it is sufficient to keep \lc and \lq
equal. More generally, the multi-index sets $\multiindices{\nz}{\lc}$ in
\eref{chaos-expansion} and $\multiindices{\nz}{\lq}$ in \eref{chaos-coefficient}
should be synchronized.

An important generalization of the isotropic Smolyak algorithm in
\eref{integration-smolyak} is the anisotropic Smolyak algorithm
\cite{nobile2008}. The main difference between the isotropic and anisotropic
versions lies in the content of $\multiindices{\nz}{\lq}$. An anisotropic
total-order multi-index set is defined as follows:
\begin{equation} \elab{index-total-order-anisotropic}
  \multiindices{\nz}{\lq} = \left\{ \vi \in \natural^\nz: \innerproduct{\v{c}}{\vi} \leq \lq \, \min_{i = 1}^\nz c_i \right\}
\end{equation}
where $\v{c} = (c_i) \in \real^\nz$ with $c_i \geq 0$ for $i = \range{1}{\nz}$
is a vector assigning importance weights to the dimensions, and
$\innerproduct{\cdot}{\cdot}$ is the standard inner product in $\real^\nz$.
Equation \eref{index-total-order-anisotropic} plugged into
\eref{integration-smolyak} results in a sparse grid that is exact for the
polynomial subspace obtained using the same multi-index set.

The above approach allows one to exploit the highly anisotropic behavior that is
inherent in many practical problems \cite{nobile2008}. It provides a fine
control over the computation time associated with the construction of \ac{PC}
expansions: a carefully chosen importance vector $\v{c}$ in
\eref{index-total-order-anisotropic} can significantly reduce the number of
polynomial terms in \eref{chaos-expansion} and the number of quadrature points
needed in \eref{chaos-coefficient} in order to compute the coefficients of those
polynomial terms. Then the question is in the choice of $\v{c}$. When the
\ac{KL} decomposition is utilized as a part of $\transform$ in
\eref{chaos-transformation}, a viable option is to rely on the variance
contributions of the dimensions given by $\{ \lambda_i \}_{i = 1}^\nu$ in
\eref{karhunen-loeve}. Specifically, we let
\begin{equation} \elab{chaos-anisotropic-weight}
  c_i = \left(\frac{\lambda_i}{\sum_{j = 1}^\nu \lambda_j}\right)^\gamma
\end{equation}
for $i = \range{1}{\nz}$ where $\gamma \in [0, 1]$ is a tuning parameter. The
isotropic scenario can be recovered by setting $\gamma = 0$; other values of
$\gamma$ correspond to various levels of anisotropy with the maximum attained by
setting $\gamma = 1$.

Once \vz has been identified, and \lc, \lq, and $\v{c}$ have been chosen, the
corresponding polynomial basis and quadrature rule stay the same for all
quantities that one might be interested in studying. This observation is of high
importance as a lot of preparatory work can and should be done only once and
then reused as needed. In particular, the construction in \eref{chaos-expansion}
can be reduced to one matrix multiplication with a precomputed matrix, which we
show next.

\inputalgorithm{chaos-expansion}
Let $\nc = \cardinality{\multiindices{\nz}{\lc}}$ be the cardinality of
$\multiindices{\nz}{\lc}$, which is also the number of polynomial terms or,
equivalently, coefficients in \eref{chaos-expansion}. Assume that the
multi-index set $\multiindices{\nz}{\lc}$ is given a certain ordering so that
one can refer to its elements using an ordinary index $i = \range{1}{\nc}$. Now,
let
\begin{equation} \elab{chaos-projection-matrix}
  \Pi = (\psi_{\vi_i}(\vx_j) w_j)_{i = 1, j = 1}^{i = \nc, j = \nq}
\end{equation}
The element on row $i$ and column $j$ of $\Pi$ is the polynomial identified by
multi-index $i$ evaluated at quadrature point $j$ and multiplied by quadrature
weight $j$. The matrix $\Pi$ is referred to as the projection matrix. Using
$\Pi$, the coefficients $\{ \hat{g}_{\vi}: \vi \in \multiindices{\nz}{\lc} \}$
in \eref{chaos-expansion} can now be trivially computed as
\begin{equation} \elab{chaos-coefficients}
  \hat{\vg} = \Pi \, \vg
\end{equation}
where
\begin{align*}
  & \hat{\vg} = (\hat{\g}_i)_{i = 1}^\nc \text{ and} \\
  & \vg = (\g(\transform{\vx_i}))_{i = 1}^\nq.
\end{align*}
It can be seen that this formula is a matrix version of
\eref{chaos-coefficient}. The matrix $\Pi$ is the one that should be precomputed
and stored for future use. The pseudocode of the expansion procedure is given in
\aref{chaos-expansion} where Algorithm~G stands for the routine that calculates
\g for a given \vu, which is problem specific.

Let us summarize this subsection. In order to give a probabilistic
characterization of a quantity of interest, we construct a polynomial expansion
of this quantity as shown in \eref{chaos-expansion}. The coefficients of this
expansion are found by means of a suitable multivariate quadrature as shown in
\eref{chaos-coefficient}. The quadrature is constructed via the Smolyak formula
given in \eref{integration-smolyak}. The multi-index set used in both
\eref{chaos-expansion} and \eref{integration-smolyak} is the one given in
\eref{index-total-order-anisotropic} where the anisotropic weights can be set
according to \eref{chaos-anisotropic-weight}. For computational efficiency, the
projection matrix in \eref{chaos-projection-matrix} is to be calculated once and
used thereafter.

\subsection{Post-Processing}
\slab{chaos-post-processing}

Due to the properties of \ac{PC} expansions---in particular, due to the mutual
orthogonality of the basis functions discussed in \sref{polynomial-chaos}---the
obtained polynomial representation allows for various statistics about \g to be
estimated with little effort, which is the subject of Stage~4 in
\fref{chaos-overview}. The reason behind the straightforwardness is that the
function given in \eref{chaos-expansion} is nothing more than a polynomial;
hence, it is easy to interpret and easy to evaluate.

Let us find, for example, the expectation and variance of \g. Since the first
polynomial $\psi_{\v{0}}$ in a polynomial basis is unity by definition
\cite{xiu2010},
\[
  \expectation{\psi_{\v{0}}} = 1.
\]
Therefore, using the orthogonality property in \eref{chaos-orthogonality}, we
conclude that
\[
  \expectation{\psi_{\vi}} = 0
\]
for $\vi \in \multiindices{\nz}{\lc} \setminus \{ \v{0} \}$. Consequently, the
expected value and variance of \g have the following straightforward
expressions:
\begin{equation} \elab{chaos-moments}
  \begin{split}
    & \expectation{\g} = \hat{\g}_{\v{0}} \text{ and} \\
    & \variance{\g} = \sum_{\vi \in \multiindices{\nz}{\lc} \setminus \{ \v{0} \}} \hat{\g}_{\vi}^2,
  \end{split}
\end{equation}
respectively, the squaring should be understood element-wise. It can be seen
that the \ac{PC} decomposition provides analytical formulae for these
probabilistic moments, and that they are solely based on the coefficients of the
expansion.

The \ac{CDF} and \ac{PDF} of \g as well as probabilities of various events can
be estimated by means of \ac{MC} sampling applied to \eref{chaos-expansion}. The
procedure can be better understood by rewriting \eref{chaos-expansion}, which is
given in terms of operators, as follows:
\[
  \g \approx \chaos{\nz}{\lc}{\g}(\vz) = \sum_{\vi \in \multiindices{\nz}{\lc}} \hat{\g}_{\vi} \psi_{\vi}(\vz).
\]
Here the aforementioned operators are applied to an outcome of \vz drawn from
the corresponding distribution. Consequently, in this case, each sample is a
trivial evaluation of a polynomial, and, hence, \ac{MC} sampling is
computationally cheap. Furthermore, global and local sensitivity analysis of
deterministic and stochastic quantities can be readily conducted on the
expansion.

\begin{remark} \rlab{chaos-multidimensional-output}
The development given here remains valid even when \g is multidimensional from
the standpoint of the number of outputs. In this case, it is considered as a row
vector with an appropriate number of elements. Then all the operations with
respect to \g, such as those in \eref{chaos-expansion},
\eref{chaos-coefficient}, and \eref{chaos-moments}, should be undertaken
element-wise. In \eref{chaos-coefficients} and \aref{chaos-expansion}, \vg and
$\hat{\vg}$ are treated as matrices with \nc rows, and $\g_i$ as a row vector.
The output of Algorithm~G in \aref{chaos-expansion} is assumed to be
automatically reshaped into a row vector.
\end{remark}

In what follows, we shall apply the uncertainty analysis developed in this
section to a number of concrete problems, namely, transient and dynamic
steady-state temperature analysis and reliability analysis and optimization.
