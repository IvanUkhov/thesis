In this section, we evaluate the performance of our approach to reliability
analysis and optimization presented in \sref{chaos-reliability-analysis} and
\sref{chaos-reliability-optimization} considering the illustrative application
described in \sref{chaos-reliability-application}. The technique for dynamic
steady-state analysis under process variation delineated in
\sref{chaos-dynamic-steady-state-analysis} is also a part of the assessment
since it is included in the reliability model. All the experiments are conducted
on a \up{GNU}/Linux machine equipped with 16 processors Intel Xeon E5520
2.27~\up{GH}z and 24~\up{GB} of \up{RAM}. All the configuration files used in
the experiments are available online at \cite{eslab2015}.

We consider a 45-nm technological process and rely on the 45-nm open cell
library by NanGate \cite{nangate}. The effective channel length and gate oxide
thickness are assumed to have nominal values equal to 22.5~nm and 1~nm,
respectively. Based on \up{ITRS} \cite{itrs}, each process parameter is assumed
to deviate up to 12\% of its nominal value, and this percentage is treated as
three standard deviations. Regarding the correlation function in
\eref{inference-correlation}, the weight coefficient $w$ is set to 0.5, and the
length-scale parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of
the die. The model-order-reduction procedure in
\sref{probability-transformation} is set to preserve 95\% of the variance of the
problem. The parameter $\gamma$ in \eref{chaos-anisotropic-weight} tuning the
anisotropy of \ac{PC} expansions and integration grids is set to 0.25.

Heterogeneous platforms and periodic applications are generated randomly via
\up{TGFF} \cite{dick1998} in such a way that the execution time of each task is
uniformly distributed between 10 and 30~ms, and its dynamic power is uniformly
distributed between 6 and 20~W. The floorplans of the platforms are regular
grids where each processing element occupies $2 \times 2$~mm\textsuperscript{2}.
The sampling interval \dt of power and temperature profiles is set to 1~ms; in
practice, \dt should be set to a value that is reasonable for the problem at
hand. The stopping condition in \aref{chaos-dynamic-steady-state-iterative} is
that the \ac{NRMSE} between two successive temperature profiles becomes smaller
than 1\%, which typically requires 3--5 iterations.

\hiddensubsection{Approximation Accuracy}

Our goal here is to study the accuracy of the proposed solutions. To this end,
in this subsection, we consider the quantity of interest given in
\eref{chaos-optimization-quantity} in isolation. This quantity plays the key
role in the subsequent optimization since the optimization objective and
constraints depend on it as discussed previously.

We compare our performance with the performance of \ac{MC} sampling applied to
\eref{chaos-optimization-quantity}. The operations performed by the
\ac{MC}-based approach for one sample are exactly the same as those performed by
our technique for one quadrature point. The only difference is that no model
order reduction of any kind is undertaken prior to \ac{MC} sampling, which
ensures that the resulting accuracy is not compromised. The number of \ac{MC}
samples is set to 10\textsuperscript{4}, which is a practical assumption based
on our experience, the experience from the literature \cite{xiang2010, juan2012,
lee2013}, and the theoretical estimates given in \cite{diaz-emparanza2002}.
Thus, we consider this setup of \ac{MC} sampling to be an adequate reference for
comparison.

\inputtable{chaos-optimization-accuracy}
The obtained results are displayed in \tref{chaos-optimization-accuracy} where
we consider a quad-core platform, that is, $\np = 4$, with 10 randomly generated
applications and vary the level of polynomial expansions \lc from 1 to 5. The
errors for the three components of $\vg = (E, Q, \life)$ are denoted by
$\epsilon_E$, $\epsilon_Q$, and $\epsilon_{\life}$, respectively. Each error
indicator shows the distance between the empirical probability distributions
produced by our approach and the ones produced by \ac{MC}\ sampling, and the
measure of this distance is the Kullback--Leibler divergence (\up{KLD})
\cite{gelman2013, hastie2013} where the results of \ac{MC} sampling are treated
as the true ones. The \up{KLD} takes non-negative values and attains zero only
when two distributions are equal almost everywhere \cite{durrett2010}. In
general, the errors decrease as \lc increases. This trend, however, is not
monotonic for \ac{PC} expansions of high levels; see $\epsilon_Q$ and
$\epsilon_{\life}$ for $\lc = 5$. The observation can be ascribed to the random
nature of sampling and to the reduction procedures that we undertake in order to
gain speed; they might impose limitations on the accuracy that can be attained
by polynomial approximations. \tref{chaos-optimization-accuracy} also contains
the numbers of polynomial terms \nc and quadrature points \nq corresponding to
each value of \lc. We have also performed the above experiment for platforms
with fewer and with more processing elements and observed results similar to
those in \tref{chaos-optimization-accuracy}.

Based on the figures reported in \tref{chaos-optimization-accuracy}, we consider
the results delivered by third-level \ac{PC} expansions---where the \up{KLD}
drops to the third decimal place for all three quantities---to be sufficiently
accurate. Therefore, we fix \lc (and \lq as they are kept synchronized) to three
for the rest of the experiments.

\hiddensubsection{Computational Speed}

\inputtable{chaos-optimization-speed}
Our objective is now to assess the speed of the proposed solutions by
considering the same setup as the one outlined in the previous subsection.
\tref{chaos-optimization-speed} displays the time needed to perform one
characterization of \vg for the number of processing elements \np ranging from 2
to 32. Note that, in this experiment, no parallel computing is utilized. It can
be seen that the computation time ranges from a fraction of a second to around
two seconds. More importantly, \tref{chaos-optimization-speed} provides
information about a number of complementary quantities that are of high interest
to the designer, which we discuss below.

The primary quantity to pay attention to is the number of random variables \nz
preserved after the reduction procedure noted in
\sref{chaos-probability-transformation} and described in
\sref{probability-transformation}. Without this reduction, \nz would be $2 \np$
since there are two process parameters per processing element; see
\sref{chaos-reliability-application}. It can be seen in
\tref{chaos-optimization-speed} that there is no reduction for the dual-core
platform while around 80\% of the stochastic dimensions are eliminated for the
platform with 32 cores. In addition, one can note that \nz is the same for the
last two platforms. The magnitude of reduction is solely determined by the
correlation patterns assumed (see \sref{chaos-reliability-application}) and the
floorplans of the considered platforms, which we also observe and elaborate on
in \sref{chaos-transient-results}.

Another important quantity displayed in \tref{chaos-optimization-speed} is the
number of quadrature points \nq. This number is the main indicator of the
computational complexity of our probabilistic analysis: it equals to the number
of times Algorithm~G in \aref{chaos-expansion} is to be executed in order to
construct a polynomial expansion for the quantity of interest \g, which, in this
case, is given in \eref{chaos-optimization-quantity}. It can be seen that \nq is
very low. In order to substantiate this, the last column of
\tref{chaos-optimization-speed} shows the speedup of our approach with respect
to 10\textsuperscript{4} \ac{MC} samples. Our solution is faster by
approximately 100--200 times while delivering highly accurate results as
discussed earlier. It should be noted that the comparison has been drawn based
on the number of evaluation points rather than on the actual time since the
relative cost of other computations is negligible.

To conclude, the proposed solutions to dynamic steady-state analysis and
reliability analysis under process variation have been assessed using the
composite quantity in \eref{chaos-optimization-quantity}. The results shown in
\tref{chaos-optimization-accuracy} and \tref{chaos-optimization-speed} allow us
to conclude that our approach is both accurate and computationally efficient.

\hiddensubsection{Optimization Effectiveness}

\inputtable{chaos-optimization-objective}
In this subsection, the results of the optimization procedure formulated in
\sref{chaos-reliability-optimization} are reported. To reiterate, the objective
is to minimize the expected energy consumption as shown in
\eref{chaos-optimization-objective} while satisfying a set of constraints on the
maximum application period, maximum temperature, and minimum lifetime as shown
in \eref{chaos-optimization-constraints}. To this end, we employ a genetic
algorithm. The population is evaluated in parallel using 16 computational cores.

The goal of this experiment is to justify the following assertion: reliability
analysis has to account for the effect of process variation on temperature. To
this end, for each problem (a pair of a platform and an application), we run the
optimization procedure twice: the first run assumes the usual setup discussed so
far, and the second run treats the objective in
\eref{chaos-optimization-objective} and the constraints in
\eref{chaos-optimization-constraints} as deterministic. To elaborate, the second
run assumes that temperature is deterministic and can be computed using the
nominal values of the process parameters. Hence, in the deterministic case, only
one execution of the system is needed in order to evaluate the fitness function,
and \eref{chaos-optimization-objective} and \eref{chaos-optimization-constraints}
become, respectively,
\[
    \min_{\schedule} E(\schedule)
\]
and
\[
  \begin{split}
    & \period(\schedule) \leq \period_\maximum, \\
    & Q(\schedule) \geq \q_\maximum, \text{ and} \\
    & \life(\schedule) \leq \life_\minimum.
  \end{split}
\]

We consider 5 platforms with the number of processing elements \np taking values
in $\{ 2^i \}_{i = 1}^5$ and 10 applications with the number of tasks $\nt = 20
\np$; thus, there are 50 problems in total. The platforms and applications are
generated randomly using \up{TGFF} \cite{dick1998}. In
\eref{chaos-optimization-constraints}, $\rho_\burn$ and $\rho_\wear$ are set to
0.01. Due to the diversity of the problems, $\period_\maximum$, $\q_\maximum$,
and $\life_\minimum$ are found individually for each problem, ensuring that they
make sense for the subsequent optimization. For instance, $\q_\maximum$ is found
between 90 and \celsius{120}. Note, however, that these three parameters stay
the same for both the stochastic and deterministic cases.

The obtained results are reported in \tref{chaos-optimization-objective}. The
most important message is in the last column where \emph{failure} refers to the
ratio of the solutions produced by the deterministic optimization that, after
being reevaluated using our probabilistic approach (that is, after taking
process variation into account), have been found to be violating the
probabilistic constraints given in \eref{chaos-optimization-constraints}. To
give an example, for the quad-core platform, 6 out of 10 schedules proposed by
the deterministic approach violate the constraint on the maximum temperature or
minimum lifetime (or both) when process variation is taken into consideration.
The more complex the problem becomes, the higher values the failure rate
attains: with 16 and 32 processing elements (320 and 640 tasks, respectively),
all deterministic solutions violate the imposed constraints. Moreover, the
difference between the acceptable one percent of burn and wear ($\rho_\burn =
\rho_\wear = 0.01$) and the actual probability of burn and wear is found to be
as high as 80\% in some cases, which is unacceptable.

In addition, we take a close look at those few deterministic solutions that have
passed the probabilistic reevaluation and observe that the reported reduction of
the energy consumption and maximum temperature as well as the reported increase
in the lifetime are overoptimistic. To elaborate, the predictions produced by
the deterministic optimization, which neglects process variation, are compared
with the expected values obtained when process variation is taken into account.
The comparison shows that the expected energy and temperature are up to 5\%
higher while the expected lifetime is up to 20\% shorter than the ones estimated
by the deterministic approach. This aspect of the deterministic optimization can
mislead the designer. Consequently, when studying those aspects of electronic
systems that are concerned with power, temperature, and reliability, the
ignorance of the deteriorating effect of process variation can severely
compromise the associated design decisions, making them less profitable in the
best case and dangerous in the worst scenario.

Let us now comment on the optimization time shown in
\tref{chaos-optimization-objective}. It can be seen that our implementation of
the proposed framework takes from about one minute to six hours (utilizing 16
cores) in order to perform the optimization, and that the deterministic
optimization is approximately 2--40 times faster. However, the price to pay when
relying on the deterministic approach is considerably high as discussed above.
The deterministic technique is blind-guessing with highly unfavorable odds of
succeeding. Therefore, we consider the computation time of our framework to be
reasonable and affordable.

Lastly, we perform an experiment targeted at investigating the impact of the
lifetime constraint in \eref{chaos-optimization-constraints} on the reduction of
the expected energy consumption. To this end, we run our stochastic optimization
(all 50 problems) without the reliability constraint and compare the
corresponding results with those obtained when the lifetime constraint is
included. We observe that the expected energy consumption is higher when the
constraint is taken into account; however, the difference vanishes when the
complexity of the problem increases. On average, the cost of the lifetime
constraint is below 5\% in terms of the expected energy consumption. Without the
constraint, however, no (probabilistic) guarantees on the lifetime of the
considered systems can be given.
