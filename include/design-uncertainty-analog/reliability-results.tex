In this section, we evaluate the performance of the proposed approach to
reliability analysis and optimization presented in
\sref{chaos-reliability-analysis} and \sref{chaos-reliability-optimization}
considering the illustrative application that is described in
\sref{chaos-reliability-application}. All the experiments are conducted on a
\up{GNU}/Linux machine equipped with 16 processors Intel Xeon E5520
2.27~\up{GH}z and 24~\up{GB} of \up{RAM}. All the configuration files used in
the experiments are available online at \cite{eslab2015}.

We consider a 45-nm technological process and rely on the 45-nm open cell
library by NanGate \cite{nangate}. The effective channel length and gate oxide
thickness are assumed to have nominal values equal to 22.5~nm and 1~nm,
respectively. Based on \up{ITRS} \cite{itrs}, each process parameter is assumed
to deviate up to 12\% of its nominal value, and this percentage is treated as
three standard deviations. Regarding the correlation function in
\eref{inference-correlation}, the weight coefficient $w$ is set to 0.5, and the
length-scale parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of
the die. The model-order-reduction procedure in
\sref{probability-transformation} is set to preserve 95\% of the variance of the
problem. The parameter $\gamma$ in \eref{chaos-anisotropic-weight} tuning the
anisotropy of \ac{PC} expansions and integration grids is set to 0.25.

Heterogeneous platforms and periodic applications are generated randomly via
\up{TGFF} \cite{dick1998} in such a way that the execution time of each task is
uniformly distributed between 10 and 30~ms, and its dynamic power is uniformly
distributed between 6 and 20~W. The floorplans of the platforms are regular
grids where each processing element occupies $2 \times 2$~mm\textsuperscript{2}.
The sampling interval \dt of power and temperature profiles is set to 1~ms; in
practice, \dt should be set to a value that is reasonable for the problem at
hand. The stopping condition in \aref{chaos-dynamic-steady-state-iterative} is
that the \ac{NRMSE} between two successive temperature profiles becomes smaller
than 1\%, which typically requires 3--5 iterations.

\hiddensubsection{Accuracy and Speed}

\inputtable{chaos-optimization-accuracy}
\inputtable{chaos-optimization-speed}
Our objective here is to study the accuracy and speed of the proposed solutions.
Since the optimization procedure described in \sref{reliability-optimization}
embraces all the techniques developed throughout the paper, we shall perform the
assessment directly in the design-space-exploration context. In other words, we
do not consider temperature analysis or reliability analysis as a separate
uncertainty quantification problem in our experiments and shall focus on the
quantity of interest given in \eref{quantity-of-interest}. This quantity plays
the key role as the objective function in \eref{objective} and the constraints
in \eref{thermal-constraint} and \eref{reliability-constraint} are entirely
based on it.

We shall compare our performance with the performance of \ac{MC} sampling. The
operations performed by the \ac{MC}-based approach for one sample are exactly
the same as those performed by our technique for one quadrature point. The only
difference is that no reduction of any kind is undertaken prior to \ac{MC}
sampling. In other words, the \ac{MC}-based approach samples the \emph{original}
model and, hence, does not compromise any resulting accuracy. The number of
\ac{MC} samples is set to $10^4$, which is a practical assumption that conforms
to the experience from the literature \cite{ukhov2014, lee2013, juan2012,
xiang2010} and to the theoretical estimates given in \cite{diaz-emparanza2002}.
Hence, we consider this setup of \ac{MC} sampling to be a paragon of accuracy.

The results concerning accuracy are displayed in \tref{accuracy} where we
consider a quad-core platform, that is, $\np = 4$, with ten randomly generated
applications and vary the level of polynomial expansions \lc from one to five.
The errors for the three components of $\vq = (E, Q, T)$ are denoted by
$\epsilon_E$, $\epsilon_Q$, and $\epsilon_T$, respectively. Each error indicator
shows the distance between the empirical probability distributions produced by
our approach and the ones produced by \ac{MC}\ sampling, and the measure of this
distance is the popular Kullback--Leibler divergence (\up{KLD}) wherein the
results of \ac{MC}\ sampling are treaded as the ``true'' ones. The \up{KLD}
takes nonnegative values and attains zero only when two distributions are equal
almost everywhere \cite{durrett2010}. In general, the errors decrease as \lc
increases. This trend, however, is not monotonic for expansions of high levels
(see $\epsilon_Q$ and $\epsilon_T$ for $\lc = 5$). The observation can be
ascribed to the random nature of sampling and the fact that the reduction
procedures, which we undertake to gain speed, might impose limitations on the
accuracy that can be attained by polynomial expansions. \tref{accuracy} also
contains the numbers of polynomial terms \nc and quadrature points \nq
corresponding to each value of \lc. We also performed the above experiment for
platforms with fewer/more processing elements; the observations were similar to
the ones in \tref{accuracy}.

Based on \tref{accuracy}, we consider the results delivered by third-level
polynomial expansions, where the \up{KLD} drops to the third decimal place for
all quantities, to be sufficiently accurate, and, therefore, we fix $\lc = \lq =
l = 3$ (recall the notation in the last paragraph of
\sref{classical-decomposition}) for the rest of the experiments.

\tref{speed} displays the time needed to perform one characterization of \vq for
the number of processing elements \np swept from 2 to 32. It can be seen that
the computational time ranges from a fraction of a second to around two seconds.
More importantly, \tref{speed} provides information about a number of
complementary quantities that are of high interest for the designer of the
proposed techniques, which we discuss below.

The primary quantity to pay attention to is the number of random variables \nz
preserved after the reduction procedure described in \sref{preprocessing} and
\sref{model-order-reduction}. Without this reduction, \nz would be $2 \np$ as
there are two process parameters per processing element. It can be seen that
there is no reduction for the dual-core platform while around 80\% of the
stochastic dimensions have been eliminated for the platform with 32 cores. In
addition, one can note that \nz is the same for the last two platforms. The
magnitude of reduction is solely determined by the correlation patterns assumed
(see \sref{preprocessing}) and the floorplans of the considered platforms.

Another important quantity displayed in \tref{speed} is the number of quadrature
nodes \nq. This number is the main indicator of the computational complexity of
our probabilistic analysis: it equals to the number of times Algorithm~G in
\aref{surrogate-construction} is executed to construct a polynomial expansion of
\eref{quantity-of-interest} needed for the evaluation of the fitness function.
It can be seen that \nq is very low. To illustrate this, the last column of
\tref{speed} shows the speedup of our approach with respect to $10^4$ \ac{MC}.
Our solution is faster by approximately 100--200 times while delivering highly
accurate results as discussed earlier. It should be noted that the comparison
has been drawn based on the number of evaluation points rather than on the
actual time since the relative cost of other computations is negligible.

To conclude, the proposed solutions to temperature and reliability analyses
under process variation have been assessed using the composite quantity of
interest given in \eref{quantity-of-interest}. The results shown in
\tref{accuracy} and \tref{speed} allow us to conclude that our approach is both
accurate and computationally efficient.

\hiddensubsection{Probabilistic Optimization}

\inputtable{chaos-optimization-objective}
Parallel computing is utilized only in the experiments reported in
\sref{experimental-results-optimization}.

In this subsection, we report the results of the optimization procedure
formulated in \sref{reliability-optimization}. To reiterate, the objective is to
minimize energy as shown in \eref{objective} while satisfying a set of
constraints on the application period, maximum temperature, and minimum lifetime
as shown in \eref{timing-constraint}, \eref{thermal-constraint}, and
\eref{reliability-constraint}, respectively. We employ a genetic algorithm for
optimization. The population is evaluated in parallel using 16 processors; this
job is delegated to the parallel computing toolbox of \up{MATLAB} \cite{matlab}.

The goal of this experiment is to justify the following assertion: reliability
analysis has to account for the effect of process variation on temperature. To
this end, for each problem (a pair of a platform and an application), we shall
run the optimization procedure twice: once using the setup that has been
described so far and once making the objective in \eref{objective} and the
constraints in \eref{thermal-constraint} and \eref{reliability-constraint}
deterministic. To elaborate, the second run assumes that temperature is
deterministic and can be computed using the nominal values of the process
parameters. Consequently, only one simulation of the system is needed in the
deterministic case to evaluate the fitness function, and \eref{objective},
\eref{thermal-constraint}, and \eref{reliability-constraint} become,
respectively,
\[
  \min_{\schedule} E(\schedule), \hspace{0.7em}
  Q(\schedule) \geq \q_\maximum, \hspace{0.7em} \text{and} \hspace{0.7em}
  T(\schedule) \leq T_\minimum.
\]

We consider platforms with $\np = 2$, 4, 8, 16, and 32 cores. Ten applications
with the number of tasks $\nt = 20 \, \np$ (that is, 40 tasks for 2 cores up to
640 tasks for 32 cores) are randomly generated for each platform; thus, 50
problems in total. The floorplans of the platforms and the task graphs of the
applications, including the execution time and dynamic power consumption of each
task on each core, are available online at \cite{sources}. $\rho_\burn$ and
$\rho_\wear$ in \eref{thermal-constraint} and \eref{reliability-constraint},
respectively, are set to 0.01. Due to the diversity of the problems,
$t_\maximum$, $\q_\maximum$, and $T_\minimum$ are found individually for each
problem, ensuring that they make sense for the subsequent optimization. For
instance, $\q_\maximum$ was found within the range 90--120${}^\circ{}C$. Note,
however, that these three parameters stay the same for both the probabilistic
and deterministic variants of the optimization.

The obtained results are reported in \tref{optimization}, and the most important
message is in the last column. \emph{Failure rate} refers to the ratio of the
solutions produced by the deterministic optimization that, after being
reevaluated using the probabilistic approach (that is, after taking process
variation into account), have been found to be violating the probabilistic
constraints given in \eref{thermal-constraint} and/or
\eref{reliability-constraint}. To give an example, for the quad-core platform,
six out of ten schedules proposed by the deterministic approach violate the
constraints on the maximum temperature and/or minimum lifetime when evaluated
considering process variation. The more complex the problem becomes, the higher
values the failure rate attains: with 16 and 32 processing elements (320 and 640
tasks, respectively), all deterministic solutions violate the imposed
constraints. Moreover, the difference between the acceptable one percent of
burn/wear ($\rho_\burn = \rho_\wear = 0.01$) and the actual probability of
burn/wear was found to be as high as 80\% in some cases, which is unacceptable.

In addition, we inspected those few deterministic solutions that had passed the
probabilistic reevaluation and observed that the reported reduction of the
energy consumption and maximum temperature as well as the reported increase of
the lifetime were overoptimistic. More precisely, the predictions produced by
the deterministic optimization, which ignores variations, were compared with the
expected values obtained when process variation was taken into account. The
comparison showed that the expected energy and temperature were up to 5\% higher
while the expected lifetime was up to 20\% shorter than the ones estimated by
the deterministic approach. This aspect of the deterministic optimization can
mislead the designer.

Consequently, when studying those aspects of electronic systems that are
concerned with power, temperature, and reliability, the ignorance of the
deteriorating effect of process variation can severely compromise the associated
design decisions making them less profitable in the best case and dangerous,
harmful in the worst scenario.

Let us now comment on the optimization time shown in \tref{optimization}. It can
be seen that the prototype of the proposed framework takes from about one minute
to six hours (utilizing 16 \up{CPU}s) in order to perform optimization, and
the deterministic optimization is approximately 2--40 times faster. However, the
price to pay when relying on the deterministic approach is considerably high as
we discussed in the previous paragraphs. It can be summarized as ``blind
guessing with highly unfavorable odds of succeeding." Consequently, we consider
the computational time of our framework to be reasonable and affordable,
especially in an industrial setting.

Lastly, we performed experiments also to investigate the impact of the lifetime
constraint in \eref{reliability-constraint} on the reduction of the expected
energy consumption. To this end, we ran our probabilistic optimization (all 50
problems) without the constraint in \eref{reliability-constraint} and compared
the corresponding results with those obtained considering the lifetime
constraint. We observed that the expected energy consumption was higher when
\eref{reliability-constraint} was taken into account, but the difference
vanishes when the complexity of the problems increases. On average, the cost of
\eref{reliability-constraint} was below 5\% of the expected energy consumption.
Without \eref{reliability-constraint}, however, no (probabilistic) guarantees on
the lifetime of the considered systems can be given.
