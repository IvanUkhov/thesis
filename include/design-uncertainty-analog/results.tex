In this section, we evaluate our framework on different configurations of the
illustrative application given in \sref{chaos-application}. All the experiments
are conducted on a \up{GNU}/Linux machine with Intel Core i7 2.66~\up{GH}z and
8~\up{GB} of \up{RAM}. All the configuration files used in the experiments are
available online at \cite{eslab2014}.

Let us first elaborate on the default configuration of our experimental setup,
which, in the following subsections, will be adjusted according to the purpose
of each particular experiment. We consider a 45-nanometer technological process.
The effective channel length is assumed to have a nominal value of 17.5~nm
\cite{ptm} and a standard deviation of 2.25~nm where the global and local
variations are equally weighted. Correlation matrices are computed according to
\eref{inference-correlation} where the length-scale parameters $\ell_\SE$ and
$\ell_\OU$ are set to half the size of the square die. In the model order
reduction technique describe in \sref{probability-transformation}, the threshold
parameter $\eta$ is set to 0.99, which preserves 99\% of the variance of the
data. Dynamic power profiles involved in the experiments are based on
simulations of randomly generated applications via \cite{dick1998}; in practice,
such profiles are typically obtained via an adequate simulator of the
architecture of interest. The floorplans are constructed in such a way that the
processing elements form regular grids. The time step of power and temperature
traces is set to 1~ms, which is also the time step of the recurrence in
\eref{chaos-recurrence}.

Since the temperature part of our power and temperature analysis is our primary
contribution in this chapter, we focus on the assessment of temperature
profiles. Note, however, that the results for temperature allow one to
implicitly draw reasonable conclusions regarding power since power is an
intermediate step towards temperature, and any accuracy problems with respect to
power are expected to propagate to temperature. Also, since the
temperature-driven studies \cite{juan2011, juan2012, huang2009a, lee2013}
discussed in \sref{chaos-prior} work under the static steady-state
assumption---the work in \cite{juan2011} is also limited to the maximum
temperature, and the one in \cite{huang2009a} does not model the
power-temperature interplay---a one-to-one comparison with our framework is not
possible.

For comparison purposes, we employ \ac{MC} sampling. The \ac{MC} approach is set
up to preserve the whole variance of the problem, that is, there is no model
order reduction applied, and to solve \eref{temperature-model-original} directly
using the Runge--Kutta fourth- and fifth-order formulae combined (the
Dormand--Prince method).

\hiddensubsection{Approximation Accuracy}

The first set of experiments is to identify the accuracy of our framework with
respect to \ac{MC} sampling. At this point, it is important to note that the
true distributions of temperature are unknown, and both the \ac{PC} and \ac{MC}
approaches introduce errors. These errors decrease as the accuracy level \lc of
\ac{PC} expansions and the number of samples \no of \ac{MC} sampling increase.
Hence, instead of postulating that the \ac{MC} technique with a certain number
of samples is the solution that we should achieve, we vary both \lc and \no and
monitor the corresponding difference between the results produced by the two
alternatives.

We also inspect the impact of the correlation patterns between the local random
variables $\{ u_{\local, i} \}_{i = 1}^\np$; recall \sref{chaos-application}.
More specifically, apart from \lc and \no, we change the balance between the two
correlation functions shown in \eref{inference-correlation}, that is, the
squared-exponential $k_\SE$ and Ornstein--Uhlenbeck $k_\OU$ kernels, which is
controlled by the weight coefficient $w \in [0, 1]$.

The \ac{PC} and \ac{MC} methods are compared by means of three metrics. The
first two are the \acfp{NRMSE} of the expectation and variance of the computed
stochastic temperature profiles. The third metric is the mean of the \acp{NRMSE}
of the empirical \acf{PDF} of temperature constructed at each time step for each
processing element. The error metrics are denoted by $\epsilon_{\expectation}$,
$\epsilon_{\variance}$, and $\epsilon_f$, respectively. The metrics
$\epsilon_{\expectation}$ and $\epsilon_{\variance}$ are straightforward to
interpret, and they are calculated using the analytical expressions in
\eref{chaos-moments}. The metric $\epsilon_f$ is a strong indicator of the
quality of the distributions estimated by our framework, and it is computed by
sampling the constructed \ac{PC} expansions. In contrast to the \ac{MC}
approach, this sampling has a negligible overhead as we note in
\sref{chaos-post-processing}.

\inputtable{chaos-accuracy}
The considered values of \lc, \no, and $w$ are in the sets $\{ i \}_{i = 1}^7$,
$\{ 10^i \}_{i = 2}^5$, and $\{ 0, 0.5, 1 \}$, respectively. The three variants
of $w$ correspond to the total dominance of $k_\OU$ ($w = 0$), perfect balance
between $k_\SE$ and $k_\OU$ ($w = 0.5$), and total dominance of $k_\SE$ ($w =
1$). A comparison for a quad-core architecture with a dynamic power profile of
$\ns = 10^2$ steps is given in \tref{chaos-accuracy-0},
\tref{chaos-accuracy-0-5}, and \tref{chaos-accuracy-1}, which correspond to $w
= 0$, $w = 0.5$, and $w = 1$, respectively. Each table contains three
subtables: one is for $\epsilon_{\expectation}$ (left), one is for
$\epsilon_{\variance}$ (middle), and one is for $\epsilon_f$ (right), which
gives nine subtables in total.

The columns of the tables that correspond to high values of \no can be used to
assess the accuracy of the constructed \ac{PC} expansions; likewise, the rows
that correspond to high values of \lc can be used to judge about the
sufficiency of the number of \ac{MC} sampling. One can immediately note that,
in all the subtables, all the error metrics tend to decrease from the top-left
corners (low values of \lc and \ns) to the bottom-right corners (high values of
\lc and \ns), which suggests that the \ac{PC} and \ac{MC} methods converge.
There are a few outliers, associated with low \ac{PC} levels and the random
nature of sampling; for instance, $\epsilon_{\variance}$ increases from 66.13 to
66.7 and $\epsilon_f$ from 1.59 to 1.62 when \no increases from
10\textsuperscript{4} and $10^5$ in \tref{chaos-accuracy-0-5}. However, the
aforementioned main trend is still clear.

For clarity of the discussions below, we focus primarily on one of the three
tables, namely, on \tref{chaos-accuracy-0-5}, since the case with $w = 0.5$
turns out to be the most challenging, which we shall elaborate on shortly. The
drawn conclusions are generalized to the other two tables at the end of this
subsection.

\inputfigure{chaos-example-density}
First, we inspect the accuracy of our technique and, therefore, pay particular
attention the columns of \tref{chaos-accuracy-0-5} corresponding to high values
of \no. It can be seen that the error of the expectation is small even for $\lc
= 1$. Concretely, it is bounded by 0.6\%; see $\epsilon_{\expectation}$ for
$\lc \geq 1$ and $\no \geq 10^4$. The error of the variance starts from 66.7\%
for the first-level \ac{PC} expansions and drops significantly to 5.71\% and
below for the fourth level and higher; see $\epsilon_{\variance}$ for $\lc \geq
4$ and $\no = 10^5$. It should be noted, however, that, for a fixed $\lc \geq
4$, $\epsilon_{\variance}$ exhibits a considerable decrease even when \no
transitions from 10\textsuperscript{4} to 10\textsuperscript{5}. The rate of
this decrease suggests that $\no = 10^4$ is not sufficient to reach the accuracy
delivered by the proposed framework, and $\no = 10^5$ might not be either.
Finally, the error of the \acp{PDF} allows us to conclude that the \ac{PDF}
computed by the third-level (and higher) \ac{PC} expansions closely follow those
estimated by the \ac{MC} technique with large numbers of samples. The observed
difference in \tref{chaos-accuracy-0-5} is bounded by 1.83\%; see $\epsilon_f$
for $\lc \geq 3$ and $\no \geq 10^4$. In order to give a better intuition about
the proximity of the two methods, \fref{chaos-example-density} displays the
\acp{PDF} computed using our framework with $\lc = 4$ (the solid lines) along
with those calculated by the \ac{MC} approach with $\no = 10^4$ (the dashed
lines) for time moment 50~ms. It can be seen that the \acp{PDF} tightly match
each other. Note that this example captures one particular time moment, and such
curves are also readily available for the other steps of the considered time
span.

Second, we investigate the convergence of the \ac{MC} technique and, therefore,
watch the rows of \tref{chaos-accuracy-0-5} that correspond to \ac{PC}
expansions of high levels. Similar to the previous observations, even for low
values of \no, the error of the expectation estimated by \ac{MC} sampling is
relatively small, bounded by 1.19\%; see $\epsilon_{\expectation}$ for $\lc
\geq 4$ and $\no = 10^2$. At the same time, the case with $\no = 10^2$ has high
error rates in terms of the variance and \acp{PDF}: they are above 38\% and
almost 3.5\%, respectively; see $\epsilon_{\variance}$ and $\epsilon_f$ for
$\lc = 7$ and $\no = 10^2$. The results for the case with $\no = 10^3$ are
reasonably more accurate. However, this trend is compromised by the figures in
\tref{chaos-accuracy-1}: 10\textsuperscript{3} samples leave an error of more
than 7\% for the variance; see $\epsilon_{\variance}$ for $\lc \geq 4$ and $\no
= 10^3$.

The above conclusions based on the results in \tref{chaos-accuracy-0-5} ($w =
0.5$) are directly applicable to those in \tref{chaos-accuracy-0} ($w = 0$) and
\tref{chaos-accuracy-1} ($w = 1$). The only difference is that the average
error rates are lower when either of the two correlation kernels in
\eref{inference-correlation} dominates. In particular, according to
$\epsilon_{\variance}$, the case with $w = 1$, which corresponds to $k_\SE$,
stands out to be the least error prone.

Guided by the observations in this subsection, we conclude that our framework
delivers accurate results starting from $\lc = 4$. The \ac{MC} estimates, on
the other hand, can be considered as sufficiently reliable starting from $\no =
10^4$. The last conclusion, however, is biased in favor of the \ac{MC}
technique since, as noted earlier, there is evidence that $10^4$ samples might
still be not enough.

\hiddensubsection{Computational Speed}

The second set of experiments is to measure the speed of the framework with
respect to \ac{MC} sampling. For clarify, we use the same level of \ac{PC}
expansions and the same number of \ac{MC} samples in each case. More precisely,
based on the conclusions from the previous subsection, $\lc = 4$ and $\no =
10^4$; the latter is also consistent with the experience from the literature
\cite{ghanta2006, bhardwaj2008, huang2009a, shen2009, lee2013} and with the
theoretical results on the accuracy of \ac{MC} sampling given in
\cite{diaz-emparanza2002}. As before, we report the results obtained for various
values of the weight coefficient $w$, which impacts the number of the
independent variables $\vz: \Omega \to \real^\nz$ preserved after the model
order reduction procedure described in \sref{probability-transformation}.

\inputtable{chaos-scaling}
First, we vary the number of processing elements \np, which directly affects the
dimensionality of the uncertain parameters $\vu: \Omega \to \real^\nu$; recall
\sref{chaos-application}. The results, including the dimensionality \nz of \vz,
are given in \tref{chaos-scaling-elements} where the considered values of \np
are in $\{ 2^i \}_{i = 1}^5$, and $\ns = 10^3$. It can be seen that the
correlation patters inherent to the manufacturing process \cite{cheng2011} open
a great possibility for model order reduction: \nz is observed to be at most 12
while the maximum number without reduction is 33 (1 global variable and 32 local
ones corresponding to the case with $\np = 32$). This reduction depends also on
the floorplans, which is illustrated by the decrease of \nz when \np increases
from 16 to 32 for $w = 1$. To elaborate, one floorplan is a four-by-four grid, a
perfect square, while the other an eight-by-four grid, a rectangle. Since both
are fitted into square dies, the former is spread across the whole die whereas
the latter is concentrated along the middle line; the rest is ascribed to the
particularities of $k_\SE$. On average, the $k_\OU$ kernel ($w = 0$) requires
the fewest number of variables while the mixture of $k_\SE$ and $k_\OU$ ($w =
0.5$) requires the most. It means that, in the latter case, more variables
should be preserved in order to retain 99\% of the variance. Hence, the case
with $w = 0.5$ is the most demanding in terms of computational complexity. The
results in the previous subsection correspond to the case with $\np = 4$;
therefore, \nz is two, five, and five in \tref{chaos-accuracy-0},
\tref{chaos-accuracy-0-5}, and \tref{chaos-accuracy-1}, respectively.

At this point, it is important to note the following. First, since the curse of
dimensionality constitutes arguably the major concern of the theory of \ac{PC}
expansions, the applicability of our framework primarily depends on how this
curse manifests itself in the problem at hand, that is, on the dimensionality
\nz of \vz. Second, since \vz is a result of a procedure that depends on many
factors, the relation between \vu and \vz is not straightforward, which is also
illustrated in the previous paragraph. Consequently, \nu can be misleading when
reasoning about the applicability of our technique; \nz is well suited for this
purpose.

Another observation with respect to \tref{chaos-scaling-elements} is the low
slope of the execution time of the \ac{MC} technique, which illustrates the
well-known fact that the workload per sample is independent of the number of
stochastic dimensions. On the other hand, the rows with $\nz \geq 10$ hint at
the curse of dimensionality, which \ac{PC} expansions suffer from. However, even
in high dimensions, the proposed framework significantly outperforms \ac{MC}
sampling. For instance, in order to analyze a dynamic power profile with
10\textsuperscript{3} steps of a platform with 32 processing elements, the
\ac{MC} approach requires more than 40 hours whereas our framework takes less
than two minutes; see the case with $w = 0.5$.

Second, we investigate the scaling properties of the framework with respect to
the duration of the analyzed time span, which is directly proportional to the
number of time steps \ns covered by power and temperature profiles. The results
for a quad-core platform are reported in \tref{chaos-scaling-steps}. Due to the
long execution times demonstrated by the \ac{MC} approach, its statistics for
high values of \ns are extrapolated based on a smaller number of samples, that
is, $\no < 10^4$. Similar to \tref{chaos-scaling-elements}, we observe the
dependency of the constructed expansions on the dimensionality \nz, which is two
for $w = 0$ and five for the other two values of $w$; see
\tref{chaos-scaling-elements} for $\np = 4$. It can be seen in
\tref{chaos-scaling-steps} that the computation times of both methods grow
linearly with respect to \ns, which is expected. However, the proposed framework
shows a vastly superior performance being up to five orders of magnitude faster
than the \ac{MC} alternative.

It is worth noting that the observed speedups are due to two major reasons.
First of all, \ac{PC} expansions are generally superior to \ac{MC} sampling when
the curse of dimensionality is suppressed \cite{eldred2008, xiu2010}, which we
accomplish by model order reduction and efficient integration schemes. The
second reason is the particular technique used in the framework for solving the
temperature model and constructing \ac{PC} expansions in a stepwise manner shown
in \eref{chaos-recurrence}.
