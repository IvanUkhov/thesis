In this chapter, we shift our attention from analyzing process variation to
designing with process variation. In other words, instead of analyzing the
original variability, we now study its impact on higher-level characteristics so
that this deteriorating impact can be taken into account in the final design.

\section{Introduction}

As it is emphasized in \sref{inference-introduction}, process variation is a
major concern of the designer. In order to assist the designer, we develop a
framework that allows the designer to propagate the corresponding uncertainty
through the system under development and investigate its impact on the system's
behavior.

We proceed as follows. A motivational example is considered in
\sref{analog-example}. The objective of our study is formulated in
\sref{analog-problem}. \sref{analog-prior} provides an overview of the prior
work. The proposed framework is presented in \sref{analog-solution}. An
exemplary application of our approach is discussed in \sref{analog-application},
and the corresponding experimental results are given and compared with \ac{MC}
simulations in \sref{analog-results}. \sref{analog-conclusion} concludes the
chapter.

\section{Motivational Example}
\slab{analog-example}

\inputfigure{analog-example}
Consider a quad-core architecture exposed to the uncertainty due to parameters
that affect the leakage current. Assume first that these parameters have nominal
values. We can then simulate the system under a certain workload and observe the
corresponding temperature profile; the experimental setup is to be detailed in
\sref{analog-example} and \sref{analog-results}. The result is depicted in
\fref{analog-example} by a blue line, which corresponds to the temperature of
one of the processors. It can be seen that the temperature is always below
\celsius{90}. Let us now assume a mild deviation of the parameters from the
nominal values and simulate once again. The result is the orange line in
\fref{analog-example}; the maximum temperature is approaching \celsius{100}.
Finally, we repeat the experiment considering a severe deviation of the
parameters and observe the yellow line in \fref{analog-example}; the maximum
temperature is almost \celsius{110}. Suppose that the designer is tuning a
solution constrained by a maximum temperature of \celsius{90} and is guided
exclusively by the nominal parameters. In this case, even with mild deviations,
the circuits might be burnt. Another path that the designer can take is to
design the system for severe conditions. In this scenario, however, the system
might easily become too conservative, overdesigned. Consequently, such
uncertainty has to be addressed in order to pursue efficiency and robustness.
Nevertheless, the majority of the literature related to power and temperature
analysis of electronic systems ignores this important aspect; see, for instance,
\cite{rao2009, rai2011, thiele2011}. This negligence is also present in the
analysis and optimization described in \cref{design-certainty}, and the goal of
this chapter is to eliminate it in the case of the uncertainty due to process
variation.

\section{Problem Formulation}
\slab{analog-problem}

Let $(\Omega, \mathcal{F}, \probability)$ be a probability space as it defined
in \sref{probability-theory} and assume the same system model as the one
described in \sref{system-model}. The system depends on \nu process parameters
that are uncertain at the design stage. These parameters are denoted by a random
vector $\vu: \Omega \to \real^\nu$. Once the fabrication process yields a
particular outcome, \vu takes potentially different values across each
fabricated chip and stays unchanged thereafter. This variability leads to
deviations of power from the nominal values and, therefore, to deviations of
temperature from the one corresponding to the nominal power consumption.

The goal is to develop a system-level framework for transient temperature and,
hence, power analysis of electronic systems where the power consumption and heat
dissipation are stochastic due to their dependency on the uncertain parameters
\vu. The user is required to specify \one~the probability distribution of \vu
and \two~the dependency of the power consumption on \vu, which can be given as a
``black box.'' The framework is to provide the user with the tools for analyzing
the system under a given workload, without imposing constraints on the nature of
this workload, and obtaining the corresponding stochastic power \mp and
temperature \mq profiles with a desired level of accuracy and at low costs.

\section{Prior Work}
\slab{analog-prior}

Since the appearance of the first digital computers in 1940s, \acf{MC} sampling
remains one of the most well-known and widely used methods for the analysis of
stochastic systems. The reason for this popularity lies in the ease of
implementation, in the independence of the stochastic dimensionality of the
problem at hand, and in the fact that, by the law of large numbers
\cite{durrett2010}, the quantities estimated using \ac{MC} sampling
asymptotically approach the true values. The main problem with \ac{MC} sampling,
however, is the low rate of convergence: the error decreases as $\no^{-1/2}$
where \no is the number of drawn samples. This means that an additional decimal
point of accuracy requires hundred times more samples. Each such sample implies
a complete realization of the whole system, which renders \ac{MC}-based methods
slow and often unfeasible since the needed number of simulations can be
extremely large \cite{diaz-emparanza2002}. There are other sampling techniques
that have better convergence rates than the one of the classical \ac{MC}
sampling such as quasi-\ac{MC} sampling; however, due to additional
restrictions, their applicability is often limited \cite{xiu2010}.

In order to overcome the limitations of deterministic techniques and, at the
same time, to eliminate or, at least, mitigate the computation costs associated
with \ac{MC} sampling, a number of stochastic techniques have been introduced,
which we elaborate on in what follows. We are particularly interested in power
and temperature and, therefore, shape the exposition accordingly. Since the
static component of the total power dissipation is influenced by process
variation the most, which is due to the leakage current, the techniques
discussed below primarily focus on the variability of this component.

A solely power-targeted but temperature-aware solution is proposed in
\cite{chandra2010} where the driving force of the analysis is \ac{MC} sampling
with partially precomputed data. A learning-based approach is presented in
\cite{juan2011} in order to estimate the maximum temperature under the static
steady-state condition; recall \sref{static-steady-state}. Temperature-related
issues originating from process variation are also considered in \cite{juan2012}
where a statistical model of the static steady-state temperature based on
Gaussian distributions is derived. A statistical simulator of the static
steady-state temperature is developed in \cite{huang2009a} using \ac{PC}
expansions and the \acf{KL} decomposition; see
\sref{probability-transformation}. A \ac{KL}-aided stochastic collocation
\cite{xiu2010} approach to static steady-state temperature analysis is presented
in \cite{lee2013}. In \cite{shen2009}, \ac{PC} expansions are employed in order
to estimate the leakage power of the entire chip. The \ac{KL} decomposition is
utilized in \cite{bhardwaj2006} for leakage calculations. In
\cite{bhardwaj2008}, the total leakage is quantified using the \ac{PC} and
\ac{KL} methods. The same combination of tools is employed in
\cite{vrudhula2006} and \cite{ghanta2006} in order to analyze the response of
interconnect networks and power grids, respectively, under process variation.

The last five of the aforementioned techniques, that is, \cite{bhardwaj2006,
vrudhula2006, ghanta2006, bhardwaj2008, shen2009}, perform only stochastic power
analysis and ignore the interdependence between power and temperature described
in \sref{power-model}. The others are temperature-related approaches, but none
of them attempts to tackle stochastic transient temperature analysis and to
compute the probability distribution of temperature that evolves over time.
However, such transient curves are of practical importance. First, certain
procedures cannot be undertaken without the knowledge of time-dependent
temperature variations; an example is reliability optimization based on the
thermal-cycling fatigue, which is discussed in
\sref{thermal-cycling-exploration}. Second, the static steady-state assumption
considered, for instance, in \cite{huang2009a, juan2011, juan2012, lee2013} can
rarely be justified since power profiles are not invariant in reality. In
addition, one frequently encounters the assumption that power and temperature
follow \emph{a priori} known probability distributions; Gaussian and log-normal
distributions are popular choices as in \cite{bhardwaj2006, srivastava2010,
juan2012}. However, this assumption often fails in practice---which is also
noted in \cite{juan2012} regarding the normality of the leakage current---due to
\one~the nonlinear dependence of power on process parameters and \two~the
nonlinear interdependence between power and temperature. In order to illustrate
this, we simulate $10^4$ times the example given in \sref{analog-example}
assuming the widespread Gaussian model of the effective channel length and apply
the Jarque--Bera test of normality to the collected temperature directly as well
as after processing them with the log transformation. The null hypothesis that
the data are from an unspecified Gaussian distribution is firmly rejected in
both cases at the significance level of 5\%. Therefore, the two distributions
are neither Gaussian nor log-normal, which can also be seen in
\fref{analog-example-density} described in \sref{analog-results}.

To conclude, the prior techniques for stochastic power and temperature analysis
are restricted in use due to one or several of the following traits: based on
\ac{MC} simulations (potentially slow) \cite{chandra2010}, limited to power
analysis \cite{bhardwaj2006, ghanta2006, vrudhula2006, bhardwaj2008, shen2009,
chandra2010}, ignoring the power-temperature interplay \cite{bhardwaj2006,
ghanta2006, vrudhula2006, bhardwaj2008, huang2009a, shen2009}, limited to the
static steady-state temperature \cite{huang2009a, juan2011, juan2012, lee2013},
exclusive focus on the maximum temperature \cite{juan2011}, and \emph{a priori}
chosen distributions of power and temperature \cite{bhardwaj2006,
srivastava2010, juan2012}. Consequently, there is a lack of flexible techniques
for stochastic power and temperature analysis.

\section{Our Solution}
\slab{analog-solution}

We develop a framework for the analysis of the transient power and temperature
profiles of electronic systems subject to the uncertainty due to process
variation. The proposed technique is flexible in modeling diverse probability
distributions, specified by the user, of the uncertain parameters, such as the
effective channel length and gate oxide thickness. Moreover, there are no
assumptions on the distributions of the resulting power and temperature traces
as these distributions are unlikely to be known \emph{a priori}. The proposed
technique is capable of capturing arbitrary joint effects of the uncertain
parameters on the system since the impact of these parameters is introduced into
the framework as a ``black box,'' which is also defined by the user. In
particular, it allows for the power-temperature interplay to be taken into
account with no effort. Our approach is founded on the basis of \acf{PC}
expansions, which constitute an attractive alternative to \ac{MC} sampling. This
is due to the fact that \ac{PC} expansions possess much faster convergence
properties and provide succinct and intuitive representations of system
responses to stochastic inputs. In addition, we illustrate the framework
considering one of the most important parameters affected by process variation,
namely, the effective channel length. Note, however, that our approach is not
bound to any particular source of variability and, apart from the effective
channel length, can be applied to other process parameters such as the gate
oxide thickness.

The main idea is to construct a surrogate model for the joint power and
temperature model of the system using \ac{PC} expansions. Having constructed
this surrogate, such quantities as the \acf{CDF} and \acf{PDF} can be easily
estimated. Moreover, the representation that we compute provides analytical
formulae for probabilistic moments, which means that the expectation and
variance are readily available.

\inputfigure{analog-overview}
The major stages of our technique are depicted in \fref{analog-overview}. At
Stage~1 (\sref{analog-probability-transformation}), the uncertain parameters \vu
are transformed into independent random variables \vz since independence is a
prerequisite of the \ac{PC} approach. At Stage~2 (\sref{analog-power-model}),
the user specifies the power model of the system via a ``black-box'' function
$f$ that computes the total power \vp for a particular temperature \vq and a
particular outcome of the parameters \vu. At Stage~3
(\sref{analog-temperature-model}), based on \sref{transient-state-solution}, a
mathematical description of the thermal behavior of the system is obtained. At
Stage~4 (\sref{analog-surrogate-model}), the surrogate model is computed by
traversing the desired time span and gradually constructing \ac{PC} expansions
of the stochastic power and temperature profiles. At Stage~5
(\sref{analog-post-processing}), the computed \ac{PC} expansions are analyzed in
order to obtain the needed characteristics of the system such as \acp{PDF} and
moments.

Let us now detail each of the five stages of the proposed framework.

\subsection{Probability Transformation}
\slab{analog-probability-transformation}

Independence is required by \ac{PC} expansions. In general, the \nu individual
variables in \vu are dependent and, therefore, should be preprocessed in order
to fulfill the requirement. To this end, an adequate probability transformation
should be undertaken \cite{eldred2008}. Denote such a transformation by $\vu =
\transform{\vz}$ where $\vz: \Omega \to \real^\nz$ is a random vector with \nz
mutually independent components.

Correlated random variables can be transformed into linearly uncorrelated ones
via the \ac{KL} decomposition given in \eref{karhunen-loeve}. If, in addition,
the correlated variables form a Gaussian vector, the uncorrelated ones are also
mutually independent. In the general case (non-Gaussian), the most prominent
solutions to attain independence are the Rosenblatt \cite{rosenblatt1952} and
Nataf transformations \cite{liu1986}. Rosenblatt's approach is suitable when the
joint distribution function of \vu is known; however, such information is rarely
available. A set of marginal distributions and a correlation matrix are more
likely to be given, which are already sufficient for the Nataf transformation.
See \sref{probability-transformation} for more detail.

Apart from the extraction of the independent variables \vz, an essential
operation at this stage is model order reduction since the number of stochastic
dimensions directly impacts the complexity of the rest of the computations. The
intuition is that, due to the correlations between the random variables in \vu,
some of them can be harmlessly replaced by combinations of the rest, leading to
a random vector \vz with fewer dimensions. This operation is often treated as a
part of the \ac{KL} decomposition, which is also covered in
\sref{probability-transformation}.

\subsection{Power Model}
\slab{analog-power-model}

As stated in \sref{analog-problem}, the user is supposed to decide on the power
model for the system, which we denote as follows:
\begin{equation} \elab{analog-power-model}
  \vp(t) = f(t, \vq, \vu)
\end{equation}
where $f$ is a function that evaluates the power $\vp \in \real^\np$ of the
processing elements given their temperature $\vq \in \real^\np$ and parameters
$\vu \in \real^\nu$. It should be understood that \vp, \vq, and \vu are random
vectors in general, and that $f$ consumes $\vq(t, \omega)$ and $\vu(\omega)$ and
yields $\vp(t, \omega)$ for some particular outcome $\omega \in \Omega$.

The user can choose any $f$. It can be, for instance, a closed-form formula or a
piece of code. The only assumption we make about $f$ is that it is smooth in \vz
and belongs to $\L{2}(\Omega, \mathcal{F}, \probability)$ when it is viewed as a
random variable (see \sref{probability-theory}), which is generally applicable
to most physical systems \cite{xiu2010}. Note also that the operation performed
by this ``black box'' is purely deterministic. The definition of $f$ is flexible
enough to account for such effects as the interdependence between power and
temperature discussed in \sref{power-model}.

\subsection{Temperature Model}
\slab{analog-temperature-model}

The temperature model is based on the one described in \sref{temperature-model},
and transient temperature analysis is based on the one presented in
\sref{transient-state-solution}. The major difference is that
\eref{temperature-model} implicitly operates on stochastic quantities.
Consequently, the recurrent solution in \eref{transient-state-recurrence}, that
is,
\begin{equation} \elab{analog-recurrence-original}
  \vs_i = \m{E} \vs_{i - 1} + \m{F} \vp_i
\end{equation}
for $i = \range{1}{\ns}$ where $\vs_0 = \v{0}$, is such as well. In the
deterministic case, it can be readily employed to perform deterministic
transient power and temperature analysis via the techniques in
\sref{power-temperature-interplay}. In the stochastic case, however, the
situation is substantially different since $\vp_i$ and, consequently, $\vs_i$
and $\vq_i$ are probabilistic quantities. Moreover, at each step, $\vp_i$ is an
arbitrary transformation of the uncertain parameters \vu and stochastic
temperature $\vq_i$, which results in a multivariate random variable with a
generally unknown probability distribution. Furthermore, $\vp_i$, $\vq_i$,
$\vs_i$, and \vu are dependent random vectors since the first three are
functions of the last. Consequently, the operations involved in the recurrence
are to be performed on dependent random vectors with arbitrary probability
distributions, which, in general, have no closed-form solutions, which we
address in the following subsection.

\subsection{Surrogate Model}
\slab{analog-surrogate-model}

The goal now is to transform the recurrence in \eref{analog-recurrence-original}
in such a way that it becomes possible to estimate the probability distributions
of power and temperature. Our solution is the construction of a surrogate model
for the power function $f$, which we then propagate through the recurrence in
order to obtain an approximation for power and an approximation for temperature.
To this end, we employ \ac{PC}, which is introduce in \sref{polynomial-chaos}.

The first step toward a polynomial expansion is the choice of a suitable
polynomial basis, which is typically made based on the Askey scheme of
orthogonal polynomials \cite{xiu2010}. The step is crucial as the rate of
convergence of \ac{PC} expansions closely depends on it. Although there are no
strict rules that guarantee the optimal choice \cite{knio2006}, there are best
practices saying that one should be guided by the probability distributions of
the random variables that drive the stochastic system at hand. For instance,
when a random variable follows a beta distribution, the Jacobi basis is worth
being tried first; on the other hand, the Hermite basis is preferable for
Gaussian distributions.

Having chosen an appropriate basis, we apply the \ac{PC} formalism to the power
term $\vp_i$ in \eref{analog-recurrence-original} as described in
\eref{chaos-expansion}. The result is
\[
  \chaos{\nz}{\lc}{\vp_i}(\vz) = \sum_{j = 1}^\nc \hat{\vp}_{ij} \psi_j(\vz)
\]
where $\{ \psi_j: \real^\nz \to \real \}_{j = 1}^\nc$ are the first \nc
polynomials of the basis; $\{ \hat{\vp}_{ij} \in \real^\np \}_{j = 1}^\nc$ are
the coefficients of the expansion; and \nc is given by
\eref{chaos-length-total-order}.

It can be seen in \eref{analog-recurrence-original} that, due to the linearity
of the operations involved in the recurrence, $\vs_i$ attains an expansion that
has the same structure as the one of $\vp_i$, and the recurrence can be
rewritten as follows:
\[
  \chaos{\nz}{\lc}{\vs_i}(\vz) = \m{E} \, \chaos{\nz}{\lc}{\vs_{i - 1}}(\vz) + \m{F} \, \chaos{\nz}{\lc}{\vp_i}(\vz)
\]
for $i = \range{1}{\ns}$. Consequently, there are two concurrent \ac{PC}
expansions: one is for power, and one is for temperature; the two expansions
have the same basis but different coefficients. In order to understand the
structure of the above formula better, let us spell it out as
\[
  \sum_{j = 1}^\nc \hat{\vs}_{ij} \psi_j(\vz) =
  \sum_{j = 1}^\nc \left( \m{E} \, \hat{\vs}_{i - 1, j} + \m{F} \, \hat{\vp}_{ij} \right) \psi_j(\vz).
\]
Multiplying the above equation by each polynomial from the basis and making use
of the orthogonality property in \eref{polynomial-orthogonality}, we obtain the
following recurrence:
\begin{equation} \elab{analog-recurrence}
  \hat{\vs}_{ij} = \m{E} \hat{\vs}_{i - 1, j} + \m{F} \hat{\vp}_{ij}
\end{equation}
for $i = \range{1}{\ns}$ and $j = \range{1}{\nc}$. Combining the recurrence with
\eref{temperature-algebraic}, we obtain the coefficients of the \ac{PC}
expansion of the temperature vector $\vq_k$.

Let us now find the coefficients $\{ \hat{\vp}_{ij} \}_{i = 1, j = 1}^{\ns,
\nc}$ of the \ac{PC} expansion of power, which will subsequently be propagated
to temperature. The coefficients are computed using spectral projections as
shown in \eref{chaos-projection}. The inner product in the denominator of
\eref{chaos-projection} does not pose any computation problem: it is a
normalization constant that can be considered given along with the basis. The
inner product in the enumerator, however, should generally be evaluated
numerically, which is described in \sref{numerical-integration}. To this end, an
adequate quadrature $\quadrature{\nz}{\lq}$ should be used. The result is
\begin{equation} \elab{analog-coefficient}
  \hat{\vp}_{ij} = \frac{\quadrature{\nz}{\lq}{\vp_i \psi_j}}{\innerproduct{\psi_j}{\psi_j}}
\end{equation}
for $i = \range{1}{\ns}$ and $j = \range{1}{\nc}$ where the integration should
be understood element-wise. It is worth emphasizing that, since power depends on
temperature as shown in \sref{power-model}, at each step of the recurrence in
\eref{analog-recurrence}, the computation of $\hat{\vp}_{ij}$ should be done
with respect to the \ac{PC} expansion of $\vq_{i - 1}$.

The construction process of the stochastic power and temperature profiles is
estimated to have the following time complexity per time step:
\[
  \bigo{\nn \nn \nc + \nn \np \nq \nc + \nq f(\np)}
\]
where \nn, \np, \nc, and \nq are the number of thermal nodes, processing
elements, polynomial terms, and quadrature points, respectively,
and $f(\np)$ denotes the contribution of the power model in
\eref{analog-power-model}. The expression can be detailed further by expanding
\nc and \nq. The formula of \nc is given in \eref{chaos-length-total-order},
which behaves as $\nz^\nc / \nc!$ in the limit with respect to \nz. For
quadrature rules based on the full tensor product, $\log(\nq) \propto \nz$,
which means that the dependency of \nq on \nz is exponential. It can be seen
that the theory of \ac{PC} expansions suffers from the curse of dimensionality
\cite{eldred2008, xiu2010}. Concretely, when \nz increases, the number of
polynomial terms as well as the complexity of the corresponding coefficients
exhibit a growth, which is exponential without special treatments. The problem
does not have a general solution and is one of the central topics of many
ongoing studies. We mitigate this issue by \one~keeping the number of stochastic
dimensions low using the model order reduction based on the \ac{KL}
decomposition discussed in \sref{probability-transformation} and \two~utilizing
efficient integration techniques discussed in \sref{numerical-integration}. In
particular, for sparse integration grids based on Gaussian quadrature rules,
$\log(\nq) \propto \log(\nz)$, which means that the dependency of \nq on \nz is
only polynomial \cite{heiss2008}.

Let us summarize this subsection. Recall the stochastic recurrence in
\eref{analog-recurrence-original} where, in the presence of dependencies, an
arbitrary functional $\vp_k$ of the uncertain parameters \vu and stochastic
temperature $\vq_k$---which can be seen in \eref{analog-power-model}---needs to
be evaluated and combined with another random vector $\vs_k$. This recurrence in
\eref{analog-recurrence-original} has been replaced with a purely deterministic
recurrence in \eref{analog-recurrence}. More globally, the whole system
including the heavy temperature model in \eref{temperature-model-original} has
been substituted with a light surrogate defined by a set of polynomials $\{
\psi_j \}_{j = 1}^\nc$ in \vz, a set of coefficients $\{ \hat{\vp}_{ij} \}_{i =
1, j = 1}^{\ns, \nc}$ for power, and a set of coefficient $\{ \hat{\vq}_{ij}
\}_{i = 1, j = 1}^{\ns, \nc}$ for temperature. These quantities constitute a
stochastic power profile \mp and a stochastic temperature profile \mq, and these
profiles are ready to be analyzed, which we discuss in the following subsection.

Before we proceed, let us draw attention to the ease and generality of taking
uncertainty into consideration using the proposed approach. The above
description is delivered from any explicit formula of any particular uncertain
parameter. In contrast, the solutions from the literature related to process
variation are typically based on ad~hoc expressions and should be tailored by
the user for each new parameter individually; see, for instance,
\cite{ghanta2006, bhardwaj2008, huang2009a}. Our framework provides a great
flexibility in this regard.

\subsection{Post-Processing}
\slab{analog-post-processing}

Due to the properties of \ac{PC} expansions---in particular, due to the mutual
orthogonality of the basis functions discussed in \sref{polynomial-chaos}---the
obtained polynomial representations allow for various prospective analyses to be
performed with no effort. For instance, consider the expansion of temperature at
step $i$
\[
  \chaos{\nz}{\lc}{\vq_i}(\vz) = \sum_{j = 1}^{\nc} \hat{\vq}_{ij} \psi_j(\vz)
\]
where $\hat{\vq}_{ij}$ are computed using \eref{analog-recurrence} and
\eref{temperature-algebraic}. Let us, for example, find the expectation and
variance of the temperature at this time moment. Since the first polynomial
$\psi_1$ in a polynomial basis is unity by definition \cite{xiu2010},
$\expectation{\psi_1(\vz)} = 1$. Therefore, using the orthogonality property in
\eref{polynomial-orthogonality}, we conclude that $\expectation{\psi_i(\vz)} =
0$ for $i = \range{2}{\nc}$. Consequently, the expected value and variance have
the following simple expressions solely based on the coefficients:
\[
  \begin{split}
    & \expectation{\vq_i} = \hat{\vq}_{i1} \text{ and} \\
    & \variance{\vq_i} = \sum_{j = 2}^{\nc} \innerproduct{\psi_j}{\psi_j} \hat{\vq}_{ij}^2,
  \end{split}
\]
respectively, where the squaring should be understood element-wise. Such
quantities as \acp{CDF}, \acp{PDF}, and probabilities of certain events can be
estimated by means of \ac{MC} sampling applied to the expansion. In this case,
each sample is a trivial evaluation of a polynomial, and, hence, \ac{MC}
sampling is readily adequate. Furthermore, global and local sensitivity analysis
of deterministic and stochastic quantities can be readily conducted on a \ac{PC}
expansion.

\section{Illustrative Application}
\slab{analog-application}

So far we have not made any assumptions regarding the cause of variability. In
this section, we consider a particular application of the proposed framework in
order to illustrate how it is supposed to be used in practice.

As discussed in \sref{power-model}, the total dissipation of power is composed
of two major parts: dynamic and static. The influence of process variation on
the dynamic power is known to be negligibly small \cite{srivastava2010}; on the
other hand, the variability of the static power is substantial, in which the
subthreshold leakage current contributes the most \cite{juan2011, juan2012}.
Hence, we focus on the subthreshold leakage and, more specifically, on the
effective channel length denoted by \u since it has the strongest influence on
the leakage current and is severely deteriorated by process variation
\cite{chandrakasan2000}. In particular, \u also affects the threshold voltage.

\inputfigure{beta-gaussian}
It is well known that the dispersion of the effective channel length around its
nominal value resembles the bell shape of Gaussian distributions. Therefore,
such variations are often conveniently modeled using Gaussian random variables
\cite{bhardwaj2006, ghanta2006, huang2009a, shen2009, chandra2010,
srivastava2010, juan2011, juan2012, lee2013}. Due to both the underlying physics
and demonstration purposes, we make a step further and bake into the model the
fact that the effective channel length---occupying the space between the drain
and source of a transistor---cannot be arbitrarily large or take a negative
value, which Gaussian distributions allow it to do. In other words, we require
the model of \u to have a bounded support. With this in mind, we propose to
model the effective channel length and other physically-bounded parameters using
the four-parametric family of beta distributions as
\[
  \u \sim \mathrm{Beta}(a, b, c, d)
\]
where $a$ and $b$ are the shape parameters, and $c$ and $d$ are the left and
right bounds of the support, respectively. The parameters $a$ and $b$ can be
chosen so that the typically found bell shape of the distribution is preserved.
An illustration is given in \fref{beta-gaussian} where a beta distribution is
fitted to the standard Gaussian distribution; alternatively, one can match
probabilistic moments. It can be seen that the curves are nearly
indistinguishable, but the beta one has a bounded support $[-4, 4]$, which can
potentially lead to more realistic models.

The variability of $u$ is split into global $u_\overall$ and local $u_\local$
parts \cite{shen2009, chandra2010}. Without loss of generality, $u_\overall$ can
be treated as a composition of independent inter-lot, inter-wafer, and inter-die
variations; likewise, $u_\local$ can be treated as a composition of independent
and dependent local variations. The variability $u_\overall$ is assumed to be
shared by all the \np processing elements whereas each processing element has
its own local parameter $u_{\local, i}$. Therefore, the effective channel length
of processing element $i$ is modeled as follows:
\[
  u_i = u_\nominal + u_\overall + u_{\local, i}
\]
where $u_\nominal$ is the nominal value of the effective channel length. Hence,
the uncertain parameters of the problem are
\[
  \vu = (u_{\local, 1}, \dotsc, u_{\local, \np}, u_\overall): \Omega \to \real^{\np + 1}.
\]

Global variations are typically assumed to be uncorrelated with respect to the
local ones. The latter, however, are known to have high spatial correlations.
Similar to \cref{analysis-analog-uncertainty}, these correlations are modeled
here using the correlation function given in \eref{inference-correlation}. This
choice is guided by the observations of the correlation structured induced by
the manufacturing process \cite{friedberg2005, chandrakasan2000, cheng2011}:
$k_\SE$ imposes similarities between the spatial locations that are close to
each other, and $k_\OU$ imposes similarities between the locations that are at
the same distance from the center of the die; see also \cite{ghanem1991,
ghanta2006, bhardwaj2008, huang2009a, lee2013}.

Although \eref{inference-correlation} captures certain features inherent to the
manufacturing process, it is still an idealization. In practice, it can be
difficult to make a justifiable choice and tune such a formula, which is a
prerequisite for the techniques that are discussed in \sref{analog-prior} and
based on the continuous \ac{KL} decomposition. A correlation matrix, on the
other hand, can readily be estimated from measurements and, thus, is a more
probable input to probabilistic analysis. Therefore, we use
\eref{inference-correlation} with the only purpose of constructing a correlation
matrix of $\{ u_{\local, i} \}_{i = 1}^\np$. For convenience, the resulting
matrix is extended by one dimension in order to accommodate $u_\overall$ along
with $\{ u_{\local, i} \}_{i = 1}^\np$. In this case, the correlation matrix
obtains one additional nonzero diagonal element equal to unity; the result is
the correlation matrix of \vu denoted by $\correlation{\vu}$.

To conclude, the input to our analysis is composed of the marginal distributions
of the uncertain parameters \vu, which are beta distributions, and the
corresponding correlation matrix $\correlation{\vu}$. Let us now go over the
stages of the proposed framework discussed in \sref{analog-solution} and
depicted in \fref{analog-overview}.

\subsection{Probability Transformation}

At Stage~1 in \fref{analog-overview}, \vu should be preprocessed in order to
extract a vector of mutually independent random variables denoted by \vz.
Following the guidance given in \sref{analog-probability-transformation}, the
most suitable transformation for the ongoing scenario is the one given in
\eref{probability-transformation}, which we denote here by
\[
  \vu \approx \transform{\vz}.
\]

Since the number of stochastic dimensions, which is $\nu = \np + 1$ for \vu,
directly impacts the computational cost of \ac{PC} expansions, which is
elaborated on in \sref{analog-surrogate-model}, one should consider a
possibility for model order reduction before constructing \ac{PC} expansions. To
this end, the model order reduction described in
\sref{probability-transformation} is an essential part of and assumed to be
active in the above transformation. The reduce dimensionality is denoted by \nz.

The marginal distributions of \vz can be arbitrary as long as one can construct
a suitable polynomial basis as described in \sref{analog-surrogate-model}. We
let \vz have beta distributions, staying in the same family of distributions
with \vu.

\subsection{Power Model}

At Stage~2 in \fref{analog-overview}, we have to decide on the power model with
the identified uncertain parameters as an input. To this end,
\eref{analog-power-model} is decomposed into the sum of the dynamic and static
components
\[
  f(t, \vq, \vu) = f_\dynamic(t) + f_\static(\vq, \vu)
\]
where, as motivated earlier, $f_\dynamic$ does not depend on \vu. We assume that
the desired workload of the system is given as a dynamic power profile denoted
by $\mp_\dynamic$. Similar to \sref{inference-results}, without loss of
generality, the development of the static part is based on \up{SPICE}
simulations of a reference electrical circuit composed of \up{BSIM4} devices
\cite{bsim} configured according to the 45-nm \up{PTM} model \cite{ptm}.
Specifically, we use a series of \up{CMOS} invertors for this purpose. The
simulations are performed for a sufficiently wide fine-grained two-dimensional
grid, the effective channel length and temperature, and the results are
tabulated. An interpolation technique is then utilized whenever we need to
evaluate the static power for a particular point within the range of the grid.

\subsection{Temperature Model}

At Stage~3 in \fref{analog-overview}, the temperature model is to be detailed.
More concretely, we need to provide an adequate thermal \up{RC} circuit. Given
the specification of the platform at hand---including the floorplan of the die
and the configuration of the thermal package---such a circuit is constructed by
means of HotSpot \cite{skadron2003}; the structure is the default one described
in \sref{temperature-model}. Finally, using the solution technique presented in
\sref{transient-state-solution}, the two coefficient matrices $\m{E}$ and
$\m{F}$ used in the recurrence in \eref{analog-recurrence} are calculated.

\subsection{Surrogate Model}

At Stage~4 in \fref{analog-overview}, the uncertain parameters, power model, and
temperature model developed in the previous subsections are to be fused together
under the desired workload $\mp_\dynamic$ in order to produce the corresponding
stochastic power and temperature profiles denoted by \mp and \mq, respectively.

In the current scenario, the construction of \ac{PC} expansions is based on the
Jacobi polynomial basis as it is preferable in situations involving
beta-distributed parameters \cite{xiu2010}. To give an example, for a dual-core
platform, that is, $\np = 2$, with two stochastic dimensions, that is, $\nz =
2$, the second-order PC expansion, that is, $\nc = 2$, of temperature at step
$i$ is as follows:
\[
  \begin{split}
    \chaos{2}{2}{\vq_i}(\vz)
    =    {} & \hat{\vq}_{i1} \, \psi_1(\vz) + \hat{\vq}_{i2} \, \psi_2(\vz) + \hat{\vq}_{i3} \, \psi_3(\vz) \\
    {} + {} & \hat{\vq}_{i4} \, \psi_4(\vz) + \hat{\vq}_{i5} \, \psi_5(\vz) + \hat{\vq}_{i6} \, \psi_6(\vz)
  \end{split}
\]
where the coefficients $\{ \hat{\vq}_{ij} \}_{j = 1}^6$ are vectors with two
elements corresponding to the two processing elements. Regarding the basis,
\begin{align*}
  & \psi_1(\vz) = 1, \\
  & \psi_2(\vz) = 2 z_1, \\
  & \psi_3(\vz) = 2 z_2, \\
  & \psi_4(\vz) = 4 z_1 z_2 \\
  & \psi_5(\vz) = \frac{15}{4} z_1^2 - \frac{3}{4}, \text{ and} \\
  & \psi_6(\vz) = \frac{15}{4} z_2^2 - \frac{3}{4}.
\end{align*}
The Jacobi polynomials have two parameters \cite{xiu2010}, and the ones shown
above correspond to the case where both parameters are equal to two. Such a
series might be shorter or longer depending on the accuracy requirements given
by \lc. The expansion for power has the same structure but different
coefficients.

The next step is to compute the coefficients $\{ \hat{\vp}_{ij} \}_{j = 1}^\nc$
in \eref{analog-recurrence}, which subsequently yield $\{ \hat{\vq}_{ij} \}_{j =
1}^\nc$. As shown in \sref{polynomial-chaos}, these computations involve
multidimensional integration with respect to the distribution of \vz, which
should be done numerically using a quadrature rule; recall
\sref{analog-surrogate-model}. When beta distributions are concerned, a natural
choice of such a rule is the Gauss--Jacobi quadrature; see
\sref{numerical-integration}. Then, using \eref{quadrature-summation}, the
coefficient are computed as shown in \eref{analog-coefficient}. The
normalization constants $\{ \innerproduct{\psi_j}{\psi_j} \}_{j = 1}^\nc$ are
computed exactly either by applying the same quadrature rule or by taking
products of the one-dimensional counterparts with known analytical expressions
\cite{xiu2010}; the result is tabulated. It is important to note that \lq should
be chosen in such a way that the rule is exact for polynomials of a total order
up to $2 \lc$, that is, twice the order of \ac{PC} expansions, which can be seen
in \eref{chaos-projection} \cite{eldred2008}. Therefore, $\lq \geq \lc + 1$ as
the quadrature is Gaussian.

To summarize, we have completed four out of five stages of the proposed
framework depicted in \fref{analog-overview}. The result is a light surrogate
for the entire system. At each time step, the surrogate is composed of two
\np-valued polynomials---one is for power, and one is for temperature---which
are defined in terms of \nz mutually independent random variables.

\subsection{Post-Processing}

\inputfigure{analog-example-density}
At Stage~5 in \fref{algorithm}, it can be seen in, for example,
\eref{pc-example} that the surrogate model has a negligibly small computational
cost at this stage: for any outcome of the parameters $\vz \equiv \vz(\omega)$,
we can easily compute the corresponding temperature by plugging in \vz into
\eref{pc-example}; the same applies to power. Hence, the constructed
representation can be trivially analyzed to retrieve various statistics about
the system in \eref{fourier-system}. Let us illustrate a few of them still
retaining the example in \eref{pc-example}. Assume that the dynamic power
profile $\mp_\dynamic$ corresponding to the considered workload is the one shown
in \fref{application-power}. Having constructed the surrogate with respect to
this profile, we can then rigorously estimate, say, the PDF of temperature at
some $k$th moment of time by sampling the surrogate and obtain curves similar to
those shown \fref{experimental-results-pdf} (discussed in
\sref{experimental-results}). Furthermore, the expectation and variance of
temperature are trivially calculated using the formulae in \eref{pc-moments}
where $\nc = 6$. For the whole time span of the power profile $\mp_\dynamic$
depicted in \fref{application-power}, these quantities are plotted in
\fref{application-temperature}. The displayed curves closely match those
obtained via MC simulations with $10^4$ samples; however, our method takes less
than a second whilst MC sampling takes more than a day as we shall see next.

\section{Experimental Results}
\slab{analog-results}

In this section, we report the results of the proposed framework for different
configurations of the illustrative example in \sref{illustrative-example}. All
the experiments are conducted on a GNU/Linux machine with Intel Core i7 2.66~GHz
and 8~GB of RAM.

Now we shall elaborate on the default configuration of our experimental setup,
which, in the following subsections, will be adjusted according to the purpose
of each particular experiment. We consider a 45-nanometer technological process.
The effective channel length is assumed to have a nominal value of
$17.5\,\text{nm}$ \cite{ptm} and a standard deviation of $2.25\,\text{nm}$ where
the global and local variations are equally weighted. Correlation matrices are
computed according to \eref{correlation-function} where the length-scale
parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of the square die.
In the model order reduction technique (see \sref{ie-uncertain-parameters}), the
threshold parameter is set to 0.99 preserving 99\% of the variance of the data.
Dynamic power profiles involved in the experiments are based on simulations of
randomly generated applications defined as directed acyclic task
graphs.\footnote{In practice, dynamic power profiles are typically obtained via
an adequate simulator of the architecture of interest.} The floorplans of the
platforms are constructed in such a way that the processing elements form
regular grids.\footnote{The task graphs of the applications, floorplans of the
platforms, configuration of HotSpot, which was used to construct thermal RC
circuits for our experiments, are available online at \cite{eslab2014}.} The
time step of power and temperature traces is set to $1\,\text{ms}$ (see
\sref{problem-formulation}), which is also the time step of the recurrence in
\eref{pc-recurrence}. As a comparison to our polynomial chaos (PC) expansions,
we employ Monte Carlo (MC) sampling. The MC approach is set up to preserve the
whole variance of the problem, that is, no model order reduction, and to solve
\eref{fourier-system} directly using the Runge-Kutta formulae (the
Dormand-Prince method) available in MATLAB \cite{matlab}.

Since the temperature part of PTA is the main contribution of this work, we
shall focus on the assessment of temperature profiles. Note, however, that the
results for temperature allow one to implicitly draw reasonable conclusions
regarding power since power is an intermediate step towards temperature, and any
accuracy problems with respect to power are expected to propagate to
temperature. Also, since the temperature-driven studies \cite{juan2011,
juan2012, huang2009a, lee2013} work under the steady-state assumption
(\cite{juan2011} is also limited to the maximum temperature, and
\cite{huang2009a} does not model the leakage-temperature interplay), a
one-to-one comparison with our framework is not possible.

\subsection{Approximation Accuracy}

The first set of experiments is aimed to identify the accuracy of our framework
with respect to MC simulations. At this point, it is important to note that the
true distributions of temperature are unknown, and both the PC and MC approaches
introduce errors. These errors decrease as the order of PC expansions \lc and
the number of MC samples \ns, respectively, increase. Therefore, instead of
postulating that the MC technique with a certain number of samples is the
``universal truth'' that we should achieve, we shall vary both \lc and \ns and
monitor the corresponding difference between the results produced by the two
alternatives.

In order to make the comparison even more comprehensive, let us also inspect the
effect of the correlation patterns between the local random variables $\{ L_i
\}$ (recall \sref{illustrative-example}). Specifically, apart from \lc and \ns,
we shall change the balance between the two correlation kernels shown in
\eref{correlation-function}, that is, the squared-exponential $k_\SE$ and
Ornstein-Uhlenbeck $k_\OU$ kernels, which is controlled by the weight parameter
$\eta \in [0, 1]$.

The PC and MC methods are compared by means of three error metrics. The first
two are the \acf{NRMSE} of the expectation and variance of the computed
temperature profiles.\footnote{In the context of \acp{NRMSE}, we treat the MC
results as the observed data and the PC results as the corresponding model
predictions.} The third metric is the mean of the \acp{NRMSE} of the empirical
PDFs of temperature constructed at each time step for each processing element.
The error metrics are denoted by $\epsilon_{\expectation}$,
$\epsilon_{\variance}$, and $\epsilon_{\probability}$, respectively.
$\epsilon_{\expectation}$ and $\epsilon_{\variance}$ are easy to interpret, and
they are based on the analytical formulae in \eref{pc-moments}.
$\epsilon_{\probability}$ is a strong indicator of the quality of the
distributions estimated by our framework, and it is computed by sampling the
constructed PC expansions. In contrast to the MC approach, this sampling has a
negligible overhead as we discussed in \sref{post-processing}.

The considered values for \lc, \ns, and $\eta$ are the sets $\{ n \}_{n = 1}^7$,
$\{ 10^n \}_{n = 2}^5$, and $\{ 0, 0.5, 1 \}$, respectively. The three cases of
$\eta$ correspond to the total dominance of $k_\OU$ ($\eta = 0$), perfect
balance between $k_\SE$ and $k_\OU$ ($\eta = 0.5$), and total dominance of
$k_\SE$ ($\eta = 1$). A comparison for a quad-core architecture with a dynamic
power profile of $\ns = 10^2$ steps is given in \tref{accuracy-eta-0},
\tref{accuracy-eta-0-5}, and \tref{accuracy-eta-1}, which correspond to $\eta =
0$, $\eta = 0.5$, and $\eta = 1$, respectively. Each table contains three
subtables: one for $\epsilon_{\expectation}$ (the left most), one for
$\epsilon_{\variance}$ (in the middle), and one for $\epsilon_{\probability}$
(the right most), which gives nine subtables in total. The columns of the tables
that correspond to high values of \ns can be used to assess the accuracy of the
constructed PC expansions; likewise, the rows that correspond to high values of
\lc can be used to judge about the sufficiency of the number of MC samples. One
can immediately note that, in all the subtables, all the error metrics tend to
decrease from the top left corners (low values of \lc and \ns) to the bottom
right corners (high values of \lc and \ns), which suggests that the PC and MC
methods converge. There are a few outliers, associated with low PC orders and/or
the random nature of sampling, for instance, $\epsilon_{\variance}$ increases
from 66.13 to 66.70 and $\epsilon_{\probability}$ from 1.59 to 1.62 when \ns
increases from $10^4$ and $10^5$ in \tref{accuracy-eta-0-5}; however, the
aforementioned main trend is still clear.

For clarity of the discussions below, we shall primarily focus on one of the
tables, namely, on the middle table, \tref{accuracy-eta-0-5}, as the case with
$\eta = 0.5$ turned out to be the most challenging (explained in
\sref{er-speed}). The drawn conclusions will be generalized to the other two
tables later on.

First, we concentrate on the accuracy of our technique and, thus, pay particular
attention the columns of \tref{accuracy-eta-0-5} corresponding to high values of
\ns. It can be seen that the error $\epsilon_{\expectation}$ of the expected
value is small even for $\lc = 1$: it is bounded by 0.6\% (see
$\epsilon_{\expectation}$ for $\lc \geq 1$ and $\ns \geq 10^4$).

The error $\epsilon_{\variance}$ of the second central moment starts from 66.7\%
for the first-order PC expansions and drops significantly to 5.71\% and below
for the fourth order and higher (see $\epsilon_{\variance}$ for $\lc \geq 4$ and
$\ns = 10^5$). It should be noted, however, that, for a fixed $\lc \geq 4$,
$\epsilon_{\variance}$ exhibits a considerable decrease even when \ns
transitions from $10^4$ to $10^5$. The rate of this decrease suggests that $\ns
= 10^4$ is not sufficient to reach the same accuracy as the one delivered by the
proposed framework, and $\ns = 10^5$ might not be either.

The results of the third metric $\epsilon_{\probability}$ allow us to conclude
that the PDFs computed by the third-order (and higher) PC expansions closely
follow those estimated by the MC technique with large numbers of samples,
namely, the observed difference in \tref{accuracy-eta-0-5} is bounded by 1.83\%
(see $\epsilon_{\probability}$ for $\lc \geq 3$ and $\ns \geq 10^4$). To give a
better appreciation of the proximity of the two methods,
\fref{experimental-results-pdf} displays the PDFs computed using our framework
for time moment 50~ms with $\lc = 4$ (the dashed lines) along with those
calculated by the MC approach with $\ns = 10^4$ (the solid lines). It can be
seen that the PDFs tightly match each other. Note that this example captures one
particular time moment, and such curves are readily available for all the other
steps of the considered time span.

Now we take a closer look at the convergence of the MC-based technique. With
this in mind, we focus on the rows of \tref{accuracy-eta-0-5} that correspond to
PC expansions of high orders. Similar to the previous observations, even for low
values of \ns, the error of the expected values estimated by MC sampling is
relatively small, namely, bounded by 1.19\% (see $\epsilon_{\expectation}$ for
$\lc \geq 4$ and $\ns = 10^2$). Meanwhile, the case with $\ns = 10^2$ has a high
error rate in terms of $\epsilon_{\variance}$ and $\epsilon_{\probability}$: it
is above 38\% for variance and almost 3.5\% for PDFs (see $\epsilon_{\variance}$
and $\epsilon_{\probability}$ for $\lc = 7$ and $\ns = 10^2$). The results with
$\ns = 10^3$ are reasonably more accurate; however, this trend is compromised by
\tref{accuracy-eta-1}: $10^3$ samples leave an error of more than 7\% for
variance (see $\epsilon_{\variance}$ for $\lc \geq 4$ and $\ns = 10^3$).

The aforementioned conclusions, based on \tref{accuracy-eta-0-5} ($\eta = 0.5$),
are directly applicable to \tref{accuracy-eta-0} ($\eta = 0$) and
\tref{accuracy-eta-1} ($\eta = 1$). The only difference is that the average
error rates are lower when either of the two correlation kernels dominates. In
particular, according to $\epsilon_{\variance}$, the case with $\eta = 1$, which
corresponds to $k_\SE$, stands out to be the least error prone.

Guided by the observations in this subsection, we conclude that our framework
delivers accurate results starting from $\lc = 4$. The MC estimates, on the
other hand, can be considered as sufficiently reliable starting from $\ns =
10^4$. The last conclusion, however, is biased in favor of the MC technique
since, as we noted earlier, there is evidence that $10^4$ samples might still
not be enough.

\subsection{Computational Speed}

In this section, we focus on the speed of our framework. In order to increase
the clarity of the comparisons given below, we use the same order of PC
expansions and the same number of MC samples in each case. Namely, based on the
conclusions from the previous subsection, \lc is set to four, and \ns is set to
$10^4$; the latter also conforms to the experience from the literature
\cite{huang2009, lee2013, shen2009, bhardwaj2008, ghanta2006} and to the
theoretical results on the accuracy of MC sampling given in
\cite{diaz-emparanza2002}.

First, we vary the number of processing elements \np, which directly affects the
dimensionality of the uncertain parameters $\vu \in \real^\nu$ (recall
\sref{illustrative-example}). As before, we shall report the results obtained
for various correlation weights $\eta$, which impacts the number of the
independent variables $\vz \in \real^\nz$, preserved after the model order
reduction procedure described in \sref{ie-uncertain-parameters} and
\sref{karhunen-loeve}. The results, including the dimensionality \nz of \vz, are
given in \tref{speed-processing-elements} where the considered values for \np
are $\{ 2^n \}_{n = 1}^5$, and the number of time steps \ns is set to $10^3$. It
can be seen that the correlation patters inherent to the fabrication process
\cite{cheng2011} open a great possibility for model order reduction: \nz is
observed to be at most 12 while the maximum number without reduction is 33 (one
global variable and 32 local ones corresponding to the case with 32 processing
elements). This reduction also depends on the floorplans, which is illustrated
by the decrease of \nz when \np increases from 16 to 32 for $\eta = 1$. To
elaborate, one floorplan is a four-by-four grid, a perfect square, while the
other an eight-by-four grid, a rectangle. Since both are fitted into square
dies, the former is spread across the whole die whereas the latter is
concentrated along the middle line; the rest is ascribed to the particularities
of $k_\SE$. On average, the $k_\OU$ kernel ($\eta = 0$) requires the fewest
number of variables while the mixture of $k_\SE$ and $k_\OU$ ($\eta = 0.5$)
requires the most.\footnote{The results in \sref{er-accuracy} correspond to the
case with $\np = 4$; therefore, \nz is two, five, and five for
\tref{accuracy-eta-0}, \tref{accuracy-eta-0-5}, and \tref{accuracy-eta-1},
respectively.} It means that, in the latter case, more variables should be
preserved in order to retain 99\% of the variance. Hence, the case with $\eta =
0.5$ is the most demanding in terms of complexity; see
\sref{computational-challenges}.

It is important to note the following. First, since the curse of dimensionality
constitutes arguably the major concern of the theory of PC expansions, the
applicability of our framework primarily depends on how this curse manifests
itself in the problem at hand, that is, on the dimensionality \nz of \vz.
Second, since \vz is a result of the preprocessing stage depending on many
factors, the relation between $\vu$ and $\vz$ is not straightforward, which is
illustrated in the previous paragraph. Consequently, the dimensionality of \vu
can be misleading when reasoning about the applicability of our technique, and
\nz shown \tref{speed-processing-elements} is well suited for this purpose.

Another observation from \tref{speed-processing-elements} is the low slope of
the execution time of the MC technique, which illustrates the well-known fact
that the workload per MC sample is independent of the number of stochastic
dimensions. On the other hand, the rows with $\nz > 10$ hint at the curse of
dimensionality characteristic to PC expansions (see
\sref{computational-challenges}). However, even with high dimensions, our
framework significantly outperforms MC sampling. For instance, in order to
analyze a power profile with $10^3$ steps of a system with 32 cores, the MC
approach requires more than 40 hours whereas the proposed framework takes less
than two minutes (the case with $\eta = 0.5$).

Finally, we investigate the scaling properties of the proposed framework with
respect to the duration of the considered time spans, which is directly
proportional to the number of steps \ns in the power and temperature profiles.
The results for a quad-core architecture are given in \tref{speed-time-spans}.
Due to the long execution times demonstrated by the MC approach, its statistics
for high values of \ns are extrapolated based on a smaller number of samples,
that is, $\ns \ll 10^4$. As it was noted before regarding the results in
\tref{speed-processing-elements}, we observe the dependency of the PC expansions
on the dimensionality \nz of \vz, which is two for $\eta = 0$ and five for the
other two values of $\eta$ (see \tref{speed-processing-elements} for $\np = 4$).
It can be seen in \tref{speed-time-spans} that the computational times of both
methods grow linearly with $\ns$, which is expected. However, the proposed
framework shows a vastly superior performance being up to five orders of
magnitude faster than MC sampling.

It is worth noting that the observed speedups are due to two major reasons.
First of all, PC expansions are generally superior to MC sampling when the curse
of dimensionality is suppressed \cite{xiu2010, eldred2008}, which we accomplish
by model order reduction and efficient integration schemes; see
\sref{computational-challenges}. The second reason is the particular solution
process used in this work to solve the thermal model and construct PC expansions
in a stepwise manner; see \sref{pc-recurrence}.

\section{Conclusion}
\slab{analog-conclusion}

We presented a framework for transient power-temperature analysis (PTA) of
electronic systems under process variation. Our general technique was then
applied in a context of particular importance wherein the variability of the
effective channel length was addressed. Note, however, that the framework can be
readily utilized to analyze any other quantities affected by process variation
and to study their combinations. Finally, we drew a comparison with MC sampling,
which confirmed the efficiency of our approach in terms of both accuracy and
speed. The reduced execution times, by up to five orders of magnitude, implied
by the proposed framework allow for PTA to be efficiently performed inside
design space exploration loops aimed at, for instance, energy and reliability
optimization with temperature-related constraints under process variation.

Electronic system design based on deterministic techniques for power-temperature
analysis is, in the context of current and future technologies, both unreliable
and inefficient since the presence of uncertainty, in particular, due to process
variation, is disregarded. In this work, we propose a flexible probabilistic
framework targeted at the quantification of the transient power and temperature
variations of an electronic system. The framework is capable of modeling diverse
probability laws of the underlying uncertain parameters and arbitrary
dependencies of the system on such parameters. For the considered system, under
a given workload, our technique delivers analytical representations of the
corresponding stochastic power and temperature profiles. These representations
allow for a computationally efficient estimation of the probability
distributions and accompanying quantities of the power and temperature
characteristics of the system. The approximation accuracy and computational time
of our approach are assessed by a range of comparisons with Monte Carlo
simulations, which confirm the efficiency of the proposed technique.
