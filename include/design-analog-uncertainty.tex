In this chapter, we shift our attention from analyzing process variation to
designing with process variation. In other words, instead of analyzing the
original variability, we now study its impact on higher-level characteristics so
that this deteriorating impact can be taken into account in the final design.

\section{Introduction}

As it is emphasized in \sref{inference-introduction}, process variation is a
major concern of the designer. In order to assist the designer, we develop a
framework that allows the designer to propagate the corresponding uncertainty
through the system under development and investigate its impact on the system's
behavior.

We proceed as follows. A motivational example is considered in
\sref{analog-example}. The objective of our study is formulated in
\sref{analog-problem}. \sref{analog-prior} provides an overview of the prior
work. The proposed framework is presented in \sref{analog-solution}. An
exemplary application of our approach is discussed in \sref{analog-example}, and
the corresponding experimental results are given and compared with \ac{MC}
simulations in \sref{analog-results}. \sref{analog-conclusion} concludes the
chapter.

\section{Motivational Example}
\slab{analog-example}

\inputfigure{analog-example}
Consider a quad-core architecture exposed to the uncertainty due to parameters
that affect the leakage current. Assume first that these parameters have nominal
values. We can then simulate the system under a certain workload and observe the
corresponding temperature profile; the experimental setup is to be detailed in
\sref{analog-example} and \sref{analog-results}. The result is depicted in
\fref{analog-example} by a blue line, which corresponds to the temperature of
one of the processors. It can be seen that the temperature is always below
\celsius{90}. Let us now assume a mild deviation of the parameters from the
nominal values and simulate once again. The result is the orange line in
\fref{analog-example}; the maximum temperature is approaching \celsius{100}.
Finally, we repeat the experiment considering a severe deviation of the
parameters and observe the yellow line in \fref{analog-example}; the maximum
temperature is almost \celsius{110}. Suppose that the designer is tuning a
solution constrained by a maximum temperature of \celsius{90} and is guided
exclusively by the nominal parameters. In this case, even with mild deviations,
the circuits might be burnt. Another path that the designer can take is to
design the system for severe conditions. In this scenario, however, the system
might easily become too conservative, overdesigned. Consequently, such
uncertainty has to be addressed in order to pursue efficiency and robustness.
Nevertheless, the majority of the literature related to power and temperature
analysis of electronic systems ignores this important aspect; see, for instance,
\cite{rao2009, rai2011, thiele2011}. This negligence is also present in the
analysis and optimization described in \cref{design-certainty}, and the goal of
this chapter is to eliminate it in the case of the uncertainty due to process
variation.

\section{Problem Formulation}
\slab{analog-problem}

Let $(\Omega, \mathcal{F}, \probability)$ be a probability space as it defined
in \sref{probability-theory} and assume the same system model as the one
described in \sref{system-model}. The system depends on \nu process parameters
that are uncertain at the design stage. These parameters are denoted by a random
vector $\vu: \Omega \to \real^\nu$. Once the fabrication process yields a
particular outcome, \vu takes potentially different values across each
fabricated chip and stays unchanged thereafter. This variability leads to
deviations of power from the nominal values and, therefore, to deviations of
temperature from the one corresponding to the nominal power consumption.

The goal is to develop a system-level framework for transient temperature and,
hence, power analysis of electronic systems where the power consumption and heat
dissipation are stochastic due to their dependency on the uncertain parameters
\vu. The user is required to specify \one~the probability distribution of \vu
and \two~the dependency of the power consumption on \vu, which can be given as a
``black box.'' The framework is to provide the user with the tools for analyzing
the system under a given workload, without imposing constraints on the nature of
this workload, and obtaining the corresponding stochastic power \mp and
temperature \mq profiles with a desired level of accuracy and at low costs.

\section{Prior Work}
\slab{analog-prior}

Since the appearance of the first digital computers in 1940s, \acf{MC} sampling
remains one of the most well-known and widely used methods for the analysis of
stochastic systems. The reason for this popularity lies in the ease of
implementation, in the independence of the stochastic dimensionality of the
problem at hand, and in the fact that, by the law of large numbers
\cite{durrett2010}, the quantities estimated using \ac{MC} sampling
asymptotically approach the true values. The main problem with \ac{MC} sampling,
however, is the low rate of convergence: the error decreases as $\no^{-1/2}$
where \no is the number of drawn samples. This means that an additional decimal
point of accuracy requires hundred times more samples. Each such sample implies
a complete realization of the whole system, which renders \ac{MC}-based methods
slow and often unfeasible since the needed number of simulations can be
extremely large \cite{diaz-emparanza2002}. There are other sampling techniques
that have better convergence rates than the one of the classical \ac{MC}
sampling such as quasi-\ac{MC} sampling; however, due to additional
restrictions, their applicability is often limited \cite{xiu2010}.

In order to overcome the limitations of deterministic techniques and, at the
same time, to eliminate or, at least, mitigate the computation costs associated
with \ac{MC} sampling, a number of stochastic techniques have been introduced,
which we elaborate on in what follows. We are particularly interested in power
and temperature and, therefore, shape the exposition accordingly. Since the
static component of the total power dissipation is influenced by process
variation the most, which is due to the leakage current, the techniques
discussed below primarily focus on the variability of this component.

A solely power-targeted but temperature-aware solution is proposed in
\cite{chandra2010} where the driving force of the analysis is \ac{MC} sampling
with partially precomputed data. A learning-based approach is presented in
\cite{juan2011} in order to estimate the maximum temperature under the static
steady-state condition; recall \sref{static-steady-state}. Temperature-related
issues originating from process variation are also considered in \cite{juan2012}
where a statistical model of the static steady-state temperature based on
Gaussian distributions is derived. A statistical simulator of the static
steady-state temperature is developed in \cite{huang2009a} using \ac{PC}
expansions and the \acf{KL} decomposition; see
\sref{probability-transformation}. A \ac{KL}-aided stochastic collocation
\cite{xiu2010} approach to static steady-state temperature analysis is presented
in \cite{lee2013}. In \cite{shen2009}, \ac{PC} expansions are employed in order
to estimate the leakage power of the entire chip. The \ac{KL} decomposition is
utilized in \cite{bhardwaj2006} for leakage calculations. In
\cite{bhardwaj2008}, the total leakage is quantified using the \ac{PC} and
\ac{KL} methods. The same combination of tools is employed in
\cite{vrudhula2006} and \cite{ghanta2006} in order to analyze the response of
interconnect networks and power grids, respectively, under process variation.

The last five of the aforementioned techniques, that is, \cite{bhardwaj2006,
vrudhula2006, ghanta2006, bhardwaj2008, shen2009}, perform only stochastic power
analysis and ignore the interdependence between power and temperature described
in \sref{power-model}. The others are temperature-related approaches, but none
of them attempts to tackle stochastic transient temperature analysis and to
compute the probability distribution of temperature that evolves over time.
However, such transient curves are of practical importance. First, certain
procedures cannot be undertaken without the knowledge of time-dependent
temperature variations; an example is reliability optimization based on the
thermal-cycling fatigue, which is discussed in
\sref{thermal-cycling-exploration}. Second, the static steady-state assumption
considered, for instance, in \cite{huang2009a, juan2011, juan2012, lee2013} can
rarely be justified since power profiles are not invariant in reality. In
addition, one frequently encounters the assumption that power and temperature
follow \emph{a priori} known probability distributions; Gaussian and log-normal
distributions are popular choices as in \cite{bhardwaj2006, srivastava2010,
juan2012}. However, this assumption often fails in practice---which is also
noted in \cite{juan2012} regarding the normality of the leakage current---due to
\one~the nonlinear dependence of power on process parameters and \two~the
nonlinear interdependence between power and temperature. In order to illustrate
this, we simulate $10^4$ times the example given in \sref{analog-example}
assuming the widespread Gaussian model of the effective channel length and apply
the Jarque--Bera test of normality to the collected temperature directly as well
as after processing them with the log transformation. The null hypothesis that
the data are from an unspecified Gaussian distribution is firmly rejected in
both cases at the significance level of 5\%. Therefore, the two distributions
are neither Gaussian nor log-normal, which can also be seen in
\fref{analog-example-density} described in \sref{analog-results}.

To conclude, the prior techniques for stochastic power and temperature analysis
are restricted in use due to one or several of the following traits: based on
\ac{MC} simulations (potentially slow) \cite{chandra2010}, limited to power
analysis \cite{bhardwaj2006, ghanta2006, vrudhula2006, bhardwaj2008, shen2009,
chandra2010}, ignoring the power-temperature interplay \cite{bhardwaj2006,
ghanta2006, vrudhula2006, bhardwaj2008, huang2009a, shen2009}, limited to the
static steady-state temperature \cite{huang2009a, juan2011, juan2012, lee2013},
exclusive focus on the maximum temperature \cite{juan2011}, and \emph{a priori}
chosen distributions of power and temperature \cite{bhardwaj2006,
srivastava2010, juan2012}. Consequently, there is a lack of flexible techniques
for stochastic power and temperature analysis.

\section{Our Solution}
\slab{analog-solution}

We develop a framework for the analysis of the transient power and temperature
profiles of electronic systems subject to the uncertainty due to process
variation. The proposed technique is flexible in modeling diverse probability
distributions, specified by the user, of the uncertain parameters, such as the
effective channel length and gate oxide thickness. Moreover, there are no
assumptions on the distributions of the resulting power and temperature traces
as these distributions are unlikely to be known \emph{a priori}. The proposed
technique is capable of capturing arbitrary joint effects of the uncertain
parameters on the system since the impact of these parameters is introduced into
the framework as a ``black box,'' which is also defined by the user. In
particular, it allows for the power-temperature interplay to be taken into
account with no effort. Our approach is founded on the basis of \acf{PC}
expansions, which constitute an attractive alternative to \ac{MC} sampling. This
is due to the fact that \ac{PC} expansions possess much faster convergence
properties and provide succinct and intuitive representations of system
responses to stochastic inputs. In addition, we illustrate the framework
considering one of the most important parameters affected by process variation,
namely, the effective channel length. Note, however, that our approach is not
bound to any particular source of variability and, apart from the effective
channel length, can be applied to other process parameters such as the gate
oxide thickness.

The main idea is to construct a surrogate model for the joint power and
temperature model of the system using \ac{PC} expansions. Having constructed
this surrogate, such quantities as the \acf{CDF} and \acf{PDF} can be easily
estimated. Moreover, the representation that we compute provides analytical
formulae for probabilistic moments, which means that the expectation and
variance are readily available.

\inputfigure{analog-overview}
The major stages of our technique are depicted in \fref{analog-overview}. At
Stage~1 (\sref{analog-parameter-transformation}), the uncertain parameters \vu
are transformed into independent random variables \vz since independence is a
prerequisite of the \ac{PC} approach. At Stage~2 (\sref{analog-power-model}),
the user specifies the power model of the system via a ``black-box'' function
$f$ that computes the total power \vp for a particular temperature \vq and a
particular outcome of the parameters \vu. At Stage~3
(\sref{analog-temperature-model}), based on \sref{transient-state-solution}, a
mathematical description of the thermal behavior of the system is obtained. At
Stage~4 (\sref{analog-surrogate-model}), the surrogate model is computed by
traversing the desired time span and gradually constructing \ac{PC} expansions
of the stochastic power and temperature profiles. At Stage~5
(\sref{analog-post-processing}), the computed \ac{PC} expansions are analyzed in
order to obtain the needed characteristics of the system such as \acp{PDF} and
moments.

Let us now detail each of the five stages of the proposed framework.

\subsection{Parameter Transformation}
\slab{analog-parameter-transformation}

Independence is required by \ac{PC} expansions. In general, the \nu individual
variables in \vu are dependent and, therefore, should be preprocessed in order
to fulfill the requirement. To this end, an adequate probability transformation
should be undertaken \cite{eldred2008}. Denote such a transformation by $\vu =
\transform{\vz}$ where $\vz: \Omega \to \real^\nz$ is a random vector with \nz
mutually independent components.

Correlated random variables can be transformed into linearly uncorrelated ones
via the \ac{KL} decomposition presented in \sref{probability-transformation}.
If, in addition, the correlated variables form a Gaussian vector, the
uncorrelated ones are also mutually independent. In the general case
(non-Gaussian), the most prominent solutions to attain independence are the
Rosenblatt \cite{rosenblatt1952} and Nataf transformations \cite{li2008}.
Rosenblatt's approach is suitable when the joint probability distribution
function of \vu is known; however, such information is rarely available. A set
of marginal probability distributions and a correlation matrix are more likely
to be given, which are already sufficient for perform the Nataf transformation.
The Nataf transformation is an approximation that operates under the assumption
that the copula of the distribution is elliptical, and it produces correlated
Gaussian variables, which are then turned into mutually independent ones by
virtue of the aforementioned \ac{KL} decomposition.

Apart from the extraction of the independent variables \vz, an essential
operation at this stage is model order reduction since the number of stochastic
dimensions directly impacts the complexity of the rest of the computations. The
intuition is that, due to the correlations between the random variables in \vu,
some of them can be harmlessly replaced by combinations of the rest, leading to
a random vector \vz with fewer dimensions. This operation is often treated as a
part of the \ac{KL} decomposition and is explained in
\sref{probability-transformation}.

In \sref{analog-example}, we shall demonstrate the Nataf transformation together
with the discrete \ac{KL} decomposition using a concrete example.

\subsection{Power Model}
\slab{analog-power-model}

As stated in \sref{analog-problem}, the user is supposed to decide on the power
model for the system, which we denote as follows:
\[
  \vp(t) = f(t, \vq(t), \vu)
\]
where $f$ is a function that evaluates the power $\vp \in \real^\np$ of the
processing elements given their temperature $\vq \in \real^\np$ and parameters
$\vu \in \real^\nu$. It should be understood that \vp, \vq, and \vu are random
vectors in general, and that $f$ consumes $\vq(t, \omega)$ and $\vu(\omega)$ and
yields $\vp(t, \omega)$ for some particular outcome $\omega \in \Omega$.

The user can choose any $f$. It can be, for instance, a closed-form formula or a
piece of code. The only assumption we make about $f$ is that it is smooth in \vz
and belongs to $\L{2}(\Omega, \mathcal{F}, \probability)$ when it is viewed as a
random variable (see \sref{probability-theory}), which is generally applicable
to most physical systems \cite{xiu2010}. Note also that the operation performed
by this ``black box'' is purely deterministic. The definition of $f$ is flexible
enough to account for such effects as the interdependence between power and
temperature discussed in \sref{power-model}.

\subsection{Temperature Model}
\slab{analog-temperature-model}

The temperature model is based on the one described in \sref{temperature-model},
and transient temperature analysis is based on the one presented in
\sref{transient-state-solution}. The major difference is that
\eref{temperature-model} implicitly operates on stochastic quantities.
Consequently, the recurrent solution in \eref{transient-state-recurrence}, that
is,
\[
  \vs_i = \m{E} \vs_{i - 1} + \m{F} \vp_i
\]
for $i = \range{1}{\ns}$ where $\vs_0 = \v{0}$, is such as well. In the
deterministic case, it can be readily employed to perform deterministic
transient power and temperature analysis via the techniques in
\sref{power-temperature-interplay}. In the stochastic case, however, the
situation is substantially different since $\vp_i$ and, consequently, $\vs_i$
and $\vq_i$ are probabilistic quantities. Moreover, at each step, $\vp_i$ is an
arbitrary transformation of the uncertain parameters \vu and stochastic
temperature $\vq_i$, which results in a multivariate random variable with a
generally unknown probability distribution. Furthermore, $\vp_i$, $\vq_i$,
$\vs_i$, and \vu are dependent random vectors since the first three are
functions of the last. Hence, the operations involved in the recurrence are to
be performed on dependent random vectors with arbitrary probability
distributions, which, in general, have no closed-form solutions. In order to
tackle this difficulty, we reside to \ac{PC} expansions as follows.

\subsection{Surrogate Model}
\slab{analog-surrogate-model}

The goal now is to transform the ``problematic'' term in \eref{recurrence}, that
is, the power term defined by \eref{power-model}, in such a way that the
recurrence in \eref{recurrence} becomes computationally tractable. Our solution
is the construction of a surrogate model for the power model in
\eref{power-model}, which we further propagate through \eref{recurrence} to
obtain an approximation for temperature. To this end, we employ polynomial chaos
(PC) \cite{xiu2010}, which decomposes stochastic quantities into infinite series
of orthogonal polynomials of random variables. Such series are especially
attractive from the post-processing perspective as they are nothing more than
polynomials; hence, PC expansions are easy to interpret and easy to evaluate. An
introduction to orthogonal polynomials, which we rely on in what follows, is
given in \sref{polynomial-chaos}.

\subsubsection{Polynomial basis}
\slab{pc-basis}

The first step towards a polynomial expansion is the choice of a suitable
polynomial basis, which is typically made based on the Askey scheme of
orthogonal polynomials \cite{xiu2010}. The step is crucial as the rate of
convergence of PC expansions closely depends on it. Although there are no strict
rules that guarantee the optimal choice \cite{knio2006}, there are best
practices saying that one should be guided by the probability distributions of
the random variables that drive the stochastic system at hand. For instance,
when a random variable follows a beta distribution, the Jacobi basis is worth
being tried first; on the other hand, the Hermite basis is preferable for
Gaussian distributions. In multiple dimensions, which is the case with the
\nz-dimensional random variable \vz, several (possibly different)
univariate bases are to be combined together to produce a single
\nz-variate polynomial basis, which we denote by $\{ \psi_i: \real^\nz
\to \real \}_{i = 1}^\infty$; see \cite{xiu2010}.

\subsubsection{Recurrence of polynomial expansions}
\slab{pc-recurrence}

Having chosen an appropriate basis, we apply the PC expansion formalism to the
power term in \eref{recurrence} and truncate the resulting infinite series in
order to make it feasible for practical implementations. Such an expansion is
defined as follows:
\begin{equation} \elab{pc-expansion}
  \chaos{\nz}{\lc}{\vp_k} = \sum_{i = 1}^{\nc} \hat{\vp}_{ki} \psi_i(\vz)
\end{equation}
where $\{ \psi_i: \real^\nz \to \real \}_{i = 1}^{\nc}$ is the truncated
basis with \nc polynomials in \nz variables, and $\hat{\vp}_{ki} \in \real^\np$
are the coefficients of the expansion, which are deterministic. The latter can
be computed using spectral projections as it is described in
\sref{pc-coefficients}. \lc denotes the order of the expansion, which determines
the maximum degree of the \nz-variate polynomials involved in the expansion;
hence, \lc also determines the resulting accuracy. The total number of the PC
coefficients \nc is given by the following expression, which corresponds to the
total-order polynomial space \cite{eldred2008, beck2011}:
\begin{equation} \elab{pc-terms}
  \nc = { \lc + \nz \choose \nz } = \frac{(\lc + \nz)!}{\lc! \nz!}.
\end{equation}

It can be seen in \eref{recurrence} that, due to the linearity of the operations
involved in the recurrence, $\vs_k$ retains the same polynomial structure as
$\vp_k$. Therefore, using \eref{pc-expansion}, \eref{recurrence} is rewritten as
follows, for $k = \range{1}{\ns}$:
\begin{equation} \elab{expanded-recurrence}
  \chaos{\nz}{\lc}{\vs_k} = \m{E}_k \chaos{\nz}{\lc}{\vs_{k - 1}} + \m{F}_k
  \chaos{\nz}{\lc}{\vp_k}.
\end{equation}
Thus, there are two PC expansions for two concurrent stochastic processes with
the same basis but different coefficients.

Using \eref{pc-expansion}, \eref{expanded-recurrence} can be explicitly written
as follows:
\[
  \sum_{i = 1}^{\nc} \hat{\vs}_{ki} \psi_i(\vz) = \sum_{i = 1}^{\nc} \left( \m{E}_k \hat{\vs}_{(k - 1)i} + \m{F}_k \hat{\vp}_{ki} \right) \psi_i(\vz).
\]
Multiplying the above equation by each polynomial from the basis and making use
of the orthogonality property (given in \eref{orthogonality} in
\sref{polynomial-chaos}), we obtain the following recurrence:
\begin{equation} \elab{pc-recurrence}
  \hat{\vs}_{ki} = \m{E}_k \hat{\vs}_{(k - 1)i} + \m{F}_k \hat{\vp}_{ki}
\end{equation}
where $k = \range{1}{\ns}$ and $i = \range{1}{\nc}$. Finally,
\eref{fourier-output} and \eref{pc-recurrence} are combined together to compute
the coefficients of the PC expansion of the temperature vector $\vq_k$.

\subsubsection{Expansion coefficients}
\slab{pc-coefficients}

The general formula of a truncated PC expansion applied to the power term in
\eref{recurrence} is given in \eref{pc-expansion}. Let us now find the
coefficients $\{ \hat{\vp}_{ki} \}$ of this expansion, which will be propagated
to temperature (using \eref{pc-recurrence} and \eref{fourier-output}). To this
end, a spectral projection of the stochastic quantity being expanded---that is,
of $\vp_k$ as a function of \vz via $\vu = \transform{\vz}$ discussed in
\sref{uncertain-parameters}---is to be performed onto the space spanned by the
\nz-variate polynomials $\{ \psi_i \}_{i = 1}^{\nc}$, where \nc is the number of
polynomials in the truncated basis. This means that we need to compute the inner
product of \eref{power-model} with each polynomial from the basis as follows:
\[
  \innerproduct{\vp_k}{\psi_i} = \innerproduct{\sum_{j = 1}^{\nc} \hat{\vp}_{kj} \psi_j}{\psi_i}
\]
where $i = \range{1}{\nc}$, $k = \range{1}{\ns}$, and
$\innerproduct{\cdot}{\cdot}$ stands for the inner product (see
\sref{polynomial-chaos} for a definition), which should be understood
elementwise. Making use of the orthogonality property of the basis, we obtain
\begin{equation} \elab{pc-coefficients}
  \hat{\vp}_{ki} = \frac{\innerproduct{\vp_k}{\psi_i}}{\innerproduct{\psi_i}{\psi_i}}.
\end{equation}

In general, the inner product in \eref{pc-coefficients}, given in
\eref{inner-product} in \sref{polynomial-chaos}, should be evaluated
numerically. This task is accomplished by virtue of a quadrature rule, which is
a weighted summation over the integrand values computed at a set of prescribed
points. These points along with the corresponding weights are generally
precomputed and tabulated since they do not depend the quantity being
integrated. Denote such a quadrature-based approximation of
\eref{pc-coefficients} by
\begin{equation} \elab{pc-coefficients-quadrature}
  \hat{\vp}_{ki} = \frac{\quadrature{\nz}{\lq}{\vp_k \psi_i}}{\innerproduct{\psi_i}{\psi_i}}
\end{equation}
where \lq is the level of the quadrature utilized. The procedure is
detailed in \sref{gauss-quadrature}; for the development in this section, we
only need to note that \nz and \lq dictate the number of quadrature points,
which we shall denote by \nq. Also, it is worth emphasizing that, since power
depends on temperature as shown in \eref{power-model}, at each step of the
recurrence in \eref{pc-recurrence}, the computation of $\hat{\vp}_{ki}$ should
be done with respect to the PC expansion of the temperature vector $\vq_{k -
1}$.

\subsubsection{Computational challenges}
\slab{computational-challenges}

The construction process of the stochastic power and temperature profiles,
implemented inside our prototype of the proposed framework, has been estimated
to have the following time complexity:
\[
  \bigo{\ns \nn^2 \nc + \ns \np \nq \nc + \ns \nq f(\np)}
\]
where $\bigo{f(\np)}$ denotes the complexity of the computations associated with
the power model in \eref{power-model}. The expression can be detailed further by
expanding \nc and \nq. The exact formula for \nc is given in \eref{pc-terms},
and the limiting behavior of \nc with respect to \nz is $\bigo{\nz^\nc / \nc!}$.
For brute-force quadrature rules, $\log(\nq)$ is $\bigo{\nz}$, meaning that the
dependency of \nq on \nz is exponential. It can be seen that the theory of PC
expansions suffers from the so-called curse of dimensionality \cite{xiu2010,
eldred2008}. More precisely, when \nz increases, the number of polynomial terms
as well as the complexity of the corresponding coefficients exhibit a growth,
which is exponential without special treatments. The problem does not have a
general solution and is one of the central topics of many ongoing studies. In
this paper, we mitigate this issue by: (a) keeping the number of stochastic
dimensions low using the KL decomposition as we shall see in
\sref{ie-uncertain-parameters} and (b) utilizing efficient integration
techniques as discussed in \sref{gauss-quadrature}. In particular, for sparse
integration grids based on Gaussian quadratures, $\log(\nq)$ is
$\bigo{\log(\nz)}$, meaning that the dependency of \nq on \nz is only polynomial
\cite{heiss2008}.

To summarize, let us recall the stochastic recurrence in \eref{recurrence}
where, in the presence of correlations, an arbitrary functional $\vp_k$ of the
uncertain parameters \vu and random temperature $\vq_k$ (see \sref{power-model})
needs to be evaluated and combined with another random vector $\vs_k$. Now the
recurrence in \eref{recurrence} has been replaced with a purely deterministic
recurrence in \eref{pc-recurrence}. More importantly, the heavy thermal system
in \eref{fourier-system} has been substituted with a light polynomial surrogate
defined by a set of basis functions $\{ \psi_i \}_{i = 1}^\nc$ and the
corresponding sets of coefficients, namely, $\{ \hat{\vp}_{ki} \}_{i = 1}^\nc$
for power and $\{ \hat{\vq}_{ki} \}_{i = 1}^\nc$ for temperature, where $k$
traverses the \ns intervals of the considered time span. Consequently, the
output of the proposed PTA framework constitutes two stochastic profiles: the
power and temperature profiles denoted by \mp and \mq, respectively, which are
ready to be analyzed.

Finally, note the ease and generality of taking the uncertainty into
consideration using the proposed approach: the above derivation is delivered
from any explicit formula for any particular uncertain parameter. In contrast, a
typical solution from the literature related to process variation is based on ad
hoc expressions and should be tailored by the user for each new parameter
individually; see, for instance, \cite{huang2009a, bhardwaj2008, ghanta2006}.
Our framework provides a great flexibility in this regard.

\subsection{Post-Processing}
\slab{analog-post-processing}

Due to the properties of PC expansions---in particular, due to the pairwise
orthogonality of the basis polynomials as discussed in
\sref{polynomial-chaos}---the obtained polynomial traces allow for various
prospective analyses to be performed with no effort. For instance, consider the
PC expansion of temperature at the $k$th moment of time given by
\begin{equation} \elab{pc-k}
  \chaos{\nz}{\lc}{\vq_k} = \sum_{i = 1}^{\nc} \hat{\vq}_{ki} \psi_i(\vz)
\end{equation}
where $\hat{\vq}_{ki}$ are computed using \eref{fourier-output} and
\eref{pc-recurrence}. Let us, for example, find the expectation and variance of
the expansion. Due to the fact that, by definition \cite{xiu2010}, the first
polynomial $\psi_1$ in a polynomial basis is unity, $\expectation{\psi_1(\vz)} =
1$. Therefore, using the orthogonality property in \eref{orthogonality}, we
conclude that $\expectation{\psi_i(\vz)} = 0$ for $i = \range{2}{\nc}$.
Consequently, the expected value and variance have the following simple
expressions solely based on the coefficients:
\begin{equation} \elab{pc-moments}
  \begin{split}
    & \expectation{\vq_k} = \hat{\vq}_{k1} \text{ and} \\
    & \variance{\vq_k} = \sum_{i = 2}^{\nc} \innerproduct{\psi_i}{\psi_i} \hat{\vq}_{ki}^2
  \end{split}
\end{equation}
where the squaring should be understood elementwise. Such quantities as CDFs,
PDFs, probabilities of certain events, and so on can be estimated by sampling
\eref{pc-k}; each sample is a trivial evaluation of a polynomial. Furthermore,
global and local sensitivity analyses of deterministic and non-deterministic
quantities can be readily conducted on \eref{pc-k}.

\section{Illustrative Example}
\slab{analog-example}

So far we have not made any assumptions regarding the cause of the variability
of the power term in the thermal system given by \eref{fourier-system}. In this
section, we shall consider a particular application of the proposed framework.
To this end, we begin with the problem formulation of this application.

\subsection{Parameter Preprocessing}

At Stage~1, \vu should be preprocessed in order to extract a vector of mutually
independent random variables denoted by \vz. Following the guidance given in
\sref{uncertain-parameters}, the most suitable transformation for the ongoing
scenario is the Nataf transformation. Here we describe the algorithm in brief
and refer the interested reader to \cite{li2008} for additional details. The
transformation is typically presented in two steps. First, $\vu \in \real^\nu$,
$\nu = \np + 1$, is morphed into correlated Gaussian variables, denoted by $\vz'
\in \real^\nu$, using the knowledge of the marginal distributions and covariance
matrix of \vu. Second, the obtained correlated Gaussian variables are mapped
into independent standard Gaussian variables, denoted by $\vz'' \in \real^\nu$,
using one of several available techniques; see \cite{li2008}.

The number of stochastic dimensions, which so far is $\np+ 1$, directly impacts
the computational cost of PC expansions as it is discussed in
\sref{computational-challenges}. Therefore, one should consider a possibility
for model order reduction before constructing PC expansions. To this end, we
perform the second step of the Nataf transformation by virtue of the discrete
Karhunen--Lo\`{e}ve (KL) decomposition \cite{ghanem1991} as the reduction comes
naturally in this way. A description of this procedure can be found in
\sref{karhunen-loeve}. Let us denote the trimmed independent variables by
$\vz'''$ and their number by \nz. We also denote the whole operation, that is,
the reduction-aware Nataf transformation, by
\[
  \vu = \mathrm{Nataf}^{-1}(\vz''')
\]
where the superscript ``$-1$'' signifies the fact that we are interested in
expressing \vu via $\vz'''$ and, hence, need to perform all the operations in
the reversed order.

At this point, we have \nz independent Gaussian random variables stored in
$\vz'''$, which already suffice the independence prerequisite for PC expansions
(see \sref{uncertain-parameters}). However, we prefer to construct PC expansions
in terms of bounded variables since such expansions will also be bounded. To
this end, we undertake one additional transformation that yields a vector of
(independent) random variables $\vz \in \real^\nz$ whose distributions have
bounded supports. This transformation is a standard technique based on the
composition of the inverse CDF of $\vz'''$ and the CDF of \vz denoted by
$F^{-1}_{\vz'''}$ and $F_{\vz}$, respectively. The overall probability
transformation $\transform{}$ (see \sref{uncertain-parameters}) from \vu to \vz
is then given as follows:
\[
  \vu = \transform{\vz} = \mathrm{Nataf}^{-1}(F^{-1}_{\vz'''}(F_{\vz}(\vz))).
\]
The distributions of \vz can be chosen arbitrary as long as one can construct a
suitable polynomial basis as described in \sref{pc-basis}. We let \vz have beta
distributions, staying in the same family of distributions with the parameters
\vu.

\subsection{Power Modeling}

At Stage~2 in \fref{algorithm}, we need to decide on the power model with the
identified uncertain parameters as an input. To this end, \eref{power-model} is
decomposed into the sum of the dynamic and static components denoted by
$f_\dynamic(t, \vu)$ and $f_\static(\vq(t, \vu), \vu)$, respectively. As
motivated earlier, we let $f_\dynamic(t, \vu) = \vp_\dynamic(t)$ (does not
depend on \vu). We assume that the desired workload of the system is given as a
dynamic power profile denoted by $\mp_\dynamic$. Without loss of generality, the
development of the static part is based on SPICE simulations of a reference
electrical circuit composed of BSIM4 devices (v4.7.0) \cite{bsim} configured
according to the 45-nm PTM (high-performance) \cite{ptm}. Specifically, we use a
series of CMOS invertors for this purpose. The simulations are performed for a
fine-grained two-dimensional grid, the effective channel length vs. temperature,
and the results are tabulated. The interpolation facilities of MATLAB (vR2013a)
\cite{matlab} are then utilized whenever we need to evaluate the leakage power
for a particular point within the range of the grid, which is chosen to be
sufficiently wide.

\subsection{Thermal Modeling}

We move on to Stage~3 where the thermal model of the multiprocessor system is to
be established. Given the thermal specification \spec of the considered platform
(the floorplan of the die, the configuration of the thermal package, and so on),
we employ HotSpot (v5.02) \cite{skadron2004} in order to construct an equivalent
thermal RC circuits of the system. Specifically, we are interested in the
coefficient matrices $\m{E}(t)$ and $\m{F}(t)$ in \eref{recurrence} (see also
\fref{algorithm}), which HotSpot helps us to compute by providing the
corresponding capacitance and conductance matrices of the system as described in
\sref{thermal-model}. In this case, thermal packages are modeled with three
layers, and the relation between the number of processing elements and the
number of thermal nodes is given by $\nn = 4 \np + 12$.

To conclude, the power and thermal models of the platform are now acquired, and
we are ready to construct the corresponding surrogate model via PC expansions,
which is the topic for the discussion in the following subsection.

\subsection{Surrogate Modeling}

At Stage~4, the uncertain parameters, power model, and thermal model developed
in the previous sections are to be fused together under the desired workload
$\mp_\dynamic$ to produce the corresponding stochastic power $\mp$ and
temperature $\mq$ profiles. The construction of PC expansions, in the current
scenario, is based on the Jacobi polynomial basis as it is preferable in
situations involving beta-distributed parameters \cite{xiu2010}. To give an
example, for a dual-core platform (that is, $\np = 2$) with two stochastic
dimensions (that is, $\nz = 2$), the second-order PC expansion (that is, $\nc =
2$) of temperature at the $k$th time moment is as follows:\footnote{The Jacobi
polynomials have two parameters \cite{xiu2010}, and the shown $\{ \psi_i \}_{i =
1}^6$ correspond to the case where both parameters are equal to two.}
\begin{align}
  \chaos{2}{2}{\vq_k}
  & = \hat{\vq}_{k1} \, \psi_1(\vz) + \hat{\vq}_{k2} \, \psi_2(\vz) + \hat{\vq}_{k3} \, \psi_3(\vz) \nonumber \\
  & {} + \hat{\vq}_{k4} \, \psi_4(\vz) + \hat{\vq}_{k5} \, \psi_5(\vz) + \hat{\vq}_{k6} \, \psi_6(\vz) \elab{pc-example}
\end{align}
where the coefficients $\hat{\vq}_{ki}$ are vectors with two elements
corresponding to the two processing elements,
\begin{align*}
  & \psi_1(\v{x}) = 1, \\
  & \psi_2(\v{x}) = 2 x_1, \\
  & \psi_3(\v{x}) = 2 x_2, \\
  & \psi_4(\v{x}) = 4 x_1 x_2 \\
  & \psi_5(\v{x}) = \frac{15}{4} x_1^2 - \frac{3}{4}, \text{ and} \\
  & \psi_6(\v{x}) = \frac{15}{4} x_2^2 - \frac{3}{4}.
\end{align*}
The expansion for power has the same structure but different coefficients. Such
a series might be shorter or longer depending on the accuracy requirements
defined by \lc.

Once the basis has been chosen, we need to compute the corresponding
coefficients, specifically, $\{ \hat{\vp}_{ki} \}_{i = 1}^\nc$ in
\eref{pc-expansion}, which yield $\{ \hat{\vq}_{ki} \}_{i = 1}^\nc$. As shown in
\sref{polynomial-chaos}, these computations involve multidimensional integration
with respect to the PDF of \vz, which should be done numerically using a
quadrature rule; recall \sref{pc-coefficients}. When beta distributions are
concerned, a natural choice of such a rule is the Gauss-Jacobi quadrature.
Additional details are given in \sref{gauss-quadrature}.

To summarize, we have completed four out of five stages of the proposed
framework depicted in \fref{algorithm}. The result is a light surrogate for the
model in \eref{fourier-system}. At each moment of time, the surrogate is
composed of two \np-valued polynomials, one for power and one for temperature,
which are defined in terms of \nz mutually independent random variables; an
example of such a polynomial is given in \eref{pc-example}.

\subsection{Post-processing}

\inputfigure{analog-example-density}
We turn to Stage~5 in \fref{algorithm}. It can be seen in, for example,
\eref{pc-example} that the surrogate model has a negligibly small computational
cost at this stage: for any outcome of the parameters $\vz \equiv \vz(\omega)$,
we can easily compute the corresponding temperature by plugging in \vz into
\eref{pc-example}; the same applies to power. Hence, the constructed
representation can be trivially analyzed to retrieve various statistics about
the system in \eref{fourier-system}. Let us illustrate a few of them still
retaining the example in \eref{pc-example}. Assume that the dynamic power
profile $\mp_\dynamic$ corresponding to the considered workload is the one shown
in \fref{application-power}. Having constructed the surrogate with respect to
this profile, we can then rigorously estimate, say, the PDF of temperature at
some $k$th moment of time by sampling the surrogate and obtain curves similar to
those shown \fref{experimental-results-pdf} (discussed in
\sref{experimental-results}). Furthermore, the expectation and variance of
temperature are trivially calculated using the formulae in \eref{pc-moments}
where $\nc = 6$. For the whole time span of the power profile $\mp_\dynamic$
depicted in \fref{application-power}, these quantities are plotted in
\fref{application-temperature}. The displayed curves closely match those
obtained via MC simulations with $10^4$ samples; however, our method takes less
than a second whilst MC sampling takes more than a day as we shall see next.

\section{Experimental Results}
\slab{analog-results}

In this section, we report the results of the proposed framework for different
configurations of the illustrative example in \sref{illustrative-example}. All
the experiments are conducted on a GNU/Linux machine with Intel Core i7 2.66~GHz
and 8~GB of RAM.

Now we shall elaborate on the default configuration of our experimental setup,
which, in the following subsections, will be adjusted according to the purpose
of each particular experiment. We consider a 45-nanometer technological process.
The effective channel length is assumed to have a nominal value of
$17.5\,\text{nm}$ \cite{ptm} and a standard deviation of $2.25\,\text{nm}$ where
the global and local variations are equally weighted. Correlation matrices are
computed according to \eref{correlation-function} where the length-scale
parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of the square die.
In the model order reduction technique (see \sref{ie-uncertain-parameters}), the
threshold parameter is set to 0.99 preserving 99\% of the variance of the data.
Dynamic power profiles involved in the experiments are based on simulations of
randomly generated applications defined as directed acyclic task
graphs.\footnote{In practice, dynamic power profiles are typically obtained via
an adequate simulator of the architecture of interest.} The floorplans of the
platforms are constructed in such a way that the processing elements form
regular grids.\footnote{The task graphs of the applications, floorplans of the
platforms, configuration of HotSpot, which was used to construct thermal RC
circuits for our experiments, are available online at \cite{eslab2014}.} The
time step of power and temperature traces is set to $1\,\text{ms}$ (see
\sref{problem-formulation}), which is also the time step of the recurrence in
\eref{pc-recurrence}. As a comparison to our polynomial chaos (PC) expansions,
we employ Monte Carlo (MC) sampling. The MC approach is set up to preserve the
whole variance of the problem, that is, no model order reduction, and to solve
\eref{fourier-system} directly using the Runge-Kutta formulae (the
Dormand-Prince method) available in MATLAB \cite{matlab}.

Since the temperature part of PTA is the main contribution of this work, we
shall focus on the assessment of temperature profiles. Note, however, that the
results for temperature allow one to implicitly draw reasonable conclusions
regarding power since power is an intermediate step towards temperature, and any
accuracy problems with respect to power are expected to propagate to
temperature. Also, since the temperature-driven studies \cite{juan2011,
juan2012, huang2009a, lee2013} work under the steady-state assumption
(\cite{juan2011} is also limited to the maximum temperature, and
\cite{huang2009a} does not model the leakage-temperature interplay), a
one-to-one comparison with our framework is not possible.

\subsection{Approximation Accuracy}

The first set of experiments is aimed to identify the accuracy of our framework
with respect to MC simulations. At this point, it is important to note that the
true distributions of temperature are unknown, and both the PC and MC approaches
introduce errors. These errors decrease as the order of PC expansions \lc and
the number of MC samples \ns, respectively, increase. Therefore, instead of
postulating that the MC technique with a certain number of samples is the
``universal truth'' that we should achieve, we shall vary both \lc and \ns and
monitor the corresponding difference between the results produced by the two
alternatives.

In order to make the comparison even more comprehensive, let us also inspect the
effect of the correlation patterns between the local random variables $\{ L_i
\}$ (recall \sref{illustrative-example}). Specifically, apart from \lc and \ns,
we shall change the balance between the two correlation kernels shown in
\eref{correlation-function}, that is, the squared-exponential $k_\SE$ and
Ornstein-Uhlenbeck $k_\OU$ kernels, which is controlled by the weight parameter
$\eta \in [0, 1]$.

The PC and MC methods are compared by means of three error metrics. The first
two are the \acf{NRMSE} of the expectation and variance of the computed
temperature profiles.\footnote{In the context of \acp{NRMSE}, we treat the MC
results as the observed data and the PC results as the corresponding model
predictions.} The third metric is the mean of the \acp{NRMSE} of the empirical
PDFs of temperature constructed at each time step for each processing element.
The error metrics are denoted by $\epsilon_{\expectation}$,
$\epsilon_{\variance}$, and $\epsilon_{\probability}$, respectively.
$\epsilon_{\expectation}$ and $\epsilon_{\variance}$ are easy to interpret, and
they are based on the analytical formulae in \eref{pc-moments}.
$\epsilon_{\probability}$ is a strong indicator of the quality of the
distributions estimated by our framework, and it is computed by sampling the
constructed PC expansions. In contrast to the MC approach, this sampling has a
negligible overhead as we discussed in \sref{post-processing}.

The considered values for \lc, \ns, and $\eta$ are the sets $\{ n \}_{n = 1}^7$,
$\{ 10^n \}_{n = 2}^5$, and $\{ 0, 0.5, 1 \}$, respectively. The three cases of
$\eta$ correspond to the total dominance of $k_\OU$ ($\eta = 0$), perfect
balance between $k_\SE$ and $k_\OU$ ($\eta = 0.5$), and total dominance of
$k_\SE$ ($\eta = 1$). A comparison for a quad-core architecture with a dynamic
power profile of $\ns = 10^2$ steps is given in \tref{accuracy-eta-0},
\tref{accuracy-eta-0-5}, and \tref{accuracy-eta-1}, which correspond to $\eta =
0$, $\eta = 0.5$, and $\eta = 1$, respectively. Each table contains three
subtables: one for $\epsilon_{\expectation}$ (the left most), one for
$\epsilon_{\variance}$ (in the middle), and one for $\epsilon_{\probability}$
(the right most), which gives nine subtables in total. The columns of the tables
that correspond to high values of \ns can be used to assess the accuracy of the
constructed PC expansions; likewise, the rows that correspond to high values of
\lc can be used to judge about the sufficiency of the number of MC samples. One
can immediately note that, in all the subtables, all the error metrics tend to
decrease from the top left corners (low values of \lc and \ns) to the bottom
right corners (high values of \lc and \ns), which suggests that the PC and MC
methods converge. There are a few outliers, associated with low PC orders and/or
the random nature of sampling, for instance, $\epsilon_{\variance}$ increases
from 66.13 to 66.70 and $\epsilon_{\probability}$ from 1.59 to 1.62 when \ns
increases from $10^4$ and $10^5$ in \tref{accuracy-eta-0-5}; however, the
aforementioned main trend is still clear.

For clarity of the discussions below, we shall primarily focus on one of the
tables, namely, on the middle table, \tref{accuracy-eta-0-5}, as the case with
$\eta = 0.5$ turned out to be the most challenging (explained in
\sref{er-speed}). The drawn conclusions will be generalized to the other two
tables later on.

First, we concentrate on the accuracy of our technique and, thus, pay particular
attention the columns of \tref{accuracy-eta-0-5} corresponding to high values of
\ns. It can be seen that the error $\epsilon_{\expectation}$ of the expected
value is small even for $\lc = 1$: it is bounded by 0.6\% (see
$\epsilon_{\expectation}$ for $\lc \geq 1$ and $\ns \geq 10^4$).

The error $\epsilon_{\variance}$ of the second central moment starts from 66.7\%
for the first-order PC expansions and drops significantly to 5.71\% and below
for the fourth order and higher (see $\epsilon_{\variance}$ for $\lc \geq 4$ and
$\ns = 10^5$). It should be noted, however, that, for a fixed $\lc \geq 4$,
$\epsilon_{\variance}$ exhibits a considerable decrease even when \ns
transitions from $10^4$ to $10^5$. The rate of this decrease suggests that $\ns
= 10^4$ is not sufficient to reach the same accuracy as the one delivered by the
proposed framework, and $\ns = 10^5$ might not be either.

The results of the third metric $\epsilon_{\probability}$ allow us to conclude
that the PDFs computed by the third-order (and higher) PC expansions closely
follow those estimated by the MC technique with large numbers of samples,
namely, the observed difference in \tref{accuracy-eta-0-5} is bounded by 1.83\%
(see $\epsilon_{\probability}$ for $\lc \geq 3$ and $\ns \geq 10^4$). To give a
better appreciation of the proximity of the two methods,
\fref{experimental-results-pdf} displays the PDFs computed using our framework
for time moment 50~ms with $\lc = 4$ (the dashed lines) along with those
calculated by the MC approach with $\ns = 10^4$ (the solid lines). It can be
seen that the PDFs tightly match each other. Note that this example captures one
particular time moment, and such curves are readily available for all the other
steps of the considered time span.

Now we take a closer look at the convergence of the MC-based technique. With
this in mind, we focus on the rows of \tref{accuracy-eta-0-5} that correspond to
PC expansions of high orders. Similar to the previous observations, even for low
values of \ns, the error of the expected values estimated by MC sampling is
relatively small, namely, bounded by 1.19\% (see $\epsilon_{\expectation}$ for
$\lc \geq 4$ and $\ns = 10^2$). Meanwhile, the case with $\ns = 10^2$ has a high
error rate in terms of $\epsilon_{\variance}$ and $\epsilon_{\probability}$: it
is above 38\% for variance and almost 3.5\% for PDFs (see $\epsilon_{\variance}$
and $\epsilon_{\probability}$ for $\lc = 7$ and $\ns = 10^2$). The results with
$\ns = 10^3$ are reasonably more accurate; however, this trend is compromised by
\tref{accuracy-eta-1}: $10^3$ samples leave an error of more than 7\% for
variance (see $\epsilon_{\variance}$ for $\lc \geq 4$ and $\ns = 10^3$).

The aforementioned conclusions, based on \tref{accuracy-eta-0-5} ($\eta = 0.5$),
are directly applicable to \tref{accuracy-eta-0} ($\eta = 0$) and
\tref{accuracy-eta-1} ($\eta = 1$). The only difference is that the average
error rates are lower when either of the two correlation kernels dominates. In
particular, according to $\epsilon_{\variance}$, the case with $\eta = 1$, which
corresponds to $k_\SE$, stands out to be the least error prone.

Guided by the observations in this subsection, we conclude that our framework
delivers accurate results starting from $\lc = 4$. The MC estimates, on the
other hand, can be considered as sufficiently reliable starting from $\ns =
10^4$. The last conclusion, however, is biased in favor of the MC technique
since, as we noted earlier, there is evidence that $10^4$ samples might still
not be enough.

\subsection{Computational Speed}

In this section, we focus on the speed of our framework. In order to increase
the clarity of the comparisons given below, we use the same order of PC
expansions and the same number of MC samples in each case. Namely, based on the
conclusions from the previous subsection, \lc is set to four, and \ns is set to
$10^4$; the latter also conforms to the experience from the literature
\cite{huang2009, lee2013, shen2009, bhardwaj2008, ghanta2006} and to the
theoretical results on the accuracy of MC sampling given in
\cite{diaz-emparanza2002}.

First, we vary the number of processing elements \np, which directly affects the
dimensionality of the uncertain parameters $\vu \in \real^\nu$ (recall
\sref{illustrative-example}). As before, we shall report the results obtained
for various correlation weights $\eta$, which impacts the number of the
independent variables $\vz \in \real^\nz$, preserved after the model order
reduction procedure described in \sref{ie-uncertain-parameters} and
\sref{karhunen-loeve}. The results, including the dimensionality \nz of \vz, are
given in \tref{speed-processing-elements} where the considered values for \np
are $\{ 2^n \}_{n = 1}^5$, and the number of time steps \ns is set to $10^3$. It
can be seen that the correlation patters inherent to the fabrication process
\cite{cheng2011} open a great possibility for model order reduction: \nz is
observed to be at most 12 while the maximum number without reduction is 33 (one
global variable and 32 local ones corresponding to the case with 32 processing
elements). This reduction also depends on the floorplans, which is illustrated
by the decrease of \nz when \np increases from 16 to 32 for $\eta = 1$. To
elaborate, one floorplan is a four-by-four grid, a perfect square, while the
other an eight-by-four grid, a rectangle. Since both are fitted into square
dies, the former is spread across the whole die whereas the latter is
concentrated along the middle line; the rest is ascribed to the particularities
of $k_\SE$. On average, the $k_\OU$ kernel ($\eta = 0$) requires the fewest
number of variables while the mixture of $k_\SE$ and $k_\OU$ ($\eta = 0.5$)
requires the most.\footnote{The results in \sref{er-accuracy} correspond to the
case with $\np = 4$; therefore, \nz is two, five, and five for
\tref{accuracy-eta-0}, \tref{accuracy-eta-0-5}, and \tref{accuracy-eta-1},
respectively.} It means that, in the latter case, more variables should be
preserved in order to retain 99\% of the variance. Hence, the case with $\eta =
0.5$ is the most demanding in terms of complexity; see
\sref{computational-challenges}.

It is important to note the following. First, since the curse of dimensionality
constitutes arguably the major concern of the theory of PC expansions, the
applicability of our framework primarily depends on how this curse manifests
itself in the problem at hand, that is, on the dimensionality \nz of \vz.
Second, since \vz is a result of the preprocessing stage depending on many
factors, the relation between $\vu$ and $\vz$ is not straightforward, which is
illustrated in the previous paragraph. Consequently, the dimensionality of \vu
can be misleading when reasoning about the applicability of our technique, and
\nz shown \tref{speed-processing-elements} is well suited for this purpose.

Another observation from \tref{speed-processing-elements} is the low slope of
the execution time of the MC technique, which illustrates the well-known fact
that the workload per MC sample is independent of the number of stochastic
dimensions. On the other hand, the rows with $\nz > 10$ hint at the curse of
dimensionality characteristic to PC expansions (see
\sref{computational-challenges}). However, even with high dimensions, our
framework significantly outperforms MC sampling. For instance, in order to
analyze a power profile with $10^3$ steps of a system with 32 cores, the MC
approach requires more than 40 hours whereas the proposed framework takes less
than two minutes (the case with $\eta = 0.5$).

Finally, we investigate the scaling properties of the proposed framework with
respect to the duration of the considered time spans, which is directly
proportional to the number of steps \ns in the power and temperature profiles.
The results for a quad-core architecture are given in \tref{speed-time-spans}.
Due to the long execution times demonstrated by the MC approach, its statistics
for high values of \ns are extrapolated based on a smaller number of samples,
that is, $\ns \ll 10^4$. As it was noted before regarding the results in
\tref{speed-processing-elements}, we observe the dependency of the PC expansions
on the dimensionality \nz of \vz, which is two for $\eta = 0$ and five for the
other two values of $\eta$ (see \tref{speed-processing-elements} for $\np = 4$).
It can be seen in \tref{speed-time-spans} that the computational times of both
methods grow linearly with $\ns$, which is expected. However, the proposed
framework shows a vastly superior performance being up to five orders of
magnitude faster than MC sampling.

It is worth noting that the observed speedups are due to two major reasons.
First of all, PC expansions are generally superior to MC sampling when the curse
of dimensionality is suppressed \cite{xiu2010, eldred2008}, which we accomplish
by model order reduction and efficient integration schemes; see
\sref{computational-challenges}. The second reason is the particular solution
process used in this work to solve the thermal model and construct PC expansions
in a stepwise manner; see \sref{pc-recurrence}.

\section{Conclusion}
\slab{analog-conclusion}

We presented a framework for transient power-temperature analysis (PTA) of
electronic systems under process variation. Our general technique was then
applied in a context of particular importance wherein the variability of the
effective channel length was addressed. Note, however, that the framework can be
readily utilized to analyze any other quantities affected by process variation
and to study their combinations. Finally, we drew a comparison with MC sampling,
which confirmed the efficiency of our approach in terms of both accuracy and
speed. The reduced execution times, by up to five orders of magnitude, implied
by the proposed framework allow for PTA to be efficiently performed inside
design space exploration loops aimed at, for instance, energy and reliability
optimization with temperature-related constraints under process variation.

Electronic system design based on deterministic techniques for power-temperature
analysis is, in the context of current and future technologies, both unreliable
and inefficient since the presence of uncertainty, in particular, due to process
variation, is disregarded. In this work, we propose a flexible probabilistic
framework targeted at the quantification of the transient power and temperature
variations of an electronic system. The framework is capable of modeling diverse
probability laws of the underlying uncertain parameters and arbitrary
dependencies of the system on such parameters. For the considered system, under
a given workload, our technique delivers analytical representations of the
corresponding stochastic power and temperature profiles. These representations
allow for a computationally efficient estimation of the probability
distributions and accompanying quantities of the power and temperature
characteristics of the system. The approximation accuracy and computational time
of our approach are assessed by a range of comparisons with Monte Carlo
simulations, which confirm the efficiency of the proposed technique.

\section{Temperature Model}

In this section, we provide additional details on the thermal model utilized by
the proposed framework at Stage~3 described in \sref{thermal-model}. We use the
widespread model based on Fourier's heat equation \cite{skadron2004}, which,
after a proper spacial discretization, leads to the following system:
\begin{subnumcases}{\elab{fourier-system-original}}
  \m{C} \frac{d \vs(t)}{dt} + \m{G} \vs(t) = \m{M} \vp(t) \elab{fourier-original} \\
  \vq(t) = \m{M}^T \vs(t) + \vq_\ambient
\end{subnumcases}
where the number of differential equations is equal to the number of thermal
nodes denoted by \nn; $\m{C} \in \real^{\nn \times \nn}$ and $\m{G} \in
\real^{\nn \times \nn}$ are a diagonal matrix of the thermal capacitance and a
symmetric, positive-definite matrix of the thermal conductance, respectively;
$\vs \in \real^\nn$ is a vector of the difference between the temperature of the
thermal nodes and the ambient temperature; $\vp \in \real^\np$ and $\m{M} \in
\real^{\nn \times \np}$ are a vector of the power dissipation of the processing
elements and its mapping to the thermal nodes, respectively; $\vq \in \real^\np$
is a vector of the temperature of the processing elements; and $\vq_\ambient \in
\real^\np$ is a vector of the ambient temperature. $\m{M}$ distributes power
across the thermal nodes. Assuming that one processing element is mapped onto
one thermal node, $\m{M}$ is filled in with zeros except for \np elements equal
to unity that are located on the main diagonal. For convenience, we perform an
auxiliary transformation of the system in \eref{fourier-system-original} using
\cite{ukhov2012}
\begin{align*}
  & \vs = \m{C}^\frac{1}{2} \vs, \\
  & \m{A} = -\m{C}^{-\frac{1}{2}} \m{G} \m{C}^{-\frac{1}{2}}, \text{ and} \\
  & \m{B} = \m{C}^{-\frac{1}{2}} \m{M}
\end{align*}
and obtain the system in \eref{fourier-system} where the coefficient matrix
$\m{A}$ preserves the symmetry and positive-definiteness of $\m{G}$. In general,
the differential part in \eref{fourier-system-original} (and in
\eref{fourier-system}) is nonlinear due to the source term $\vp(t)$ since we do
not make any assumptions about its structure (see the discussion in
\sref{power-model}). Therefore, there is no closed-form solution to the system.

The time intervals of the power and temperature profiles are assumed to be short
enough such that the total power of a processing element can be approximated by
a constant within one interval. In this case, \eref{fourier-original} (and
\eref{fourier-de}) is a system of linear differential equations that can be
solved analytically. The solution is as follows \cite{ukhov2012}:
\begin{equation} \elab{ode-solution}
  \vs(t) = \m{E}(t) \vs(0) + \m{F}(t) \vp(0)
\end{equation}
where $t$ is restricted to one time interval, $\vp(0)$ is the power dissipation
at the beginning of the time interval with respect to the corresponding
temperature,
\begin{align*}
  & \m{E}(t) = e^{\m{A} t} \in \real^{\nn \times \nn}, \text{ and} \\
  & \m{F}(t) = \m{A}^{-1} (e^{\m{A} t} - \m{I}) \m{B} \in \real^{\nn \times \np}.
\end{align*}
The procedure is to be repeated for all \ns time intervals starting from the
initial temperature, which, without loss of generality, is assumed to be equal
to the ambient temperature. Note that, when the power profile is evenly sampled,
the coefficient matrices $\m{E}(t)$ and $\m{F}(t)$ are constant and can be
efficiently computed using the technique in \cite{ukhov2012}. It is also worth
noting that the described solution method belongs to the family of so-called
exponential integrators, which have good stability properties; refer to
\cite{hochbruck2010} for an overview. Finally, taking into account \vu, we
obtain \eref{recurrence}, operating on stochastic quantities.

\section{Model Order Reduction}

This section contains a description of the discrete Karhunen-Lo\`{e}ve
decomposition \cite{ghanem1991}, which is utilized at Stage~1. We shall use the
notation introduced in \sref{ie-uncertain-parameters}. Let $\m{\Sigma}_{\vz'}$
be the covariance matrix of the centered random vector $\vz'$ (which is the
result of the first step of the Nataf transformation discussed in
\sref{ie-uncertain-parameters}). Since any covariance matrix is real and
symmetric, $\m{\Sigma}_{\vz'}$ admits the eigenvalue decomposition as
$\m{\Sigma}_{\vz'} = \m{V} \m{\Lambda} \m{V}^T$ where $\m{V}$ and $\m{\Lambda}$
are an orthogonal matrix of the eigenvectors and a diagonal matrix of the
eigenvalues of $\m{\Sigma}_{\vz'}$, respectively. $\vz'$ can then be represented
as $\vz' = \m{V} \m{\Lambda}^\frac{1}{2} \vz''$ where the vector $\vz''$ is
centered, normalized, and uncorrelated, which is also independent as $\vz'$ is
Gaussian.

The aforementioned decomposition provides means for model order reduction. The
intuition is that, due to the correlations possessed by $\vz' \in \real^\nu$,
this vector can be recovered from a small subset $\vz''' \in \real^\nz$ of
$\vz'' \in \real^\nu$, $\nz \ll \nu$. Such redundancies can be revealed by
analyzing the eigenvalues $\lambda_i$ stored in $\m{\Lambda}$. Assume
$\lambda_i$, $\forall i$, are arranged in a non-increasing order and let
$\tilde{\lambda}_i = \lambda_i / \sum_j \lambda_j$. Gradually summing up the
arranged and normalized eigenvalues $\tilde{\lambda}_i$, we can identify a
subset of them that has the cumulative sum greater than a certain threshold.
When this threshold is sufficiently high (close to one), the rest of the
eigenvalues and the corresponding eigenvectors can be dropped as being
insignificant, reducing the stochastic dimensionality of the problem.

\section{Numerical Integration}

As mentioned in \sref{polynomial-chaos}, \sref{ie-polynomial-chaos}, and
\sref{polynomial-chaos}, the coefficients of PC expansions are integrals, which
should be calculated numerically at Stage~4. In numerical integration, an
integral of a function is approximated by a summation over the function values
computed at a set of prescribed points, or nodes, which are multiplied by the
corresponding prescribed weights. Such pairs of nodes and weights are called
quadrature rules. A one-dimensional quadrature rule is characterized by its
precision, which is defined as the maximum order of polynomials that the rule
integrates exactly \cite{heiss2008}. In multiple dimensions, an \nu-variate
quadrature rule is formed by tensoring one-dimensional counterparts. Such a
multidimensional rule is characterized by its accuracy level \lq, which is
defined as the index of the rule in the corresponding family of multidimensional
rules with increasing precision.

It can be seen in \eref{inner-product} that the integrand can be decomposed into
two parts: the weight function $f$ and everything else. The former always stays
the same; therefore, a rule is typically chosen in such a way that this
``constant'' part is automatically taken into consideration by the corresponding
weights since there is no point of recomputing $f$ each time when the other
part, that is, the functions that the inner product operates on, changes. In
this regard, there exist different families of quadrature rules tailored for
different weight functions. Define such a quadrature-based approximation of
\eref{inner-product} by
\begin{equation} \elab{quadrature}
  \innerproduct{h}{g} \approx \quadrature{\nu}{\lq}{h g}
  = \sum_{i = 1}^\nq h(\hat{\v{x}}_i) g(\hat{\v{x}}_i) w_i
\end{equation}
where $\hat{\v{x}}_i \in \real^\nu$ and $w_i \in \real$ are the prescribed
points and weights, respectively; \nq is their number; and \lq is the accuracy
level of the quadrature rule, which is said to be \nu-variate. It is important
to note that $\hat{\v{x}}_i$ and $w_i$ do not change when the quantity being
integrated changes. Thus, once the rule to use has been identified, it can be
utilized to compute the inner product of arbitrary $h$ and $f$ with no
additional computational effort. In our experiments in
\sref{experimental-results}, we use the library of quadrature rules available at
\cite{burkardt2013}.

Since in the example in \sref{illustrative-example} we need to compute the inner
product with respect to beta measures, the Gauss-Jacobi quadrature rule is of
particular interest. The rule belongs to a broad class of rules known as
Gaussian quadratures. The precision of a one-dimensional Gaussian quadrature
with $\hat{\nq}$ points is $2 \hat{\nq} - 1$; this feature makes such
quadratures especially efficient \cite{heiss2008}. Using \eref{quadrature}, we
rewrite \eref{pc-coefficients} as shown in \eref{pc-coefficients-quadrature}
where $\{ \innerproduct{\psi_i}{\psi_i} \}_{i = 1}^\nc$ are computed exactly,
either by applying the same quadrature rule or by taking products of the
one-dimensional counterparts with known analytical expressions \cite{xiu2010};
the result is further tabulated. It is important to note that \lq should be
chosen in such a way that the rule is exact for polynomials of the total order
at least $2 \lc$, that is, twice the order of PC expansions, which can be seen
in \eref{pc-coefficients} \cite{eldred2008}. Therefore, $\lq \geq \lc + 1$ as
the quadrature is Gaussian.

There is one more and arguably the most crucial aspect of numerical integration
that we ought to discuss: the algorithm used to construct multidimensional
quadratures. In low dimensions, the construction can be easily based on the
direct tensor product of one-dimensional rules. However, in high dimensions, the
situation changes dramatically as the number of points produced by this approach
can easily explode. For instance \cite{heiss2008}, if a one-dimensional rule has
only four nodes, that is, $\hat{\nq} = 4$, then in 10 stochastic dimensions,
that is, $\nu = 10$, the number of multivariate nodes becomes $\nq =
\hat{\nq}^\nu = 4^{10} = 1\;048\;576$, which is not affordable. Moreover, it can
be shown that most of the points obtained in such a way do not contribute to the
asymptotic accuracy and, therefore, are a waste of time. In order to effectively
alleviate this problem, we construct so-called sparse grids using the Smolyak
algorithm \cite{eldred2008, heiss2008, burkardt2013}. The algorithm preserves
the accuracy of the underlying one-dimensional rules for complete polynomials
while significantly reducing the number of integration nodes. For instance, in
the example given earlier, the number of points computed by the algorithm would
be only $1\;581$, which implies drastic savings of the computational time.
