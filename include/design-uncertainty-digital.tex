In this chapter, we continue elaborating on designing with uncertainty. However,
the source of uncertainty is different here. Specifically, the source is the
workload that the system under consideration is supposed to process.

\section{Introduction}
\inputsection{introduction}

\section{Motivational Example}
\slab{interpolant-example}
\inputsection{example}

\section{Problem Formulation}
\slab{interpolant-problem}
\inputsection{problem}

\section{Prior Work}
\slab{interpolant-prior}
\inputsection{prior}

\section{Our Solution}
\slab{interpolant-solution}
\inputsection{solution}

\section{Post-Processing}

In \sref{modeling}, we formalized the uncertainty affecting electronic systems
and discussed several aspects of such systems along with metrics \g, which the
designer is interested in evaluating. In \sref{interpolation}, we obtained an
efficient interpolation algorithm for approximating hypothetical
multidimensional functions \f. We shall now amalgamate the ideas developed in
the aforementioned two sections.

Given an electronic system dependent on a number of uncertain parameters $\vu:
\Omega \to \real^\nu$, the goal is to analyze a metric \g representing a certain
aspect of the system. For instance, \vu can correspond to the execution times of
the tasks, and \g can correspond the total energy consumed by the processing
elements, as we exemplify in \sref{time} and \sref{power}. The goal is attained
as follows; recall \fref{example}.

1) The parameterization of \g is changed from \vu to random variables $\vz:
\Omega \to [0, 1]^\nz$ via a suitable transformation $\transform$; this stage is
described in \sref{parameters}. 2) An interpolant of the resulting composition
$\g \circ \transform$ is constructed by treating the composition as a
deterministic function \f of \vz; this stage is detailed in
\sref{interpolation}. 3) An estimation of the probability distribution of \g is
undertaken in the usual sampling-based manner but relying solely on the
constructed interpolant; \g is no longer involved. This last stage boils down to
drawing independent samples from $F_{\vz}$ and evaluating the interpolant
$\interpolant{n}{l}(\f) \equiv \interpolant{n}{l}(\g \circ \transform)$ at those
points. Having collected samples of \g, other statistics about \g, such as
probabilities of particular events, can be straightforwardly estimated. We do
not discuss this estimation stage any further as it is standard.

There are two aspects concerning the usage of the proposed framework that we
would like to cover in what follows.

\subsection{Expectation and Variance}

Since the expected value and variance, which are defined in \eref{expectation}
and \eref{variance}, respectively, usually draw particular attention, we would
like to elaborate on them separately.

As shown in \sref{parameters}, \g can be reparameterized in terms of independent
variables that are uniformly distributed on $[0, 1]^\nz$. This means that the
probability density function of \vz simply equals to one. Therefore, using
\eref{expectation} and \eref{approximation}, we have
\[
  \expectation{\g} \approx \expectation{\interpolant{n}{l}(\f)} = \int_{[0, 1]^\nz} \interpolant{n}{l}(\f)(\vz) \, \d\vz = \sum_{\vi \in \sparseindex{n}{l}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) w_{\vi \vj}
\]
where
\[
  w_{\vi \vj} = \int_{[0, 1]^\nz} e_{\vi \vj}(\vz) \d\vz = \prod_{k = 1}^\nz \int_0^1 e_{i_k j_k}(\z_k) \d\z_k = \prod_{k = 1}^\nz w_{i_k j_k}.
\]
In the above equation, $w_{ij}$ is as shown in \eref{volume}. Consequently, we
have obtained an analytical formula for the expected value of \g, which does not
require any additional sampling.

Regarding the variance of \g, it can be seen in \eref{variance} that the
variance can be assembled from two components: the expected value of \g, which
we already have, and the expected value of $\g^2$, which we are missing. The
solution is to let $\h = (\g, \g^2)$ be the metric instead of \g. Then the
expected values of both \g and $\g^2$ will be available in analytical forms, and
the variance of \g can be computed using \eref{variance}. This approach can be
generalized to probabilistic moments of higher orders.

\subsection{Multiple Outputs}

The careful reader has noted a problem with the calculation of variance in the
previous subsection: \h is vector valued. More generally, the metric \g in
\sref{modeling} and the function \f in \sref{interpolation} have been depicted
as having one-dimensional codomains. This, however, has been done only for the
sake of clarity. All the mathematics and pseudocodes stay the same for
vector-valued functions. The only except is that, since a surplus
$\Delta\f(\vx_{\vi \vj})$ naturally inherits the output dimensionality of \f,
the operations that involve $\Delta\f(\vx_{\vi \vj})$ should be adequately
adjusted. If the outputs are on different scales and/or have different accuracy
requirements, one might want to have different $\epsilon_a$ and $\epsilon_r$ in
\eref{stopping-condition} for different outputs. In that case, one also needs to
device a more sensible strategy for scoring collocation nodes in \eref{score}
such as rescaling individual outputs and then calculating the uniform norm
$\norm[\infty]{\cdot}$ or $\L{2}$ norm $\norm[2]{\cdot}$. Our code
\cite{sources} has been written with multiple outputs in mind.

To summarize, once an interpolant of \g has been constructed, the distribution
of \g is estimated using versatile sampling methods applied to the interpolant.
The framework extends naturally to metrics with multiple outputs, and it
provides analytical formulae for expectations and variances.

Let us remind that the evaluation of \g is an extensive operation. Our technique
is designed to keep this expense as low as possible by choosing the evaluation
points adaptively, which is unlike traditional sampling methods. Moreover, in
contrast to \up{PC} expansions and similar techniques, the proposed framework is
well suited for the nonsmooth response surfaces.

\section{Illustrative Application}
\slab{interpolant-application}

The agenda for this section is as follows. In \sref{parameters}, the uncertain
parameters \vu are transformed into a form suitable for the subsequent
calculations. This stage is an essential part of our framework, and it is
denoted by $\transform$ in \fref{example}. The rest of the subsections,
\sref{time}--\ref{sec:temperature}, serve a strictly illustrative purpose. They
exemplify the leftmost box in \fref{example} in order to give the reader a
better intuition about the utility of the framework. The subsections introduce a
number of models and a number of metrics \g; however, it should be well
understood that the essence of \g is problem specific. In practice, \g stands
for an adequate simulator of the system under consideration. The modeling
capabilities of this simulator are naturally inherited by the proposed
framework.

\subsection{Probability Transformation}

Let us consider an example of $\transform$ in order to understand the concept
better. Assume that \vu is specified by a set of marginal distribution functions
$\{ F_i \}_{i = 1}^\nu$ and a Gaussian copula whose correlation matrix is
$\correlation{\vu}$; such a copula can be constructed as it is outlined in
\sref{chaos-formulation}. The transformation $\transform$ is
\[
  \vu = F^{-1} \left(\Phi\left(\m{U} \tm{\Lambda}^\frac{1}{2} \Phi^{-1}(\vz)\right)\right)
\]
where the random variables $\vz: \Omega \to \real^\nz$ are independent and
uniformly distributed on $[0, 1]^\nz$; $\Phi$ and $\Phi^{-1}$ are the
distribution function of the standard Gaussian distribution and its inverse,
respectively, which are applied element-wise; and $F_{\vu}^{-1} = F_{\u_1}^{-1}
\times \cdots \times F_{\u_\nz}^{-1}$ is the Cartesian product of the inverse
marginal distributions of \vu, which are applied to the corresponding element of
the vector yielded by $\Phi$. In the absence of correlations,
\eref{transformation-concrete} is simply $\vu = F_{\vu}^{-1}(\vz)$, and no
model-order reduction is possible ($\nu = \nz$).

To summarize, we have found such a transformation $\transform$ and the
corresponding random vector $\vz \sim F_{\vz}$ that: 1) $F_{\vz}$ is supported
by $[0, 1]^\nz$, and 2) \vz has the smallest number of dimensions \nz needed to
preserve $\eta$ portion of the variance. Let us emphasize that this $\transform$
is an example; the framework works with any $\transform$ that yields $\vz \sim
F_{\vz}$ for some \nz.

\subsection{Application Timing}

Suppose the application is given as a directed acyclic graph. The vertices
represent tasks, and the edges data dependency between these tasks. Suppose
further that a static cyclic scheduling policy is utilized. Note, however, these
assumptions are orthogonal to our framework: the framework can be applied to any
application model and any scheduling policy.

Each task has a start and a finish time. For task $i$, denote these two time
moments by $b_i$ and $d_i$, respectively, and let $\v{b} = (b_i)_{i = 1}^\nt$
and $\v{d} = (d_i)_{i = 1}^\nt$. Other timing characteristics of the application
can be derived from $(\v{b}, \v{d})$. An example is the end-to-end delay, which
is the difference between the finish time of the latest task and the start time
of the earliest task:
\[
  \text{End-to-end delay} = \max_{i = 1}^\nt \, d_i - \min_{i = 1}^\nt \, b_i.
\]

Suppose the execution times of the tasks depend on \vu (see \sref{problem}).
Then the tuple $(\v{b}, \v{d})$ depends on \vu. Then the end-to-end delay given
in \eref{end-to-end-delay} depends on \vu and is a potential metric \g; it is
used in \fref{example}. Note that this \g is nondifferentiable as the $\max$ and
$\min$ functions are such. Hence, \g is nonsmooth, which renders \up{PC}
expansions and similar techniques inadequate for this problem, as illustrated in
\sref{introduction}.

\begin{remark}
In general, the behavior of \g with respect to continuity, differentiability,
and smoothness cannot be inferred from the behavior of \vu. Even when the
parameters are perfectly behaved, \g can still and likely will exhibit
nondifferentiability or even discontinuity, which depends on how \g works
internally. For example, as shown in \cite{tanasa2015}, even if execution times
of tasks are continuous, due to the actual scheduling policy, end-to-end delays
are very often discontinuous.
\end{remark}

\subsection{Power Consumption}

Denote the number of processing elements present on the platform by \np. Let the
dynamic power consumed by task $j$ when running on processing element $i$ be
fixed during the execution of the task and denote this dynamic power by
$\p^\dynamic_{ij}$. The fact that $\p^\dynamic_{ij}$ is constant might seem
restrictive. However, one should keep in mind that it is an example. Our
framework does not have such a restriction. Even in this simple model, the
modeling accuracy can be substantially improved by representing large tasks as
sequences of smaller tasks.

Let the vector $\vp(t) = (\p_i(t))_{i = 1}^\np$ capture the total power
consumption of the system at time $t$. This vector is related to the dynamic
power introduced above as follows:
\[
  \p_i(t) = \sum_{j = 1}^\nt \p^\dynamic_{ij} \: \delta_{ij} (t) + \p^\static_i(t), \text{ for $i = \range{1}{\np}$},
\]
where $\delta_{ij}(t)$ is an indicator function (outputs either zero or one) of
the event that processing element $i$ executes task $j$ at time $t$, and
$\p^\static_i(t)$ is the static power consumed by processing element $j$ at time
$t$. The last component depends on time because the leakage power and
temperature are interdependent \cite{liu2007}, and temperature changes over time
(see the next subsection).

Given a set of \ns points on the timeline $\{ t_i \}_{i = 1}^\ns$, \eref{power}
can be used to construct a power profile of the system as follows:
\[
  \mp = (\p_i(t_j))_{i = 1, j = 1}^{\np, \ns} \in \real^{\np \times \ns}.
\]
The above is a matrix where row $i$ captures the power consumed by processing
element $i$ at the \ns time moments.

The total energy consumed by the system during an application run can be
computed by integrating \eref{power} over the time span of the
application---which is demarcated by the minuend and subtrahend in
\eref{end-to-end-delay}---and the corresponding integral can be estimated using
the power profile as follows:
\[
  \text{Total energy} = \sum_{i = 1}^\np \int \p_i(t) \, \d t \approx \sum_{i = 1}^\np \sum_{j = 1}^\ns \p_i(t_j) \, \Delta t_j
\]
where $\Delta t_j$ is either $t_j - t_{j - 1}$ or $t_{j + 1} - t_j$, depending
on how power values are encoded in \mp. The assumption that \eref{total-energy}
is based on is that each $\Delta t_i$ is sufficiently small so that the power
consumed within the interval does not change significantly.

Since the tuple $(\v{b}, \v{d})$ depends on \vu, the power consumption of the
system depends on \vu too. Consequently, the total energy given in
\eref{total-energy} depends on \vu and is a candidate for \g. Note that
\rref{smoothness} applies in this context to the full extent.

\subsection{Heat Dissipation}

Based on the specification of the platform including its thermal package, an
equivalent thermal \up{RC} circuit is constructed \cite{skadron2004}. The
circuit comprises \nn thermal nodes, and its structure depends on the intended
level of granularity, which impacts the resulting accuracy. For clarity, we
assume that each processing element is mapped onto one corresponding node, and
the thermal package is represented as a set of additional nodes.

The thermal dynamics of the system are modeled using the following system of
differential-algebraic equations \cite{ukhov2014, ukhov2012}:
\begin{subnumcases}{}
  \m{C} \frac{\d\vs(t)}{\d t} + \m{G} \vs(t) = \m{M} \vp(t) \\
  \vq(t) = \m{M}^T \vs(t) + \vq_\ambient
\end{subnumcases}
The coefficients $\m{C} \in \real^{\nn \times \nn}$ and $\m{G} \in \real^{\nn
\times \nn}$ are a diagonal matrix of thermal capacitance and a symmetric,
positive-definite matrix of thermal conductance, respectively. The vectors
$\vp(t) \in \real^\np$,  $\vq(t) \in \real^\np$, and $\vs(t) \in \real^\nn$
correspond the system's power, temperature, and internal state at time $t$,
respectively. The vector $\vq_\ambient \in \real^\np$ contains the ambient
temperature. The matrix $\m{M} \in \real^{\nn \times \np}$ is a mapping that
distributes the power consumption of the processing elements across the thermal
nodes; without loss of generality, $\m{M}$ is a rectangular diagonal matrix
whose diagonal elements are equal to one.

Given a set of \ns points on the timeline $\{ t_i \}_{i = 1}^\ns$,
\eref{thermal-system} can be used to compute a temperature profile of the system
as follows:
\[
  \mq = (\q_i(t_j))_{i = 1, j = 1}^{\np, \ns} \in \real^{\np \times \ns}.
\]
Then the maximum temperature of the system can be estimated using the
temperature profile as follows:
\[
  \text{Max temperature} = \max_{i = 1}^\np \, \sup_{t} \, \q_i(t) \approx \max_{i = 1}^\np \max_{j = 1}^\ns \, \q_i(t_j).
\]

Since the power consumption of the system is affected by \vu (see \sref{power}),
the system's temperature is affected by \vu as well. Therefore, the temperature
in \eref{maximum-temperature} can be considered as a metric \g. Note that, due
to the maximization involved, the metric is nondifferentiable and, hence, cannot
be adequately addressed using polynomial approximations, specially taking into
account the concern in \rref{smoothness}.

To sum up, we have discussed the transformation that needs to be applied to \vu
prior to the interpolation of \g. We have also covered three aspects of
electronic systems, namely, timing, power, and temperature, and introduced a
number of metrics associated with them; we shall come back to these metrics in
the section on experimental results, \sref{experiments}.

\subsection{Example}

In this section, we apply our framework to a small problem in order to get a
better understanding of the workflow of the framework. A detailed description of
our experimental setup is given in \sref{configuration}; here we give only the
bare minimum.

The addressed problem is depicted in \fref{example}. We consider a platform with
two processing elements, PE1 and PE2, and an application with four tasks,
T1--T4. The data dependencies between T1--T4 and their mapping onto PE1 and PE2
can be seen in \fref{example}. The metric \g is the end-to-end delay of the
application. The uncertain parameters \vu are the execution times of T2 and T4
denoted by $\u_1$ and $\u_2$, respectively.

The leftmost box in \fref{example} represents a simulator of the system at hand,
and it could involve such tools as Sniper \cite{carlson2011}. It takes an
assignment of the execution times of T2 and T3, $\u_1$ and $\u_2$, and outputs
the calculated end-to-end delay \g. The second box corresponds to the
reparameterization mentioned in \sref{solution} (to be discussed in
\sref{parameters}). It converts the auxiliary variables $\z_1$ and $\z_2$ into
$\u_1$ and $\u_2$ in accordance with $\u_1$ and $\u_2$'s joint distribution. The
third box is our interpolation engine (to be discussed in \sref{interpolation}).
Using a number of strategic invocations of the simulator, the interpolation
engine yields a light surrogate for the simulator; the surrogate corresponds to
the slim box with rounded corners. Having obtained such a surrogate, one
proceeds to sampling extensively the surrogate via a sampling method of choice
(the rightmost box). The surrogate takes $\z_1$ and $\z_2$ and returns an
approximation of \g at that point. Recall that the computation cost of this
extensive sampling is negligible as \g is not involved. The samples are then
used to compute an estimate of the distribution of \g.

In the graph on the right-hand side of \fref{example}, the blue line shows the
probability density function of \g computed by applying kernel density
estimation to the samples obtained from our surrogate. The yellow line (barely
visible behind the blue line) shows the true density of \g; its calculation is
explained in \sref{experiments}. It can be seen that our solution closely
matches the exact one. In addition, the orange line shows the estimation that
one would get if one sampled \g directly 156 times and used only those samples
in order to calculate the density of \g. We see that, for the same budget of
simulations, the solution delivered by our framework is substantially closer to
the true one than the one delivered by na\"{i}ve sampling.

At this point, we are ready to present to the proposed framework. We begin by
elaborating on the modeling of uncertain parameters and metrics of interest. We
shall then proceed to the interpolation engine (\sref{interpolation}).

\section{Experimental Results}
\slab{interpolant-results}

In this section, we evaluate the performance of our framework. Our
implementation is open source and can be found at \cite{sources}, which also
includes the experimental setup along with configuration files and input data.
The experiments discussed below are conducted on a \up{GNU}/Linux machine
equipped with 16 processors Intel Xeon E5520 2.27~\up{GH}z and 24~\up{GB} of
\up{RAM}.

We shall address $3 \times 2 \times 3 = 18$ uncertainty-quantification problems.
Specifically, we shall consider three platform sizes \np: 2, 4, and 8 processing
elements; two application sizes \nt: 10 and 20 tasks; and three metrics \g: the
end-to-end delay, total energy consumption, and maximum temperature defined in
\eref{end-to-end-delay}, \eref{total-energy}, and \eref{maximum-temperature},
respectively. At this point, it might be helpful to recall the example in
\fref{example}.

In addition to the aforementioned contribution, we open-source our
implementation \cite{sources}. The code base also includes the whole
experimental setup described in \sref{experiments}.

\hiddensubsection{Configuration}

A platform with \np processing elements and an application with \nt tasks are
generated randomly by the \up{TGFF} tool \cite{dick1998}. The tool generates \np
tables and a directed acyclic graph with \nt nodes. Each table corresponds to a
processing element, and it describes certain properties of the tasks when they
are mapped to that particular processing element. Namely, each table assigns two
numbers to each task: a reference execution time, chosen uniformly between 10
and 50~ms, and a power consumption, chosen uniformly between 5 and 25~W. The
graph captures data dependencies between the tasks. The application is scheduled
using a list scheduler \cite{adam1974}. The mapping of the application is fixed
and obtained by scheduling the tasks based on their reference execution times
and assigning them to the earliest available processing elements (a shared ready
list).

The construction of thermal \up{RC} circuits needed for temperature analysis is
delegated to the HotSpot tool \cite{skadron2003}. The floorplan of each platform
is a regular grid wherein each processing element occupies $2 \times
2~\text{mm}^2$ on the die. The output of the tool is a pair of a thermal
capacitance matrix $\m{C}$ and a thermal conductance $\m{G}$ matrix used in
\eref{thermal-system}. The leakage modeling is based on a linear fit to a data
set of \up{SPICE} simulations of a series of \up{CMOS} invertors
\cite{ukhov2012, liu2007}; see also \cite{ukhov2014}. The time step of power and
temperature profiles is constant and equal to one microsecond; see \sref{power}
and \sref{temperature}.

The uncertain parameters \vu introduced in \sref{problem} are the execution
times of the tasks; see \sref{time}. All other parameters are deterministic.
Targeting the practical scenario described in \sref{parameters}, the marginal
distributions and correlation matrix of \vu are assumed to be available. Without
loss of generality, the marginal of $\u_i$ is a four-parametric beta
distribution $\text{Beta}(\alpha_i, \beta_i, a_i, b_i)$ where $\alpha_i$ and
$\beta_i$ are the shape parameters, and $a_i$ and $b_i$ are the endpoints of the
support. The left $a_i$ and right $b_i$ endpoint are set to 80\% and 120\%,
respectively, of the reference execution time generated by the \up{TGFF} tool as
described earlier. The parameter $\alpha_i$ and $\beta_i$ are set to two and
five, respectively, for all tasks, which skews the distribution toward the left
endpoint. The execution times of the tasks are correlated based on the structure
of the graph produced by the \up{TGFF} tool: the closer task $i$ and task $j$
are in the graph as measured by the number of edges between vertex $i$ and
vertex $j$, the stronger $\u_i$ and $\u_j$ are correlated. The model-order
reduction parameter $\eta$ in \eref{reduction} (\sref{parameters}) is set to
0.9, which results in $\nz = 2$ and 3 preserved variables for applications with
$\nt = 10$ and 20 tasks, respectively.

The configuration of the interpolation algorithm (the collocation nodes, basis
functions, and adaptation strategy with stopping conditions) is as described in
\sref{interpolation}. The parameters $\epsilon_a$, $\epsilon_r$, and
$\epsilon_s$ are around $10^3$, $10^2$, and $10^4$, respectively, depending on
the problem; the exact values can be found at \cite{sources}, which, again,
contains all other details too.

The performance of our framework with respect to each problem is assessed as
follows. First, we obtain the ``true'' probability distribution of the metric in
question \g by sampling \g directly and extensively. Direct sampling means that
samples are drawn from \g itself (not from a surrogate), and that there is no
any intermediate model-order reduction (see \sref{parameters}). Second, we
construct an interpolant for \g and estimate \g's distribution by sampling the
interpolant. In both cases, we draw $10^5$ samples; let us remind, however, that
the cost of sampling the interpolant is practically negligible. Third, we
perform another round of direct sampling of \g, but this time we draw as many
samples as many times the metric was evaluated during the interpolation process.
In each of the three cases, the sampling is undertaken in accordance with a
Sobol sequence, which is a quasi-random low-discrepancy sequence featuring much
better convergence properties than those of the classical Monte-Carlo (\up{MC})
sampling \cite{joe2008}.

As a result, we obtain three estimates of \g's distribution: reference (the one
considered true), proposed (the one interpolation powered), and direct (the one
equal in terms of the number of \g's evaluations to the proposed solution). The
last two are compared with the first one. For comparing the proximity between
two distributions, we use the well-known Kolmogorov--Smirnov (\up{KS}) statistic
\cite{rao2002}, which is the supremum over the distance (pointwise) between two
empirical distribution functions and, hence, is a rather unforgiving error
indicator.

\hiddensubsection{Discussion}

The results of all 18 uncertainty-quantification problems are given in
\fref{results} as a 6-by-3 grid of plots, one plot per problem. The three
columns correspond to the three metrics at hand: the end-to-end delay (left),
total energy (middle), and maximum temperature (right). The three pairs of rows
correspond to the three platform sizes: 2 (top), 4 (middle), and 8 (bottom)
processing elements. The rows alternate between the two application sizes: 10
(odd) and 20 (even) tasks.

The horizontal axis of each plot shows the number of points, that is,
evaluations of the metric \g, and the vertical one shows the \up{KS} statistic
on a logarithmic scale. Each plot has two lines. The solid line represents our
technique. The circles on this line correspond to the steps of the interpolation
process given in \eref{approximation}. They show how the \up{KS} statistic
computed with respect to the reference solution changes as the interpolation
process takes steps (and increases the number of collocation nodes) until the
stopping condition is satisfied (\sref{adaptivity}). Note that only a subset of
the actual steps is displayed in order to make the figure legible. Synchronously
with the solid line (that is, for the same numbers of $\g$'s evaluations), the
dashed line shows the error of direct sampling, which, as before, is computed
with respect to the reference solution.

Let us first describe one particular problem shown in \fref{results}. Consider,
for instance, the one labeled with $\bigstar$. It can be seen that, at the very
beginning, our solution and the solution of direct sampling are poor. The
\up{KS} statistic tells us that there are substantial mismatches between the
estimates and the reference solution. However, as the interpolant is being
adaptively refined, our solution approaches rapidly the reference one and, by
the end of the interpolation process, leaves the solution of na\"{i}ve sampling
approximately an order of magnitude behind.

Studying \fref{results}, one can make a number of observations. First and
foremost, our interpolation-powered approach (solid lines) to probabilistic
analysis outperforms direct sampling (dashed lines) in all the cases. This means
that, given a fixed budget of the computation time---the probability
distributions delivered by our framework are much closer to the true ones than
those delivered by sampling $\g$ directly, despite the fact that the latter
relies on Sobol sequences, which are a sophisticated sampling strategy. Since
direct sampling methods try to cover the probability space impartially,
\fref{results} is a salient illustration of the difference between being
adaptive and nonadaptive.

It can also be seen in \fref{results} that, as the number of evaluations
increases, the solutions computed by our technique approach the exact ones. The
error of our framework decreases generally steeper than the one of direct
sampling. The decrease, however, tends to plateau toward the end of the
interpolation process (when the stopping condition is satisfied). This behavior
can be explained by the following two reasons. First, the algorithm has been
instructed to satiate certain accuracy requirements ($\epsilon_a$, $\epsilon_r$,
and $\epsilon_s$), and it reasonably does not do more than what has been
requested. Second, since the model-order reduction mechanism is enabled in the
case of interpolation, the metric being interpolated is not \g, strictly
speaking; it is a lower-dimensional representation of \g, which already implies
an information loss. Therefore, there is a limit on the accuracy that can be
achieved, which depends on the amount of reduction.

The message of the above observations is that the designer of an electronic
system can benefit substantially in terms of accuracy per computation time by
switching from direct sampling to the proposed technique. If the designer's
current workhorse is the classical \up{MC} sampling, the switch might lead to
even more dramatic savings than those shown in \fref{results}. Needless to
mention that the gain is especially prominent in situations where the analysis
needs to be performed many times such as when it resides in a design-space
exploration loop.

\begin{remark}
The wall-clock time taken by the experiments is not reported in this paper
because this time is irrelevant: since the evaluation of \g is time consuming
(see \sref{problem}), the number of \g's evaluations is the most apposite
expense indicator. For the curious reader, however, let us give an example by
considering the problem labeled with $\clubsuit$ in \fref{results}. Obtaining a
reference solution with $10^5$ simulations in parallel on 16 processors took us
around two hours. Constructing an interpolant with 383 collocation nodes took
around 30 seconds (this is also the time of direct sampling with 383 simulations
of \g). Evaluating the interpolant $10^5$ times took less than a second. The
relative computation cost of sampling an interpolant readily diminishes as the
complexity of \g increases; contrast it with direct sampling, whose cost grows
proportional to \g's evaluation time.
\end{remark}

\hiddensubsection{Real-life Example}

Last but not least, we investigate the viability of deploying the proposed
framework in a real environment. It means that we need to couple the framework
with a battle-proven simulator, which is used in both academia and industry, and
let it simulate a real application running on a real platform. Before we
proceed, we would like to remind that all the implementation and configuration
details including the infrastructure developed for this example can be found at
\cite{sources}.

The scenario that we consider is the same as the one depicted in \fref{example}
except for the fact that an industrial-standard simulator is put in place of the
``black box'' on the left side, and that the metric of interest \g is now the
total energy. Unlike the previous examples, there is no true solution to compare
with due to the prohibitive expense of the simulator, which is exactly why our
framework is needed in such cases.

The simulator of choice is the well-known and widely used combination of Sniper
\cite{carlson2011} and McPAT \cite{li2009}. The architecture that we simulate is
Intel's Nehalem-based Gainestown series. Sniper is distributed with a
configuration file for this architecture, and we use it without any changes. The
platform is configured to have three \up{CPU}s sharing one \up{L3} cache.

The application that has been chosen for simulation is \up{VIPS}, which is an
image-processing piece of software taken from the \up{PARSEC} benchmark suite
\cite{bienia2011}. In this scenario, \up{VIPS} applies a fixed set of operations
to a given image. The width and height of the image to process are considered as
the uncertain parameters \vu (see \sref{problem}), which are assumed to be
distributed uniformly within certain ranges.

The real-life deployment has fulfilled our expectations. The interpolation
process successfully finished and delivered a surrogate after 78 invocations of
the simulator. Each invocation took 40 minutes on average. The probability
distribution of the total energy was then estimated by sampling the constructed
surrogate $10^5$ times. These many samples would take around 6 months to obtain
on our machine if we sampled the simulator directly in parallel on 16
processors; using the proposed technique, the whole procedure took approximately
9 hours.

\section{Conclusion}
\slab{interpolant-conclusion}

In this paper, we have presented a framework for probabilistic analysis of
electronic systems. Given a description of the probability distribution of the
uncertain parameters present in the system under consideration and a simulator
of a metric of interest dependent on the parameters, the framework prescribes
the steps that need to be taken in order to computationally efficiently obtain
the probability distribution of the metric.

The proposed approach is powered by hierarchical interpolation following a
hybrid adaptation strategy. The adaptivity makes the framework particularly
suited for problems with idiosyncratic behaviors and steep response surfaces,
which arise in electronic systems due to their digital nature.

The performance of our framework has been assessed by comparing it with the
performance of an advanced sampling technique. The experimental results have
shown that, for a fixed budget of evaluations of the metric, our approach
achieves higher accuracy compared to direct simulations.

Finally, we would like to emphasize that, even though the framework has been
exemplified by considering a specific source of uncertainty and specific
metrics, it is general and can be successfully applied in many other settings.

We develop a framework for system-level analysis of electronic systems whose
runtime behaviors depend on uncertain parameters. The proposed approach thrives
on hierarchical interpolation guided by an advanced adaptation strategy, which
makes the framework general and suitable for studying various metrics that are
of interest to the designer. Examples of such metrics include the end-to-end
delay, total energy consumption, and maximum temperature of the system under
consideration. The framework delivers a light generative representation that
allows for a straightforward, computationally efficient calculation of the
probability distribution and accompanying statistics of the metric at hand. Our
technique is illustrated by considering a number of uncertainty-quantification
problems and comparing the corresponding results with exhaustive simulations.
