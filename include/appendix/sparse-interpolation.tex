Let $\f: [0, 1] \to \real$ be a function in $\continuous([0, 1])$, the space of
continuous functions defined on the unit interval $[0, 1]$. The function is
approximated by virtue of the following interpolation formula:
\begin{equation} \elab{interpolant-tensor-1d}
  \f \approx \interpolant{1}{i}(\f)
  = \sum_{j \in \tensorindex{1}{i}} \f(\x_{ij}) e_{ij}.
\end{equation}
In the notation $\interpolant{1}{i}$, the superscript 1 indicates the
dimensionality, and the subscript $i \in \natural$ signifies the level of the
interpolant. Further,
\begin{align*}
  & \X_i = \{ \x_{ij}: j \in \tensorindex{1}{i} \} \subset [0, 1] \text{ and} \\
  & \E_i = \{ e_{ij}: j \in \tensorindex{1}{i} \} \subset \continuous([0, 1])
\end{align*}
are collocation nodes and basis functions, respectively, and
\[
  \tensorindex{1}{i} = \{ j - 1 \}_{j = 1}^{n_i} \subset \natural
\]
is a set indexing the first two where $n_i$ denotes its cardinality. In this
context, the subscript $j \in \tensorindex{1}{i}$ is referred to as the order of
a node or function.

Let us now turn to the multidimensional case. Let $\f: [0, 1]^n \to \real$ be a
function in $\continuous([0, 1]^n)$, the space of continuous functions defined
on the unit hypercube $[0, 1]^n$. The function is approximated as
\[
  \f \approx \interpolant{n}{\vi}(\f)
  = \sum_{\vj \in \tensorindex{n}{\vi}} \f(\vx_{\vi \vj}) e_{\vi \vj}
\]
where $\vi = (i_k) \in \natural^n$ and $\vj = (j_k) \in \natural^n$ are
(multi-)indices specifying levels and orders, respectively. In the case,
\begin{align}
  & \X_{\vi} = \left\{ \vx_{\vi \vj}: \vj \in \tensorindex{n}{\vi} \right\} \subset [0, 1]^n \text{ and} \elab{interpolant-tensor-grid} \\
  & \E_{\vi} = \left\{ e_{\vi \vj}: \vj \in \tensorindex{n}{\vi} \right\} \subset \continuous([0, 1]^n) \elab{interpolant-tensor-basis}
\end{align}
are the collocation nodes and basis functions, respectively, and
$\tensorindex{n}{\vi} \subset \natural^n$ is an index set whose cardinality
depends on both $n$ and \vi and is denoted by $n_{\vi}$.

Similar to \sref{sparse-integration}, the construction of $\interpolant{n}{\vi}$
can be based on the full tensor product of $n$ one-dimensional interpolants as
follows:
\begin{equation} \elab{interpolant-tensor}
  \interpolant{n}{\vi} = \bigotimes_{k = 1}^n \interpolant{1}{i_k},
\end{equation}
in which case $\tensorindex{n}{\vi}$ is the same as the one in
\eref{quadrature-tensor-index}, and
\[
  e_{\vi \vj}(\vx) = \prod_{k = 1}^n e_{i_k j_k}(x_k)
\]
for any $\vx \in [0, 1]^n$. The main observation here is similar to the one made
in \sref{sparse-integration}: the number of collocation nodes grows
exponentially as the number of dimensions increases. Nevertheless,
\eref{interpolant-tensor} serves well as a building block for a more efficient
algorithm, which we discuss next.

This algorithm is the Smolyak algorithm introduced in \sref{sparse-integration}.
In this context, the algorithm combines a number of small tensor-product
interpolants in such a way that the resulting interpolant preserves the
approximating power of the full tensor product only for a certain polynomial
subspace, which allows it to drastically reduce the number of collocation nodes.

Let us present the algorithm in its incremental form. Define
\begin{align}
  & \interpolant{1}{-1} = 0, \nonumber \\
  & \Delta\interpolant{1}{i} = \interpolant{1}{i} - \interpolant{1}{i - 1}, \text{ and} \elab{interpolant-tensor-delta-1d} \\
  & \Delta\interpolant{n}{\vi} = \bigotimes_{k = 1}^n \Delta\interpolant{n}{i_k}. \nonumber
\end{align}
Then the Smolyak algorithm of level \ls is
\begin{equation} \elab{interpolant-sparse-incremental}
  \interpolant{n}{\ls}
  = \sum_{\vi \in \sparseindex{n}{\ls}} \Delta\interpolant{n}{\vi}
  = \interpolant{n}{\ls - 1} + \sum_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\interpolant{n}{\vi}
\end{equation}
where
\begin{align}
  & \sparseindex{n}{\ls} = \{ \vi: \norm[1]{\vi} \leq \ls \} \text{ and} \nonumber \\
  & \Delta\sparseindex{n}{\ls} = \{ \vi: \norm[1]{\vi} = \ls \}. \elab{interpolant-sparse-index-delta}
\end{align}
It can be seen in \eref{interpolant-sparse-incremental} that a Smolyak
interpolant can be efficiently refined: the work done in order to attain one
level can be entirely recycled in order to go to the next one. Regarding the
collocation nodes, define
\begin{align*}
  & \X^1_{-1} = \emptyset, \\
  & \Delta\X^1_i = \X^1_i \setminus \X^1_{i - 1}, \text{ and } \\
  & \Delta\X^n_{\vi} = \Delta\X^1_{i_1} \times \cdots \times \Delta\X^1_{i_n}.
\end{align*}
Then the collocation nodes of a Smolyak interpolant of level \ls are
\begin{equation} \elab{interpolant-sparse-grid}
  \X^n_{\ls}
  = \bigcup_{\vi \in \sparseindex{n}{\ls}} \Delta\X^n_{\vi}
  = \X^n_{\ls - 1} \cup \bigcup_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\X^n_{\vi}.
\end{equation}
The above formula also shows how the collocation nodes of one level are related
to the collocation nodes of the previous level. The sparsity and incremental
refinement of the Smolyak approach are remarkable properties \emph{per se};
however, they can be taken even further as we discuss next.

It can be seen in \eref{interpolant-sparse-grid} that it is beneficial to the
refinement to have that $\X^1_{i - 1}$ is entirely included in $\X^1_i$ since,
in that case, the cardinality of
\[
  \X^n_{\ls} \setminus \X^n_{\ls - 1} = \bigcup_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\X^n_{\vi}
\]
derived from \eref{interpolant-sparse-grid} decreases. In words, the values of
\f obtained on lower levels can be reused in order to attain higher levels if
the grid grows without discarding its previous structure. With this aspect in
mind, the rule used for generating successive sets of points $\{ \X^1_i \}_i$
should be chosen to be nested, that is, in such a way that $\X^1_i$ contains all
the nodes in $\X^1_{i - 1}$.

The final step is to rewrite \eref{interpolant-sparse-incremental} in its
hierarchical form. To this end, we require interpolants of higher levels to
represent exactly interpolants of lower levels. In one dimension, it means that
\begin{equation} \elab{interpolant-tensor-exactness}
  \interpolant{1}{i - 1}(\f) = \interpolant{1}{i}(\interpolant{1}{i - 1}(\f)).
\end{equation}
The above condition can be satisfied by an appropriate choice of collocation
nodes and basis functions. Assuming that it holds and using
\eref{interpolant-tensor-1d} and \eref{interpolant-tensor-delta-1d},
\[
  \Delta\interpolant{1}{i}(\f) = \sum_{j \in \Delta\tensorindex{1}{i}} \left(\f(\x_{ij}) - \interpolant{1}{i - 1}(\f)(\x_{ij})\right) e_{ij}
\]
where
\[
  \Delta\tensorindex{1}{i} = \{ j: j \in \tensorindex{1}{i}, \x_{ij} \in \Delta\X^1_i \}.
\]
The above summation is over $\Delta\X^1_i$ due to the fact that the difference
in the parentheses is zero when $\x_{ij} \in \X^1_{i - 1}$ since $\X^1_{i - 1}
\subset \X^1_i$. In multiple dimensions,
\begin{equation} \elab{interpolant-tensor-delta}
  \Delta\interpolant{n}{\vi}(\f) = \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \left(\f(\vx_{\vi \vj}) - \interpolant{n}{\norm[1]{\vi} - 1}(\f)(\vx_{\vi \vj})\right) e_{\vi \vj}
\end{equation}
where
\begin{equation} \elab{interpolant-tensor-index-delta}
  \Delta\tensorindex{n}{\vi} = \{ \vj: \vj \in \tensorindex{n}{\vi}, \vx_{\vi \vj} \in \Delta\X^n_{\vi} \}.
\end{equation}
The delta
\begin{equation} \elab{interpolant-sparse-surplus}
  \Delta\f(\vx_{\vi \vj}) = \f(\vx_{\vi \vj}) - \interpolant{n}{\norm[1]{\vi} - 1}(\f)(\vx_{\vi \vj})
\end{equation}
is called a hierarchical surplus. When increasing the interpolation level, this
surplus is the difference between the actual value of \f at a new node and the
approximation of this value computed by the interpolant constructed so far.

The final formula for (nonadaptive) hierarchical interpolation is obtained by
substituting \eref{interpolant-tensor-delta} into
\eref{interpolant-sparse-incremental}. The result is
\begin{align*}
  \interpolant{n}{\ls}(\f)
  &= \sum_{\vi \in \sparseindex{n}{\ls}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) e_{\vi \vj} \\
  &= \interpolant{n}{\ls - 1}(\f) + \sum_{\vi \in \Delta\sparseindex{n}{\ls}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) e_{\vi \vj}
\end{align*}
where $\Delta\f(\vx_{\vi \vj})$ is computed according to
\eref{interpolant-sparse-surplus}.
