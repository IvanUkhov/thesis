Due to the inherent complexity, uncertainty-quantification problems are
typically viewed as approximation problems: one usually constructs a
computationally efficient surrogate for the system under consideration and then
studies this light representation instead of the original system. One way to
construct such a surrogate is \acf{PC} \cite{xiu2010}. The technique decomposes
stochastic quantities into infinite series of mutually orthogonal polynomials of
random variables. Such series are especially attractive from the post-processing
perspective as they are nothing more than the familiar polynomials; therefore,
\ac{PC} expansions are easy to interpret and easy to evaluate.

Let $\L{2}(\Omega, \mathcal{F}, \probability)$ be as in
\eref{square-integrable-space} and $G \subset \L{2}(\Omega, \mathcal{F},
\probability)$ be the Gaussian Hilbert space \cite{janson1997} spanned by $n$
mutually independent standard Gaussian random variables, which we denote by
$\v{x}: \Omega \to \real^n$. Since the variables are independent and standard,
they form an orthonormal basis in $G$, and the dimension of $G$ is $n$. The
variables induce a probability measure on $\real^n$, and the corresponding
distribution function $F$ is standard Gaussian given by
\[
  \d F(\v{y}) = (2 \pi)^{-\frac{n}{2}} \exp\left(-\frac{\norm[2]{\v{y}}^2}{2}\right) \d \v{y}.
\]
Then the inner product in the vector space of functions over $G$ is defined as
\begin{equation} \elab{chaos-inner-product}
  \innerproduct{g}{h} = \int_{\real^n} g(\v{y}) h(\v{y}) \d F(\v{y})
\end{equation}
for any $g$ and $h$ in this space, and the norm is defined as
\[
  \norm[2]{g} = \sqrt{\innerproduct{g}{g}}.
\]

Let $\Psi_\lc(G)$ be the space of $n$-variate polynomials over $G$ such that the
total order of each polynomial is at most \lc. The space $\Psi_\lc(G)$ can be
constructed as a span of $n$-variate Hermite polynomials \cite{eldred2008,
maitre2010}
\[
  \Psi_\lc(G) = \hull{\left\{ \psi_{\multiindex}(\v{y}): \multiindex \in \multiindices{\lc}, \v{y} \in G \right\}}
\]
where $\multiindex = (i_k) \in \natural^n$ is a multi-index,
\begin{align}
  & \multiindices{\lc} = \left\{ \multiindex: \norm[1]{\multiindex} \leq \lc \right\}, \text{ and} \elab{chaos-multi-index-isotropic} \\
  & \psi_{\multiindex}(\v{y}) = \prod_{k = 1}^n \psi_{i_k}(y_k). \nonumber
\end{align}
In the above formulae, $\psi_{i_k}: \real \to \real$ is a one-dimensional
Hermite polynomial of order $i_k$, which is assumed to be normalized, and
$\norm[1]{\cdot}$ stands for the Manhattan norm. Furthermore, the multi-index
set $\multiindices{\lc}$ in \eref{chaos-multi-index-isotropic} is called a
total-order multi-index set since it constrains polynomials by their total order
\cite{eldred2008, beck2011}, and its cardinality can be calculated as follows:
\begin{equation} \elab{chaos-multi-index-isotropic-length}
  \nc = \cardinality{\multiindices{\lc}} = {\lc + n \choose n} = \frac{(\lc + \nz)!}{\lc! \, \nz!}.
\end{equation}
It is important to note that the basis polynomials $\{ \psi_{\multiindex} \}$
are orthonormal with respect to $F$, which means that
\begin{equation} \elab{chaos-orthogonality}
  \innerproduct{\psi_{\multiindex}}{\psi_{\multiindex[j]}} = \delta_{\multiindex \multiindex[j]}
\end{equation}
for any two multi-indices $\multiindex = (i_k) \in \natural^n$ and
$\multiindex[j] = (j_k) \in \natural^n$ where
\[
  \delta_{\multiindex \multiindex[j]} = \prod_{k = 1}^n \delta_{i_k j_k},
\]
and $\delta_{i_k j_k}$ is the Kronecker delta. In particular, the polynomials
are also centered with respect to $F$, and, therefore, the inner product in
\eref{chaos-inner-product} applied to two polynomials yields their correlation,
which is zero due to the orthogonality. In addition, the inner product applied
to a polynomial with itself yields the variance of that polynomial, which is
unity due to the normality.

Define $H_0 = \Psi_0(G)$ (the space of constants) and, for $i \geq 1$,
\[
  H_i = \Psi_i(G) \, \cap \, \Psi_{i - 1}(G)^\perp.
\]
The vector spaces $\{ H_i \}_{i = 0}^\infty$ are mutually orthogonal, closed
subspaces of $\L{2}(\Omega, \mathcal{F}, \probability)$. Since our scope of
interest is restricted to functions of $\v{x}$, $\mathcal{F}$ is assumed to be
generated by $\v{x}$. Then, by the Cameron--Martin theorem,
\[
  \L{2}(\Omega, \mathcal{F}, \probability) = \bigoplus_{i = 0}^\infty H_i,
\]
which is known as the Wiener chaos decomposition.

The Wiener chaos decomposition implies that any $\g \in \L{2}(\Omega,
\mathcal{F}, \probability)$ admits an expansion with respect to the polynomial
basis, which is denoted by
\[
  \g = \sum_{\multiindex \in \natural^n} \hat{g}_{\multiindex} \psi_{\multiindex}
\]
where the equality should be understood in mean square. The coefficients $\{
\hat{g}_{\multiindex} \}$ of this spectral decomposition are found by
multiplying both side of the equation by $\psi_{\multiindex}$ and making use of
\eref{chaos-orthogonality}, which results in
\begin{equation} \elab{chaos-projection}
  \hat{g}_{\multiindex} = \innerproduct{\g}{\psi_{\multiindex}}
\end{equation}
This operation is referred to as a spectral projection.

In practice, the infinite expansion is truncated, which we denote as
\begin{equation} \elab{chaos-expansion}
  \g \approx \chaos{n}{\lc}{\g} = \sum_{\multiindex \in \multiindices{\lc}} \hat{g}_{\multiindex} \psi_{\multiindex}.
\end{equation}
In the notation $\chaos{n}{\lc}$, the superscript $n$ indicates the dimension of
the vector space, and the subscript \lc indicates the level of the truncated
expansion. This truncated expansion converges in mean square to \g as $\lc \to
\infty$, that is,
\[
  \g = \lim_{\lc \to \infty} \chaos{n}{\lc}{\g}.
\]

The Wiener chaos decomposition is often referred to as the classical \ac{PC}
decomposition, and it can be generalized to other types of probability
distributions than Gaussian. Many popular distributions directly correspond to
certain families of orthogonal polynomials, which can be found in the Askey
scheme of orthogonal polynomials. A distribution that does not have such a
correspondence can be transformed into one of those that do using such
techniques as the one shown in \sref{probability-transformation}. Another
solutions is to construct a custom polynomial basis using the Gram--Schmidt
process. Lastly, let us note that the machinery of \ac{PC} expansions is
applicable to discrete distributions as well. The interested reader is referred
to \cite{xiu2010} for further discussions.
