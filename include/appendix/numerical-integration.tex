In numerical integration, the integral of a function $\f: \real \to \real$
\[
  I = \int_\real \f(\x) \d \x
\]
is approximated as
\[
  I \approx \quadrature{1}{i}{\f}
  = \sum_{j \in \tensorindex{1}{i}} \f(\x_{ij}) w_{ij},
\]
which is a summation of the function's values computed at prescribed points and
multiplied by prescribed weights. This type of paring of a set of points and a
set of weights is called a quadrature. In the notation $\quadrature{1}{i}$, the
superscript~1 indicates that it is a one-dimensional quadrature, and the
subscript $i \in \natural$ indicates the level of the quadrature. In the above
formula,
\begin{align*}
  & \X^1_i = \set{\x_{ij}}{j \in \tensorindex{1}{i}} \subset \real \text{ and} \\
  & \W^1_i = \set{w_{ij}}{j \in \tensorindex{1}{i}} \subset \real
\end{align*}
are the points and weights of the quadrature, respectively, and
$\tensorindex{1}{i} \subset \natural$ is an index set. The cardinality of
$\tensorindex{1}{i}$ depends on $i$ and is denoted by $n_i =
\cardinality{\tensorindex{1}{i}}$.

The level~$i$ is the index of the quadrature in the corresponding family of
quadratures with increasing precision. \emph{Precision} refers to the maximum
order of polynomials that the quadrature integrates exactly \cite{heiss2008}. To
give an example, consider Gaussian quadratures, which is a broad class of
quadratures incorporating many families. The precision of a Gaussian quadrature
with $n_i$ points is $2 n_i - 1$ \cite{heiss2008}. In other words, a Gaussian
quadrature with $n_i$ points is exact for polynomials of order up to $2 n_i -
1$. It is a remarkable property of Gaussian quadratures, which makes them
especially efficient and hence popular.

Different families of quadratures can have different relations between $n_i$ and
$i$. Even in the scope of the same family, the integration grid can be made to
grow differently with respect to $i$ in order to attain certain properties, such
as being nested. For our purposes, it is sufficient to mention one type of
growth: so-called slow linear growth. In this case, $n_i = i + 1$. Assuming slow
linear growth, the previous paragraph can be extended by stating that a Gaussian
quadrature with $n_i$ points is exact for polynomials of order up to $2 i + 1$.

In multiple dimensions, the integral of a function $\f: \real^n \to \real$
\[
  I = \int_{\real^n} \f(\vx) \d \vx
\]
is approximated as
\[
  I \approx \quadrature{n}{\vi}{\f}
  = \sum_{\vj \in \tensorindex{n}{\vi}} \f(\vx_{\vi \vj}) w_{\vi \vj}
\]
where $\vi = (i_k) \in \natural^n$ and $\vj = (j_k) \in \natural^n$. In the
above formula,
\begin{align*}
  & \X^n_{\vi} = \set{\vx_{\vi \vj}}{\vj \in \tensorindex{n}{\vi}} \subset \real^n \text{ and} \\
  & \W^n_{\vi} = \set{w_{\vi \vj}}{\vj \in \tensorindex{n}{\vi}} \subset \real
\end{align*}
and the points and weights, respectively, and $\tensorindex{n}{\vi} \subset
\natural^n$ is an index set. The cardinality of $\tensorindex{n}{\vi}$
dependents on both $n$ and \vi and is denoted by $n_{\vi} =
\cardinality{\tensorindex{n}{\vi}}$.

The foundation of an $n$-dimensional quadrature $\quadrature{n}{\vi}$ is a set
of one-dimensional counterparts of various levels. The most straightforward
construction of such a quadrature is the full tensor product
\begin{equation} \elab{quadrature-tensor}
  \quadrature{n}{\vi} = \bigotimes_{k = 1}^n \quadrature{1}{i_k},
\end{equation}
in which case
\begin{align}
  & \vx_{\vi \vj} = (\x_{i_k j_k})_{k = 1}^n, \nonumber \\
  & w_{\vi \vj} = \prod_{k = 1}^n w_{i_k j_k}, \nonumber \\
  & \tensorindex{n}{\vi}
  = \tensorindex{1}{i_1} \times \cdots \times \tensorindex{1}{i_n}, \text{ and} \elab{quadrature-tensor-index} \\
  & n_{\vi}
  = \cardinality{\tensorindex{n}{\vi}}
  = \prod_{k = 1}^n \cardinality{\tensorindex{1}{i_k}}
  = \prod_{k = 1}^n n_{i_k}. \nonumber
\end{align}
It can be seen that, in this case, the growth of the number of points with
respect to the number of dimensions is exponential. In low dimensions, the
growth is manageable; however, in high dimensions, the situation changes
dramatically, as the number of points produced by this approach can easily
explode.

To give an example \cite{heiss2008}, suppose that each one-dimensional
quadrature has only four points ($n_i = 4$). Then, in 10 stochastic dimensions
($n = 10$), the number of multivariate points becomes 1,048,576 ($n_{\vi} =
n_i^n = 4^{10}$), which is far beyond being affordable. Moreover, it can be
shown that most of the points obtained by the full tensor product do not
contribute to the asymptotic accuracy and, therefore, are a waste of computation
time. In particular, if the integrand under consideration is a polynomial whose
total order is constrained according to a certain strategy, the full tensor
product cannot take this information into account. Consequently, a different
construction technique should be utilized in the case of high-dimensional
integration problems.

An alternative construction is the Smolyak algorithm \cite{smolyak1963}; see
also \cite{klimke2006, eldred2008, heiss2008, maitre2010}. The algorithm is a
central technique in the field of not only integration but also interpolation;
the latter is elaborated on in \xref{hierarchical-interpolation}. In the context
of integration, the algorithm combines a range of one-dimensional quadratures in
such a way that the resulting grid is tailored to be exact only for a certain
polynomial subspace. Such grids are called sparse grids, and they allow for a
significant reduction of the number of points and thus the subsequent work. For
instance, in the example given earlier, the number of points would be only 1581,
which is a drastic reduction in computation time.

To begin with, define
\begin{align*}
  & \quadrature{1}{-1} = 0, \\
  & \Delta\quadrature{1}{i} = \quadrature{1}{i} - \quadrature{1}{i - 1}, \text{ and} \\
  & \Delta\quadrature{n}{\vi} = \bigotimes_{k = 1}^n \Delta\quadrature{1}{i_k}.
\end{align*}
Then Smolyak's formula of level \lq is as follows:
\begin{equation} \elab{quadrature-sparse}
  \quadrature{n}{\lq} = \bigoplus_{\vi \in \sparseindex{n}{\lq}} \Delta\quadrature{n}{\vi}.
\end{equation}
In the original (isotropic) formulation of the Smolyak algorithm,
\begin{equation} \elab{index-total-order-isotropic}
  \sparseindex{n}{\lq} = \set{\vi}{\vi \in \natural^n, \norm[1]{\vi} \leq \lq}
\end{equation}
where $\norm[1]{\cdot}$ stands for the Manhattan norm. The index set
$\sparseindex{n}{\lq}$ is called the total-order index set \cite{eldred2008,
beck2011}, and its cardinality can be calculated as follows:
\begin{equation} \elab{index-total-order-isotropic-length}
  \cardinality{\sparseindex{n}{\lq}} = {n + \lq \choose n} = \frac{(n + \lq)!}{n! \, \lq!}.
\end{equation}
Note that \eref{quadrature-sparse} reduces to \eref{quadrature-tensor} if we let
\[
  \sparseindex{n}{\lq} = \set{\vi}{\vi \in \natural^n, \norm[\infty]{\vi} \leq \lq}.
\]

Using \eref{quadrature-sparse}, the integral in question is approximated as
\[
  I \approx \quadrature{n}{\lq}{\f}
  = \sum_{j \in \tensorindex{n}{\lq}} \f(\vx_j) w_j.
\]
Note that, for convenience, the points and weights of $\quadrature{n}{\lq}$ are
indexed using a single one-dimensional set denoted by $\tensorindex{n}{\lq}
\subset \natural$. Note also that, even though the level \lq is not indicated in
the notation of points and weights, it should be understood that points and
weights are generally different for different levels.

Lastly, consider the following more general integral:
\[
  I = \int_{\real^n} \f(\vx) \d F(\vx).
\]
In this case, \f is integrated with respect to a measure $F: \real^n \to \real$
\cite{durrett2010} that does not necessarily correspond to the usual Lebesgue
measure, which is used in the earlier examples. Since integrating with respect
to a certain measure is a very frequent operation, there are families of
quadratures that are designed to automatically take this aspect into account in
the most common scenarios. For instance, the Gauss--Hermite family is suitable
for integrating with respect to the standard Gaussian measure, which can be seen
in \eref{gaussian-measure}.

It is also worth emphasizing that quadratures are generally precomputed and
tabulated, since they do not depend on the function being integrated.
