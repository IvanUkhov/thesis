Let $(\Omega, \F, \probability)$ be a (complete) probability space where
$\Omega$ is a set of outcomes, $\F \subseteq 2^\Omega$ is a $\sigma$-algebra on
$\Omega$, and $\probability: \F \to [0, 1]$ is a probability measure
\cite{durrett2010}. The $\sigma$-algebra represents a set of events, and the
probability measure assigns probabilities to these events. A real-valued random
variable defined on $(\Omega, \F, \probability)$ is an \F-measurable function
$\x: \Omega \to \real$. A random variable \x is uniquely characterized by its
distribution function $F$, which is also known as the \ac{CDF}, defined by
\begin{equation*}
  F(y) = \probability{\x \leq y} = \probability{\set{\omega}{\omega \in \Omega, \x(\omega) \leq y}}.
\end{equation*}
The expectation of \x is given by
\[
  \expectation{\x} = \int_\real y \, \d F(y) = \int_\Omega \x(\omega) \d \probability{\omega}.
\]
If $\expectation{\x} = 0$, \x is called centered. The variance of \x is given
by
\begin{equation} \elab{variance}
  \variance{\x}
  = \expectation{(\x - \expectation{\x})^2}
  = \expectation{\x^2} - (\expectation{\x})^2.
\end{equation}
If $\variance{\x} = 1$, \x is called normalized. If \x is both centered and
normalized (that is, $\expectation{\x} = 0$ and $\variance{\x} = 1$), \x is
called standardized.

The above quantities are well defined only when the corresponding integrals are
finite. An example of a vector space of random variables defined on $(\Omega,
\F, \probability)$ whose expectations and variances are finite is
\begin{equation} \elab{square-integrable-space}
  \L{2}{\Omega, \F, \probability} = \set{\x}{\int_\Omega \absolute{\x(\omega)}^2 \d \probability{\omega} < \infty},
\end{equation}
the Hilbert space of square-integrable random variables \cite{janson1997}. The
inner product in this vector space is defined by
\[
  \innerproduct{\x_1}{\x_2} = \expectation{(\x_1 \x_2)},
\]
and the norm is defined by
\[
  \norm[2]{\x} = \sqrt{\innerproduct{\x}{\x}}
\]
for any \x, $\x_1$, and $\x_2$ in $\L{2}{\Omega, \F, \probability}$.

If the distribution function $F$ of a random variable \x is continuous, \x is
said to be a continuous random variable. If, moreover, $F$ is absolutely
continuous, \x is said to have a density function $f$, which is also known as
the \ac{PDF}. In such a case,
\[
  F(y) = \int_{-\infty}^y f(z) \d z.
\]
Other types of random variables, such as discrete random variables, are of
little relevance to this thesis; therefore, they are not covered here.

Suppose now that there is a set of $n$ random variables $\set{\x_i}_{i = 1}^n$.
For convenience, the variables are arranged in a vector $\vx = (\x_i)_{i = 1}^n:
\Omega \to \real^n$, which is called a random vector and which can also be
viewed as a single random variable taking values in $\real^n$. The variables
obey a common distribution known as the joint distribution, and the distribution
of any subset of the variables is called a marginal distribution. The individual
variables are referred to as mutually independent if their joint distribution
function $F$ factorizes as follows:
\[
  F(\vx) = \prod_{i = 1}^n F_i(\x_i)
\]
where $F_i$ is the marginal distribution of $\x_i$ for $i = \range{1}{n}$.

Analogously to the one-dimensional case, \vx has an expectation and a variance
(if the integrals are finite) as well as a \ac{PDF} (if the \ac{CDF} is
absolutely continuous). In addition, the covariance of $\x_i$ and $\x_j$ is
defined as follows:
\[
  \covariance{\x_i}{\x_j} = \expectation{((\x_i - \expectation{\x_i})(\x_j - \expectation{\x_j}))}.
\]
If $\covariance{\x_i}{\x_j} = 0$, $\x_i$ and $\x_j$ are referred to as
uncorrelated. Furthermore, the covariance matrix of \vx is given by
\begin{equation} \elab{covariance-matrix}
  \covariance{\vx} = \left(\covariance{\x_i}{\x_j}\right)_{i = 1, j = 1}^{n, n},
\end{equation}
which is a positive semidefinite matrix by definition. Lastly, a special case of
the covariance matrix is the correlation matrix
\begin{equation} \elab{correlation-matrix}
  \correlation{\vx} = \diagonal{\covariance{\vx}}^{-\frac{1}{2}} \covariance{\vx} \diagonal{\covariance{\vx}}^{-\frac{1}{2}}
\end{equation}
where $\diagonal{\cdot}$ extracts the diagonal elements of its argument and
yields a diagonal matrix. It can be shown that the correlation matrix is the
covariance matrix of a normalized version of \vx, that is, of
\[
  \left(\frac{\x_i}{\sqrt{\variance{\x_i}}}\right)_{i = 1}^n.
\]

The joint distribution of $\vx: \Omega \to \real^n$ can be unambiguously
specified by a set of $n$ one-dimensional marginal distributions and an
$n$-dimensional copula \cite{nelsen2006}. The copula is a uniform distribution
function on $[0, 1]^n$ that captures the dependencies between the $n$ individual
variables contained in $\vx = (\x_i)$.
