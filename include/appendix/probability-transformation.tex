There are three probability transformations that are utilized in the thesis.

The first transformation $\transform_1$ is the usual integral transformation.
The technique allows one to transition from a random variable $\x_1$ with a
distribution function $F_1$ to a random variable $\x_2$ with a distribution
function $F_2$ as
\[
  \x_2 = \transform_1(\x_1) = (F_2^{-1} \circ F_1)(\x_1) = F_2^{-1}(F_1(\x_1)
\]
where $F_2^{-1}$ is the inverse of $F_2$, and the equality should be understood
in distribution. In order for $F_2^{-1}$ to exist, $F_2$ is assumed to be a
strictly increasing function. By applying $F_1$ to $\x_1$, we obtain a random
variable with a uniform distribution on $[0, 1]$, and, by applying $F_2^{-1}$ to
such a uniform random variable, we obtain a random variable distributed
according to the target $F_2$.

The second transformation $\transform_2$ is based on the eigendecomposition
given in \eref{eigendecomposition} applied to \eref{correlation-matrix}. The
technique is called the discrete \ac{KL} decomposition \cite{ghanem1991,
xiu2010}, and it is also known as principal component analysis
\cite{hastie2013}. It transforms potentially correlated random variables into
linearly uncorrelated random variables. Conversely, it allows one to transition
from a random vector $\vx_1$ with $n_1$ linearly uncorrelated components to an
$n_1$-dimensional random vector $\vx_2$ with a prescribed correlation structure.
Specifically, the relation between the two vectors is as follows:
\begin{equation} \elab{karhunen-loeve}
  \vx_2 = \transform_2(\vx_1) = \m{U} \m{\Lambda}^\frac{1}{2} \vx_1
\end{equation}
where the eigenvalues contained in $\m{\Lambda}$ correspond to the variances
induced to the uncorrelated variables when the transition is made. When $\vx_2$
obeys a multivariate Gaussian distribution, the individual variables in $\vx_1$
follow the standard Gaussian distribution and are mutually independent, and vice
versa.

In addition, the eigendecomposition in \eref{eigendecomposition} provides a
means of model order reduction. The intuition is that, since the variables in
$\vx_2$ are correlated, $\vx_2$ can be recovered sufficiently well from a small
subset of the variables in $\vx_1$. Specifically, we are to select the smallest
subset of $\vx_1$ such that its cumulative contribution to the variance of
$\vx_2$ is above a certain threshold. Formally, assuming that $\{ \lambda_i
\}_{i = 1}^{n_1}$ in $\m{\Lambda}$ are sorted in the descending order and given
a threshold $\eta \in (0, 1]$, which is the fraction of the variance to be
preserved, we are to identify the smallest $n_2 \leq n_1$ such that
\[
  \frac{\sum_{i = 1}^{n_2} \lambda_i}{\sum_{i = 1}^{n_1} \lambda_i} \geq \eta.
\]
Then, given a vector $\vx_1$ in the reduced space $\real^{n_1}$, the
corresponding vector $\vx_2$ in the original space $\real^{n_2}$ can be lossy
reconstructed as follows:
\begin{equation} \elab{model-order-reduction}
  \vx_2 \approx \transform_2(\vx_1) = \m{U} \tm{\Lambda}^\frac{1}{2} \vx_1.
\end{equation}
In this formula, $\tm{\Lambda} \in \real^{n_2 \times n_1}$ is a truncated
version of $\m{\Lambda} \in \real^{n_2 \times n_2}$ in
\eref{eigendecomposition}: the matrix contains only the first $n_1$ columns of
$\m{\Lambda}$. When $\eta$ is sufficiently large, the dropped variables are
considered insignificant, redundant.

The third transformation $\transform_3$ builds upon the previous two
transformations and allows one to tackle the following problem. Let $\vx_1:
\Omega \to \real^{n_1}$ be a random vector whose joint distribution is know, and
it is the product of the marginal distributions $\{ F_{1i} \}_{i = 1}^{n_1}$ of
the individual variables; therefore, the variables are independent. Let also
$\vx_2: \Omega \to \real^{n_2}$ be a random vector whose joint distribution is
unknown, and the only available information about $\vx_2$ is the marginal
distributions $\{ F_{2i} \}_{i = 1}^{n_2}$ and correlation matrix
$\correlation{\vx_2}$, which are not sufficient to recover the joint
distribution in general. The goal is to transform $\vx_1$ into such a random
vector that has the same marginal distributions and correlation matrix as
$\vx_2$ and, thereby, approximates $\vx_2$ as closely as possible.

Making use of the first two transformations described above, we obtain a
potential solution, which is loosely denoted as follows:
\begin{equation} \elab{probability-transformation}
  \vx_2 \approx \transform_3(\vx_1) = (\transform_1 \circ \transform_2 \circ \transform_1)(\vx_1).
\end{equation}
First, using $\transform_1$, the $n_1$ independent variables $\vx_1$ are
transformed into $n_1$ independent uniform variables and then into $n_1$
independent standard Gaussian variables; the operation should be understood
element-wise. Second, using $\transform_2$, the $n_1$ independent standard
Gaussian variables are transformed into $n_2$ dependent Gaussian variables with
a correlation matrix carefully constructed based on the knowledge about $\vx_2$.
Third, using $\transform_1$, the $n_2$ dependent Gaussian variables are
transformed into $n_2$ dependent uniform variables and then into $n_2$ dependent
variables with the marginal distributions $\{ F_{2i} \}_{i = 1}^{n_2}$ and
correlation matrix $\correlation{\vx_2}$, which approximate the $n_2$ dependent
variables $\vx_2$.

The auxiliary correlation matrix mentioned above is constructed using the Nataf
model described in \cite{liu1986}; see also \cite{li2008}. In fact, the two
outermost transformation in \eref{probability-transformation} without model
order reduction are often referred to as the Nataf transformation. The technique
operates under the assumption that the copula of $\vx_2$ is elliptical. In the
general case, it is an approximation.
