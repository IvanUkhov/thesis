Let $\f: [0, 1] \to \real$ be a function in $\continuous{[0, 1]}$, which is the
space of continuous functions defined on $[0, 1]$. The function is approximated
by virtue of the following interpolation formula:
\begin{equation} \elab{interpolant-single}
  \f \approx \interpolant{1}{i}(\f)
  = \sum_{j \in \tensorindex{1}{i}} \f(\x_{ij}) e_{ij}.
\end{equation}
In the notation $\interpolant{1}{i}$, the superscript~1 indicates the
dimensionality, and the subscript $i \in \natural$ indicates the level of the
interpolant. In the above formula,
\begin{align*}
  & \X_i = \set{\x_{ij}}{j \in \tensorindex{1}{i}} \subset [0, 1] \text{ and} \\
  & \E_i = \set{e_{ij}}{j \in \tensorindex{1}{i}} \subset \continuous{[0, 1]}
\end{align*}
are collocation nodes and basis functions, respectively, and
$\tensorindex{1}{i}$ is an index set. The cardinality of $\tensorindex{1}{i}$
depends on $i$ and is denoted by $n_i = \cardinality{\tensorindex{1}{i}}$. In
this context, the subscript $j \in \tensorindex{1}{i}$ is referred to as the
order of a node or a function.

Let us now turn to the multidimensional case. Let $\f: [0, 1]^n \to \real$ be a
function in $\continuous{[0, 1]^n}$, which is the space of continuous functions
defined on $[0, 1]^n$. The function is approximated as
\[
  \f \approx \interpolant{n}{\vi}(\f)
  = \sum_{\vj \in \tensorindex{n}{\vi}} \f(\vx_{\vi \vj}) e_{\vi \vj}
\]
where $\vi = (i_k) \in \natural^n$ and $\vj = (j_k) \in \natural^n$. In the
above formula,
\begin{align*}
  & \X_{\vi} = \set{\vx_{\vi \vj}}{\vj \in \tensorindex{n}{\vi}} \subset [0, 1]^n \text{ and} \\
  & \E_{\vi} = \set{e_{\vi \vj}}{\vj \in \tensorindex{n}{\vi}} \subset \continuous{[0, 1]^n}
\end{align*}
are the nodes and functions, respectively, and $\tensorindex{n}{\vi} \subset
\natural^n$ is an index set. The cardinality of $\tensorindex{n}{\vi}$ depends
on both $n$ and \vi and is denoted by $n_{\vi} =
\cardinality{\tensorindex{n}{\vi}}$.

Similarly to \xref{sparse-integration}, the construction of
$\interpolant{n}{\vi}$ can be based on the full tensor product of $n$
one-dimensional interpolants as follows:
\begin{equation} \elab{interpolant-tensor}
  \interpolant{n}{\vi} = \bigotimes_{k = 1}^n \interpolant{1}{i_k},
\end{equation}
in which case
\begin{align*}
  & \vx_{\vi \vj} = (\x_{i_k j_k})_{k = 1}^n, \\
  & e_{\vi \vj} = \bigotimes_{k = 1}^n e_{i_k j_k}, \text{ and}
\end{align*}
the index set $\tensorindex{n}{\vi}$ is the same as the one in
\eref{quadrature-tensor-index}. The main observation with respect to the above
construction is similar to the one made in \xref{sparse-integration}: the number
of collocation nodes grows exponentially as the number of dimensions increases.
Nevertheless, \eref{interpolant-tensor} serves well as a building block for a
more efficient algorithm, which we discuss next.

This algorithm is the Smolyak algorithm introduced in \xref{sparse-integration}.
In this context, the algorithm combines a number of one-dimensional interpolants
in such a way that the resulting interpolant preserves the approximating power
of the full tensor product only for a certain polynomial subspace, which allows
it to drastically reduce the number of collocation nodes.

Define
\begin{align}
  & \interpolant{1}{-1} = 0, \nonumber \\
  & \Delta\interpolant{1}{i} = \interpolant{1}{i} - \interpolant{1}{i - 1}, \text{ and} \elab{interpolant-single-delta} \\
  & \Delta\interpolant{n}{\vi} = \bigotimes_{k = 1}^n \Delta\interpolant{1}{i_k}. \nonumber
\end{align}
Then the Smolyak algorithm of level \ls is
\begin{equation} \elab{interpolant-sparse-incremental}
  \interpolant{n}{\ls}
  = \sum_{\vi \in \sparseindex{n}{\ls}} \Delta\interpolant{n}{\vi}
  = \interpolant{n}{\ls - 1} + \sum_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\interpolant{n}{\vi}
\end{equation}
where
\begin{align}
  & \sparseindex{n}{\ls} = \set{\vi}{\vi \in \natural^n, \norm[1]{\vi} \leq \ls} \text{ and} \nonumber \\
  & \Delta\sparseindex{n}{\ls} = \set{\vi}{\vi \in \natural^n, \norm[1]{\vi} = \ls}. \elab{interpolant-sparse-index-delta}
\end{align}
It can be seen in \eref{interpolant-sparse-incremental} that a Smolyak
interpolant can be efficiently refined: the work done in order to attain one
level can be entirely recycled in order to go to the next one. Regarding the
collocation nodes, define
\begin{align*}
  & \X^1_{-1} = \emptyset, \\
  & \Delta\X^1_i = \X^1_i \setminus \X^1_{i - 1}, \text{ and } \\
  & \Delta\X^n_{\vi} = \Delta\X^1_{i_1} \times \cdots \times \Delta\X^1_{i_n}.
\end{align*}
Then the collocation nodes are as follows:
\begin{equation} \elab{interpolant-sparse-grid}
  \X^n_{\ls}
  = \bigcup_{\vi \in \sparseindex{n}{\ls}} \Delta\X^n_{\vi}
  = \X^n_{\ls - 1} \cup \bigcup_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\X^n_{\vi}.
\end{equation}
The above formula also shows how the collocation nodes of one level are related
to the collocation nodes of the previous level. The sparsity and incremental
refinement of the Smolyak approach are remarkable properties \perse; however,
the approach can be taken even further, as we discuss next.

It can be seen in \eref{interpolant-sparse-grid} that it is beneficial to the
refinement to have $\X^1_{i - 1}$ be entirely included in $\X^1_i$ since, in
that case, the cardinality of
\[
  \X^n_{\ls} \setminus \X^n_{\ls - 1} = \bigcup_{\vi \in \Delta\sparseindex{n}{\ls}} \Delta\X^n_{\vi}
\]
derived from \eref{interpolant-sparse-grid} decreases. To express this idea in
words, the values of \f obtained at lower levels can be reused in order to
attain higher levels if the grid grows without discarding its previous
structure. With this in mind, the rule used for generating successive sets of
points $\set{\X^1_i}$ should be chosen to be nested, that is, in such a way that
$\X^1_i$ contains all the nodes present in $\X^1_{i - 1}$.

The last step is to rewrite \eref{interpolant-sparse-incremental} in a
hierarchical form. To this end, we require interpolants of higher levels to
exactly represent interpolants of lower levels. In one dimension, this means
that
\begin{equation} \elab{interpolant-tensor-exactness}
  \interpolant{1}{i - 1}(\f) = \interpolant{1}{i}(\interpolant{1}{i - 1}(\f)).
\end{equation}
The above condition can be satisfied by an appropriate choice of collocation
nodes and basis functions. Assuming that \eref{interpolant-tensor-exactness}
holds and using \eref{interpolant-single} and \eref{interpolant-single-delta},
\[
  \Delta\interpolant{1}{i}(\f) = \sum_{j \in \Delta\tensorindex{1}{i}} \left(\f(\x_{ij}) - \interpolant{1}{i - 1}(\f)(\x_{ij})\right) e_{ij}
\]
where
\[
  \Delta\tensorindex{1}{i} = \set{j}{j \in \tensorindex{1}{i}, \x_{ij} \in \Delta\X^1_i}.
\]
The above summation is over $\Delta\X^1_i$ due to the fact that the difference
in the parentheses is zero when $\x_{ij} \in \X^1_{i - 1}$ since $\X^1_{i - 1}
\subset \X^1_i$. In multiple dimensions,
\begin{equation} \elab{interpolant-tensor-delta}
  \Delta\interpolant{n}{\vi}(\f) = \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \left(\f(\vx_{\vi \vj}) - \interpolant{n}{\ls - 1}(\f)(\vx_{\vi \vj})\right) e_{\vi \vj}
\end{equation}
where $\ls = \norm[1]{\vi}$ and
\begin{equation} \elab{interpolant-tensor-index-delta}
  \Delta\tensorindex{n}{\vi} = \set{\vj}{\vj \in \tensorindex{n}{\vi}, \vx_{\vi \vj} \in \Delta\X^n_{\vi}}.
\end{equation}
The delta
\begin{equation} \elab{interpolant-sparse-surplus}
  \Delta\f(\vx_{\vi \vj}) = \f(\vx_{\vi \vj}) - \interpolant{n}{\ls - 1}(\f)(\vx_{\vi \vj})
\end{equation}
is referred to as a hierarchical surplus. When increasing the interpolation
level, this surplus is the difference between the actual value of the target
function at a new collocation node and the approximation of this value computed
by the hierarchical interpolant that has been constructed so far.

The final formula for (nonadaptive) hierarchical interpolation is obtained by
substituting \eref{interpolant-tensor-delta} into
\eref{interpolant-sparse-incremental}. The result is
\begin{align*}
  \f \approx \interpolant{n}{\ls}(\f)
  &= \sum_{\vi \in \sparseindex{n}{\ls}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) e_{\vi \vj} \\
  &= \interpolant{n}{\ls - 1}(\f) + \sum_{\vi \in \Delta\sparseindex{n}{\ls}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) e_{\vi \vj}
\end{align*}
where $\Delta\f(\vx_{\vi \vj})$ is computed according to
\eref{interpolant-sparse-surplus}.
