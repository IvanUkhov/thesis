In this section, we evaluate our framework on different configurations of the
illustrative application given in \sref{chaos-transient-application}. All the
experiments are conducted on a \up{GNU}/Linux machine equipped with Intel Core
i7 2.66~\up{GH}z and 8~\up{GB} of \up{RAM}. All the configuration files used in
the experiments are available online at \cite{eslab2014b}.

Let us first elaborate on the default configuration of our setup, which is to be
adjusted according to the purpose of each particular experiment in what follows.
We consider a 45-nm technological process. The effective channel length is
assumed to have a nominal value of 17.5~nm \cite{ptm} and a standard deviation
of 2.25~nm where the global and local variations are equally weighted. The
correlation matrices are computed according to \eref{bayes-correlation} where
the length-scale parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size
of the square die. In the model order reduction described in
\xref{probability-transformation}, the threshold parameter $\eta$ is set to
0.99, which preserves 99\% of the variance of the data. Dynamic power profiles
involved in the experiments are based on simulations of applications that are
randomly generated by means of \up{TGFF} \cite{dick1998}; in practice, such
profiles are typically obtained via adequate simulations of the architecture of
interest. The floorplans are constructed in such a way that the processing
elements form regular grids. The time step \dt of power and temperature traces
is set to 1~ms, which is also the time step of the recurrence in
\eref{chaos-recurrence}.

The construction of \ac{PC} expansions and integration grids follows the
exposition given in \sref{chaos-construction}. The tuning parameter $\gamma$ in
\eref{chaos-anisotropic-weight} is set to zero, which turns the anisotropic
index set in \eref{index-total-order-anisotropic} into the isotropic one in
\eref{index-total-order-isotropic}. Anisotropy is to be exploited in a later
section.

In what follows, we focus on the assessment of temperature profiles. Note,
however, that the results for temperature allow one to draw reasonable
conclusions about our performance with respect to power since power is an
intermediate step toward temperature, and any accuracy problems with respect to
power are expected to propagate to temperature. Also, since the
temperature-driven studies \cite{huang2009a, juan2011, juan2012, lee2013}
discussed in \sref{chaos-prior} work under the static steady-state
assumption---the work in \cite{juan2011} is also limited to the maximum
temperature, and the one in \cite{huang2009a} does not model the
power-temperature interplay---a one-to-one comparison with our framework is not
possible.

For comparison purposes, we employ \ac{MC} sampling introduced in \sref{prior}.
The \ac{MC} approach is set up to preserve the whole variance of the problem,
which means that there is no model order reduction applied, and to solve the
system in \eref{temperature-model-original} directly using the fourth- and
fifth-order Runge--Kutta formulae combined (the Dormand--Prince method)
\cite{press2007}.

\hiddensubsection{Approximation Accuracy}

The first set of experiments is to identify the accuracy of our framework with
respect to \ac{MC} sampling. At this point, it is important to note that the
true distributions of temperature are unknown, and both the \ac{PC} and \ac{MC}
approaches introduce errors. These errors decrease as the level \lc of \ac{PC}
expansions and the number of samples \no of \ac{MC} sampling increase. Hence,
instead of postulating that the \ac{MC} technique with a certain number of
samples is the solution that we should achieve, we vary both \lc and \no and
monitor the corresponding difference between the results produced by the two
alternatives.

We also inspect the impact of the correlation patterns between the local random
variables $\{ u_{\local, i} \}_{i = 1}^\np$; recall
\sref{chaos-transient-application}. More specifically, apart from \lc and \no,
we change the balance between the two correlation functions shown in
\eref{bayes-correlation}, that is, the squared-exponential kernel $k_\SE$ and
the Ornstein--Uhlenbeck kernel $k_\OU$, which is controlled by the weight
coefficient $w \in [0, 1]$.

The \ac{PC} and \ac{MC} methods are compared by means of three error metrics.
The first two are the \acp{NRMSE} of the expectation and variance of temperature
profiles. The third metric is the mean of the \acp{NRMSE} of the empirical
\ac{PDF} of temperature constructed at each time step for each processing
element. The metrics are denoted by \error{\expectation}, \error{\variance}, and
\error{f}, respectively. The first two are straightforward to interpret, and
they are calculated using the analytical expressions in \eref{chaos-moments}.
The third one is a strong indicator of the quality of the distributions
estimated by our framework, and it is computed by sampling the constructed
\ac{PC} expansions. In contrast to the \ac{MC} approach, this sampling has a
negligible overhead as noted in \sref{chaos-processing}.

\inputtable{chaos-transient-accuracy}
The considered values of \lc, \no, and $w$ are in the sets $\{ i \}_{i = 1}^7$,
$\{ 10^i \}_{i = 2}^5$, and $\{ 0, 0.5, 1 \}$, respectively. The three variants
of $w$ correspond to the total dominance of $k_\OU$ ($w = 0$), perfect balance
between $k_\SE$ and $k_\OU$ ($w = 0.5$), and total dominance of $k_\SE$ ($w =
1$). A comparison for a quad-core architecture with a dynamic power profile of
$\ns = 10^2$ steps is given in \tref{chaos-transient-accuracy-0},
\tref{chaos-transient-accuracy-0-5}, and \tref{chaos-transient-accuracy-1},
which correspond to $w = 0$, $w = 0.5$, and $w = 1$, respectively. Each table
contains three subtables: the left one is for \error{\expectation}, the middle
one for \error{\variance}, and the right one for \error{f}, which results in
nine subtables in total.

The columns of the tables that correspond to high values of \no can be used to
assess the accuracy of the constructed \ac{PC} expansions; likewise, the rows
that correspond to high values of \lc can be used to judge about the sufficiency
of the number of \ac{MC} sampling. One can immediately note that, in all the
subtables, all the error metrics tend to decrease from the top-left corners (low
values of \lc and \no) to the bottom-right corners (high values of \lc and \no),
which suggests that the \ac{PC} and \ac{MC} methods converge. There are a few
outliers, associated with low \ac{PC} levels and the random nature of sampling;
for instance, \error{\variance} increases from 66.13 to 66.7 and \error{f} from
1.59 to 1.62 when \no increases from 10\textsuperscript{4} and
10\textsuperscript{5} in \tref{chaos-transient-accuracy-0-5}. However, the
aforementioned main trend is still clear.

For clarity of the discussions below, we focus primarily on one of the three
tables, namely, on \tref{chaos-transient-accuracy-0-5}, since the case with $w =
0.5$ turns out to be the most challenging, which we shall elaborate on shortly.
The drawn conclusions are generalized to the other two tables at the end of this
subsection.

\inputfigure{chaos-application-density}
First, we inspect the accuracy of our technique and, therefore, pay particular
attention the columns of \tref{chaos-transient-accuracy-0-5} corresponding to
high values of \no. It can be seen that the error of the expectation is small
even for $\lc = 1$. Concretely, it is bounded by 0.6\%; see \error{\expectation}
for $\lc \geq 1$ and $\no \geq 10^4$. The error of the variance starts from
66.7\% for the first-level \ac{PC} expansions and drops significantly to 5.71\%
and below for the fourth level and higher; see \error{\variance} for $\lc \geq
4$ and $\no = 10^5$. It should be noted, however, that, for a fixed $\lc \geq
4$, \error{\variance} exhibits a considerable decrease even when \no transitions
from 10\textsuperscript{4} to 10\textsuperscript{5}. The rate of this decrease
suggests that $\no = 10^4$ is not sufficient to reach the accuracy delivered by
the proposed framework, and $\no = 10^5$ might not be either. Finally, the error
of the \acp{PDF} allows us to conclude that the \ac{PDF} computed by the
third-level (and higher) \ac{PC} expansions closely follow those estimated by
the \ac{MC} technique with large numbers of samples. The observed difference in
\tref{chaos-transient-accuracy-0-5} is bounded by 1.83\%; see \error{f} for $\lc
\geq 3$ and $\no \geq 10^4$. In order to give a better intuition about the
proximity of the two methods, \fref{chaos-application-density} displays the
\acp{PDF} computed using our framework with $\lc = 4$ (the solid lines) along
with those calculated by the \ac{MC} approach with $\no = 10^4$ (the dashed
lines) for time moment 50~ms. It can be seen that the \acp{PDF} tightly match
each other. Note that this example captures one particular time moment, and such
curves are also readily available for the other steps of the considered time
span.

Second, we investigate the convergence of the \ac{MC} technique and, therefore,
watch the rows of \tref{chaos-transient-accuracy-0-5} that correspond to \ac{PC}
expansions of high levels. Similar to the previous observations, even for low
values of \no, the error of the expectation estimated by \ac{MC} sampling is
relatively small, bounded by 1.19\%; see \error{\expectation} for $\lc \geq 4$
and $\no = 10^2$. At the same time, the case with $\no = 10^2$ has high error
rates in terms of the variance and \acp{PDF}: they are above 38\% and almost
3.5\%, respectively; see \error{\variance} and \error{f} for $\lc = 7$ and $\no
= 10^2$. The results for the case with $\no = 10^3$ are reasonably more
accurate. However, this trend is compromised by the figures reported in
\tref{chaos-transient-accuracy-1}: 10\textsuperscript{3} samples leave an error
of more than 7\% for the variance; see \error{\variance} for $\lc \geq 4$ and
$\no = 10^3$.

The above conclusions with respect to the results reported in
\tref{chaos-transient-accuracy-0-5} ($w = 0.5$) are directly applicable to the
results reported in \tref{chaos-transient-accuracy-0} ($w = 0$) and
\tref{chaos-transient-accuracy-1} ($w = 1$). The only difference is that the
average error rates are lower when either of the two correlation kernels shown
in \eref{bayes-correlation} dominates. In particular, according to
\error{\variance}, the case with $w = 1$, which is reported in
\tref{chaos-transient-accuracy-1} and corresponds to $k_\SE$, stands out to be
the least error prone.

Guided by the observations in this subsection, we conclude that our framework
delivers accurate results starting from $\lc = 4$. The \ac{MC} estimates, on the
other hand, can be considered as sufficiently reliable starting from $\no =
10^4$. The last conclusion, however, is biased in favor of the \ac{MC} technique
since, as noted earlier, there is evidence that 10\textsuperscript{4} samples
might still be not enough.

\hiddensubsection{Computational Speed}

The second set of experiments is to measure the speed of the framework with
respect to \ac{MC} sampling. For clarity, we use the same level of \ac{PC}
expansions and the same number of \ac{MC} samples in each case. More precisely,
based on the conclusions from the previous subsection, $\lc = 4$ and $\no =
10^4$; the latter is also consistent with the experience from the literature
\cite{ghanta2006, bhardwaj2008, huang2009a, shen2009, lee2013} and with the
theoretical results on the accuracy of \ac{MC} sampling given in
\cite{diaz-emparanza2002}. As before, we report the results obtained for various
values of the weight coefficient $w$, which impacts the number of the
independent variables $\vz: \Omega \to \real^\nz$ preserved after the model
order reduction procedure described in \xref{probability-transformation}.

\inputtable{chaos-transient-speed}
First, we vary the number of processing elements \np, which directly affects the
dimensionality of the uncertain parameters $\vu: \Omega \to \real^\nu$; recall
\sref{chaos-transient-application}. The results, including the dimensionality
\nz of \vz, are given in \tref{chaos-transient-speed-elements} where the
considered values of \np are in $\{ 2^i \}_{i = 1}^5$ and $\ns = 10^3$. It can
be seen that the correlation patterns inherent to the fabrication process
\cite{cheng2011} open a great possibility of model order reduction: \nz is
observed to be at most 12 while the maximum number without reduction is 33 (1
global variable and 32 local ones corresponding to the case with $\np = 32$).
This reduction also depends on the floorplans, which is illustrated by the
decrease of \nz when \np increases from 16 to 32 for $w = 1$. To elaborate, one
floorplan is a four-by-four grid, a perfect square, while the other an
eight-by-four grid, a rectangle. Since both are fitted into square dies, the
former is spread across the whole die whereas the latter is concentrated along
the middle line; the rest is ascribed to the particularities of $k_\SE$. On
average, the $k_\OU$ kernel ($w = 0$) requires the fewest number of variables
while the mixture of $k_\SE$ and $k_\OU$ ($w = 0.5$) requires the most. It means
that, in the latter case, more variables should be preserved in order to retain
99\% of the variance. Hence, the case with $w = 0.5$ is the most demanding in
terms of computational complexity. The results in the previous subsection
correspond to the case with $\np = 4$; therefore, \nz is two, five, and five in
\tref{chaos-transient-accuracy-0}, \tref{chaos-transient-accuracy-0-5}, and
\tref{chaos-transient-accuracy-1}, respectively.

At this point, it is important to note the following. First, since the curse of
dimensionality constitutes arguably the major concern of the theory of \ac{PC}
expansions, the applicability of our framework primarily depends on how this
curse manifests itself in the problem at hand, that is, on the dimensionality
\nz of \vz. Second, since \vz is a result of a procedure that depends on many
factors, the relation between \vu and \vz is not straightforward, which is also
illustrated in the previous paragraph. Consequently, \nu can be misleading when
reasoning about the applicability of our technique; \nz is well suited for this
purpose.

Another observation with respect to \tref{chaos-transient-speed-elements} is the
low slope of the execution time of the \ac{MC} technique, which illustrates the
well-known fact that the workload per sample is independent of the number of
stochastic dimensions. On the other hand, the rows with $\nz \geq 10$ hint at
the curse of dimensionality, which \ac{PC} expansions suffer from. However, even
in high dimensions, the proposed framework significantly outperforms \ac{MC}
sampling. For instance, in order to analyze a dynamic power profile with
10\textsuperscript{3} steps of a platform with 32 processing elements, the
\ac{MC} approach requires more than 40 hours whereas our framework takes less
than two minutes; see the case with $w = 0.5$.

Second, we investigate the scaling properties of the framework with respect to
the duration of the analyzed time span, which is directly proportional to the
number of time steps \ns covered by power and temperature profiles. The results
for a quad-core platform are reported in \tref{chaos-transient-speed-steps}. Due
to the long execution times demonstrated by the \ac{MC} approach, its statistics
for high values of \ns are extrapolated based on a smaller number of samples,
that is, $\no < 10^4$. Similar to \tref{chaos-transient-speed-elements}, we
observe the dependency of the constructed expansions on the dimensionality \nz,
which is two for $w = 0$ and five for the other two values of $w$; see
\tref{chaos-transient-speed-elements} for $\np = 4$. It can be seen in
\tref{chaos-transient-speed-steps} that the computation times of both methods
grow linearly with respect to \ns, which is expected. However, the proposed
framework shows a vastly superior performance being up to five orders of
magnitude faster than the \ac{MC} alternative.

It is worth noting that the observed speedups are due to two major reasons.
First of all, \ac{PC} expansions are generally superior to \ac{MC} sampling when
the curse of dimensionality is suppressed \cite{eldred2008, xiu2010}, which we
accomplish by model order reduction and efficient integration schemes. The
second reason is the particular technique used in the framework for solving the
temperature model and constructing \ac{PC} expansions in a stepwise manner shown
in \eref{chaos-recurrence}.
