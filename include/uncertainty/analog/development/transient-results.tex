In this section, we evaluate our framework using different configurations of the
illustrative application given in \sref{chaos-transient-application}. All the
experiments are conducted on a \up{GNU}/Linux machine equipped with Intel Core
i7 2.66~\up{GH}z and 8~\up{GB} of \up{RAM}. All the configuration files used in
the experiments are available online at \cite{eslab2014b}.

Let us first elaborate on the default configuration of our setup, which is
subsequently adjusted according to the purpose of each particular experiment. We
consider a 45-nm technological process. The effective channel length is assumed
to have a nominal value of 17.5~nm \cite{ptm} and a standard deviation of
2.25~nm where the global and local variations are equally weighted. The
correlation matrix of the uncertain parameters is computed based on
\eref{bayes-correlation} where the length-scale parameters $\ell_\SE$ and
$\ell_\OU$ are set to half the size of the square die. In the model order
reduction described in \xref{probability-transformation}, the threshold
parameter $\eta$ is set to 0.99, which preserves 99\% of the variance of the
data. The floorplan of the platform is constructed in such a way that the
processing elements form a regular grid. The dynamic power profiles involved in
the experiments are based on simulations of applications that are randomly
generated via \up{TGFF} \cite{dick1998}. The time step \dt of power and
temperature profiles is set to 1~ms, which is also the time step of the
recurrence in \eref{chaos-recurrence}.

The construction of \ac{PC} expansions and integration grids follows the
exposition given in \sref{chaos-construction}. The tuning parameter $\gamma$ in
\eref{chaos-anisotropic-weight} is set to zero, which turns the anisotropic
index set in \eref{index-total-order-anisotropic} into the isotropic one in
\eref{index-total-order-isotropic}. Anisotropy is to be exploited in a later
section.

In what follows, we focus on the assessment of temperature profiles. Note,
however, that the results for temperature allow one to draw reasonable
conclusions about the performance of the proposed framework with respect to
power since power is an intermediate step toward temperature, and any accuracy
problems with respect to power are expected to propagate to temperature.

Additionally, it is worth noting that, since the temperature-driven studies
discussed in \sref{chaos-prior}, namely \cite{huang2009a, juan2011, juan2012,
lee2013}, work under the static steady-state assumption (the work in
\cite{juan2011} is also limited to the maximum temperature, and the one in
\cite{huang2009a} does not model the power-temperature interplay), a one-to-one
comparison with the proposed technique is not possible.

For comparison purposes, we employ \ac{MC} sampling, which is introduced in
\sref{prior}. This approach samples the quantity of interest directly, meaning
that there is no any intermediate representation involved in these calculations.
There is no model order reduction applied prior to direct sampling, which
preserves the whole variance of the problem at hand, and the system in
\eref{temperature-model-original} is solved using traditional techniques, namely
the fourth- and fifth-order Runge--Kutta formulae (the Dormand--Prince method)
\cite{press2007}.

\hiddensubsection{Approximation Accuracy}

The first set of experiments aims to investigate the accuracy of our framework
with respect to direct sampling. At this point, it is important to realize that
the true distributions of temperature are unknown, and both the \ac{PC} and
\ac{MC} approaches introduce errors. These errors decrease as the level \lc of
\ac{PC} expansions and the number of samples \no of \ac{MC} sampling increase.
Hence, instead of postulating that the \ac{MC} technique with a certain number
of samples is the solution that we should achieve, we vary both \lc and \no and
monitor the corresponding difference between the results produced by the two
alternatives.

We also inspect the impact of the correlation structure between the local random
variables $\{ \u_{\local, i} \}_{i = 1}^\np$; recall
\sref{chaos-transient-application}. More specifically, apart from \lc and \no,
we change the balance between the two correlation functions shown in
\eref{bayes-correlation}, that is, the squared-exponential kernel $k_\SE$ and
the Ornstein--Uhlenbeck kernel $k_\OU$, which is controlled by the weight
coefficient $w \in [0, 1]$.

The \ac{PC} and \ac{MC} methods are compared using three error metrics. The
first two are the \acp{NRMSE} of the expectation and variance of temperature
profiles. The third metric is the mean of the \acp{NRMSE} of the empirical
\acp{PDF} of temperature constructed for all processing elements at all time
steps. The metrics are denoted by \error{\expectation}, \error{\variance}, and
\error{f}, respectively. The first two are straightforward to interpret, and
they are calculated using the analytical expressions in \eref{chaos-moments}.
The third one is a strong indicator of the quality of the distributions
estimated by our framework, and it is computed by sampling the constructed
\ac{PC} expansions. In contrast to direct sampling, this sampling has a
negligible overhead as noted in \sref{chaos-processing}.

\inputtable{chaos-transient-accuracy}
The considered values of \lc, \no, and $w$ are the sets $\{ i \}_{i = 1}^7$, $\{
10^i \}_{i = 2}^5$, and $\{ 0, 0.5, 1 \}$, respectively. The three variants of
$w$ correspond to the total dominance of $k_\OU$ ($w = 0$), perfect balance
between $k_\SE$ and $k_\OU$ ($w = 0.5$), and total dominance of $k_\SE$ ($w =
1$). A comparison for a quad-core architecture with a dynamic power profile of
$\ns = 10^2$ steps is given in \tref{chaos-transient-accuracy-0},
\tref{chaos-transient-accuracy-0-5}, and \tref{chaos-transient-accuracy-1},
which correspond to $w = 0$, $w = 0.5$, and $w = 1$, respectively. Each table
contains three subtables: the left one is for \error{\expectation}, the middle
one for \error{\variance}, and the right one for \error{f}, which results in
nine subtables in total.

The columns of the tables that correspond to high values of \no can be used to
assess the accuracy of the constructed \ac{PC} expansions; likewise, the rows
that correspond to high values of \lc can be used to judge about the sufficiency
of the number of \ac{MC} samples. One can immediately note that, in all the
subtables, all the error metrics tend to decrease from the top-left corners (low
values of \lc and \no) to the bottom-right corners (high values of \lc and \no),
which suggests that the \ac{PC} and \ac{MC} methods converge. There are a few
outliers associated with low expansion levels and the random nature of sampling.
For instance, \error{\variance} increases from 66.13 to 66.7 and \error{f} from
1.59 to 1.62 when \no makes a transition from \power{10}{4} to \power{10}{5} in
\tref{chaos-transient-accuracy-0-5}. However, the above main trend is still
clear.

For clarity of the discussions below, we focus primarily on one of the three
tables, namely \tref{chaos-transient-accuracy-0-5}. The conclusions drawn with
respect to \tref{chaos-transient-accuracy-0-5} are to be generalized to the
other two tables at the end of this subsection.

\inputfigure{chaos-application-density}
First, we inspect the accuracy of our technique and thus pay particular
attention to the columns of \tref{chaos-transient-accuracy-0-5} corresponding to
high values of \no. It can be seen that the error of the expectation is small
even when $\lc = 1$. Concretely, it is bounded by 0.6\%; see
\error{\expectation} for $\lc \geq 1$ and $\no \geq 10^4$. The error of the
variance starts from 66.7\% for the first-level \ac{PC} expansions and drops
significantly to 5.71\% and below for the fourth level and higher; see
\error{\variance} for $\lc \geq 4$ and $\no = 10^5$. The error of the \ac{PDF}
allows us to conclude that the \acp{PDF} computed by means of \ac{PC} expansions
starting from the third level closely follow those estimated by the \ac{MC}
technique with a large number of samples. The observed difference in
\tref{chaos-transient-accuracy-0-5} is bounded by 1.83\%; see \error{f} for $\lc
\geq 3$ and $\no \geq 10^4$.

In order to give a better intuition about the proximity of the two methods,
\fref{chaos-application-density} displays the \acp{PDF} computed using our
framework with $\lc = 4$ (the solid lines) along with those calculated by the
\ac{MC} approach with $\no = 10^4$ (the dashed lines) at time 50~ms. It can be
seen that the \acp{PDF} tightly match each other. Note that this example
concerns one particular time step. Such curves are readily available for the
other steps of the analyzed time span as well.

Second, we investigate the convergence of the \ac{MC} technique and consequently
watch the rows of \tref{chaos-transient-accuracy-0-5} that correspond to \ac{PC}
expansions of high levels. Similarly to the previous observations, even for low
values of \no, the error of the expectation estimated by direct sampling is
relatively small. Specifically, this error is bounded by 1.19\%; see
\error{\expectation} for $\lc \geq 4$ and $\no = 10^2$. At the same time, the
case with $\no = 10^2$ has high error rates in terms of the variance and
\ac{PDF}: they are above 38\% and around 4\%, respectively; see
\error{\variance} and \error{f} for $\lc \geq 4$ and $\no = 10^2$. The results
in the cases with $\no \geq 10^3$ are reasonably more accurate, and the one with
$\no = 10^4$ appears to be sufficiently adequate.

The above conclusions drawn with respect to the results reported in
\tref{chaos-transient-accuracy-0-5} ($w = 0.5$) are directly applicable to those
reported in \tref{chaos-transient-accuracy-0} ($w = 0$) and
\tref{chaos-transient-accuracy-1} ($w = 1$). The only difference is that the
average error rates are lower when either of the two correlation kernels shown
in \eref{bayes-correlation} dominates. In particular, according to
\error{\variance}, the case with $w = 1$, which corresponds to $k_\SE$ and is
reported in \tref{chaos-transient-accuracy-1}, stands out to be the least
challenging.

Guided by the above observations, we conclude that our framework delivers
sufficiently accurate results starting from $\lc = 4$. The \ac{MC} estimates, on
the other hand, can be considered sufficiently reliable starting from $\no =
10^4$.

\hiddensubsection{Computational Speed}

The second set of experiments is to measure the speed of our framework with
respect to direct sampling. To this end, we keep \lc and \no fixed and equal to
4 and \power{10}{4}, respectively. The latter value is also similar to those
used in the literature \cite{ghanta2006, bhardwaj2008, huang2009a, shen2009,
xiang2010, juan2012, lee2013} and consisted with the theoretical results on the
accuracy of \ac{MC} sampling presented in \cite{diaz-emparanza2002}. Analogous
to the previous subsection, we report the results obtained for different values
of the weight coefficient $w$, which impacts the number of the independent
variables $\vz: \Omega \to \real^\nz$ preserved after applying the model order
reduction described in \xref{probability-transformation}.

\inputtable{chaos-transient-speed}
First, we vary the number of processing elements \np, which directly affects the
dimensionality of the uncertain parameters $\vu: \Omega \to \real^\nu$; recall
\sref{chaos-transient-application}. The results, including the dimensionality
\nz of \vz, are given in \tref{chaos-transient-speed-elements} where the
considered values of \np are $\{ 2^i \}_{i = 1}^5$ and $\ns = 10^3$. It can be
seen that the variation patterns inherent to the fabrication process
\cite{cheng2011} open a great possibility of model order reduction: \nz is
observed to be at most 12 while the maximum number without reduction is 33 (1
global variable and 32 local ones corresponding to the case with $\np = 32$).
This reduction also depends on the floorplan, which is illustrated by the
decrease of \nz when \np increases from 16 to 32 for $w = 1$. To elaborate, one
floorplan is a four-by-four grid, a square, while the other an eight-by-four
grid, a rectangle. Since both are fitted into square dies, the former is spread
across the whole area, and the latter is concentrated along the middle line; the
rest is due to the particularities of $k_\SE$.

On average, the $k_\OU$ kernel ($w = 0$) requires the smallest number of
variables while the mixture of the $k_\SE$ and $k_\OU$ kernels ($w = 0.5$)
requires the largest. It means that, in the latter case, more variables should
be preserved in order to retain 99\% of the variance. Consequently, the case
with $w = 0.5$ is the most demanding in terms of the computation time. Note
that, since the results in the previous subsection correspond to the case with
$\np = 4$, \nz is two, five, and five in \tref{chaos-transient-accuracy-0},
\tref{chaos-transient-accuracy-0-5}, and \tref{chaos-transient-accuracy-1},
respectively.

At this point, it is important to realize the following. First, since the curse
of dimensionality constitutes arguably the major concern of the theory of
\ac{PC} expansions, the applicability of our framework depends primarily on how
this curse manifests itself in the problem at hand, that is, on the
dimensionality \nz of \vz. Second, since \vz is a result of a procedure that
depends on many factors, the relation between \vu and \vz is not
straightforward, which is also illustrated in the previous paragraphs.
Consequently, \nu can be misleading when reasoning about the applicability of
our technique; \nz is well suited for this purpose.

Another observation with respect to \tref{chaos-transient-speed-elements} is the
low slope of the execution time of the \ac{MC} technique, which illustrates the
well-known fact that the workload per sample is independent of the number of
stochastic dimensions. On the other hand, the rows with $\nz \geq 10$ hint at
the curse of dimensionality, which \ac{PC} expansions suffer from. However, even
in high dimensions, the proposed framework significantly outperforms direct
sampling. For instance, in order to analyze a dynamic power profile with
\power{10}{3} steps of a platform with 32 processing elements, the \ac{MC}
approach requires more than 40 hours, whereas our framework takes less than two
minutes; see the case with $w = 0.5$.

Second, we investigate the scaling properties of the proposed framework with
respect to the duration of the analyzed time span, which is directly
proportional to the number of time steps \ns covered by power and temperature
profiles. The results for a quad-core platform are given in
\tref{chaos-transient-speed-steps}. Due to the long execution times demonstrated
by the \ac{MC} approach, its statistics for high values of \ns are extrapolated
based on a smaller number of samples, that is, $\no < 10^4$. Similarly to
\tref{chaos-transient-speed-elements}, we observe a certain dependency of the
constructed expansions on the dimensionality \nz, which is two for $w = 0$ and
five for $w = 0.5$ and $w = 1$; see \tref{chaos-transient-speed-elements} for
$\np = 4$. It can be seen in \tref{chaos-transient-speed-steps} that the
computation times of both techniques grow linearly with respect to \ns, which is
expected. However, the proposed framework shows a superior performance, being up
to five orders of magnitude faster than the \ac{MC} alternative.

It is worth noting that the observed speedups are due to two major reasons.
First, \ac{PC} expansions are generally superior to \ac{MC} sampling when the
curse of dimensionality is suppressed \cite{eldred2008, xiu2010}, which we
accomplish by model order reduction and efficient integration schemes. The
second reason is the particular technique used in the framework for solving the
temperature model and constructing \ac{PC} expansions in a stepwise manner shown
in \eref{chaos-recurrence}.
