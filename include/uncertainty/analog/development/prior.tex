Uncertainty-unaware techniques are inadequate, and sampling methods---including
\ac{MC} sampling---as a means of uncertainty quantification have high
computational costs, which we elaborate on in \sref{motivation} and
\sref{prior}, respectively. In order to overcome the limitations of
deterministic approaches and, at the same time, to eliminate or at least
mitigate the computational costs associated with direct sampling, a number of
stochastic techniques have been introduced, which are discussed in what follows.
We are particularly interested in power and temperature and, therefore, shape
the exposition accordingly. Since the static component of power dissipation is
influenced by process variation the most, which is due to the leakage current,
the techniques discussed below primarily focus on the variability of this
component.

A solely power-targeted but temperature-aware solution is proposed in
\cite{chandra2010} where the driving force of the analysis is \ac{MC} sampling
with partially precomputed data. A learning-based approach is presented in
\cite{juan2011} in order to estimate the maximum temperature under the static
steady-state condition; recall \sref{static-steady-state-analysis}.
Temperature-related issues originating from process variation are also
considered in \cite{juan2012} where a statistical model of the static
steady-state temperature based on the linearity of Gaussian distributions and
time-invariant systems is derived. A statistical simulator of the static
steady-state temperature is developed in \cite{huang2009a} using \ac{PC}
expansions and the \ac{KL} decomposition; see \sref{probability-transformation}
and \sref{polynomial-chaos}. A \ac{KL}-aided stochastic collocation
\cite{maitre2010, xiu2010} approach to static steady-state temperature analysis
is presented in \cite{lee2013}. In \cite{shen2009}, \ac{PC} expansions are
employed in order to estimate the static power of the entire chip. The \ac{KL}
decomposition is utilized in \cite{bhardwaj2006} for calculating the static
power. The static power is also quantified in \cite{bhardwaj2008} using the
\ac{PC} and \ac{KL} methods. The same combination of tools is employed in
\cite{vrudhula2006} and \cite{ghanta2006} in order to analyze the response of
interconnect networks and power grids, respectively, under process variation.

The last five of the aforementioned techniques, that is, \cite{bhardwaj2006,
vrudhula2006, ghanta2006, bhardwaj2008, shen2009}, perform only stochastic power
analysis and ignore the interdependence between power and temperature described
in \sref{power-model}. The others are temperature-related approaches, but none
of them attempts to tackle stochastic transient temperature analysis and to
compute the probability distribution of temperature that evolves over time.
However, such transient curves are of practical importance. First, certain
procedures cannot be undertaken without the knowledge of time-dependent
temperature variations; an example is reliability optimization based on the
thermal-cycling fatigue, which is discussed in \sref{reliability-optimization}.
Second, the static steady-state assumption considered, for instance, in
\cite{huang2009a, juan2011, juan2012, lee2013} can rarely be justified since
power profiles are not invariant in reality. In addition, one frequently
encounters the assumption that power and temperature follow \emph{a priori}
known probability distributions; Gaussian and log-normal distributions are
popular choices as in \cite{bhardwaj2006, srivastava2010, juan2012}. However,
this assumption often fails in practice---which is also noted in \cite{juan2012}
regarding the normality of the leakage current---due to \one~the nonlinear
dependence of power on process parameters and \two~the nonlinear interdependence
between power and temperature. In order to illustrate this, we simulate
10\textsuperscript{4} times the example given in \sref{chaos-motivation}
assuming the widespread Gaussian model of the effective channel length and apply
the Jarque--Bera test of normality to the collected temperature directly as well
as after processing them with the log transformation. The null hypothesis that
the data are from an unspecified Gaussian distribution is firmly rejected in
both cases at the significance level of 5\%. Therefore, the two distributions
are neither Gaussian nor log-normal, which can also be seen in
\fref{chaos-example-density} described in \sref{chaos-transient-results}.

It can be seen that the overwhelming majority of the literature related to
temperature relies on static steady-state temperature analysis---which, as noted
earlier, inadequate in practice---and the other two types of temperature
analysis, that is, transient analysis and dynamic steady-state analysis, are
deprived of attention. To the best of our knowledge, they have not been studied
yet in the literature from the stochastic perspective. However, the knowledge of
transient and dynamic steady-state variations is of practical importance.

Let us now discuss reliability analysis. Reliability analysis is probabilistic
by nature. Certain components of a reliability model, however, can be treated as
either stochastic or deterministic, depending on the phenomena that the model is
designed for. Temperature is an example of such a component: it can be
considered as deterministic if the effect of process variation on temperature is
neglected and as stochastic otherwise. The former scenario is the one that is
typically addressed in the literature related to reliability. For instance, the
reliability model proposed in \cite{xiang2010} has a treatment of process
variation; however, temperature is included in the model as a deterministic
quantity. In \cite{das2014a}, a design methodology minimizing energy consumption
and temperature-induced wear of multiprocessor systems is introduced; yet
neither energy nor temperature is aware of the uncertainty due to process
variation. A similar observation can be made with respect to the work reported
in \cite{das2014c} where a reinforcement-learning algorithm is used in order to
improve the lifetime of multiprocessor systems. An extensive survey on
reliability-aware system-level design techniques given in \cite{das2014b}
confirms the trend emphasized above: the widespread device-level models of
failure mechanisms generally ignore the impact of process variation on
temperature. However, deterministic temperature is a strong assumption to make
since it can lead to a substantial yield loss.

An example of a different kind is the work in \cite{lee2013} mentioned earlier.
It introduces a statistical simulator for reliability analysis under process
variation and does consider temperature as a stochastic parameter. However, as
discussed previously, this study is limited to static steady-state temperatures,
and the presented reliability analysis is essentially an analysis of maximum
temperatures without any relation to the typical failure mechanisms
\cite{jedec2016}.

To summarize, the prior techniques for stochastic power and temperature analysis
are restricted in use due to one or several of the following traits: based on
\ac{MC} simulations (potentially slow) \cite{chandra2010}, limited to power
analysis \cite{bhardwaj2006, ghanta2006, vrudhula2006, bhardwaj2008, shen2009,
chandra2010}, ignoring the power-temperature interplay \cite{bhardwaj2006,
ghanta2006, vrudhula2006, bhardwaj2008, huang2009a, shen2009}, limited to the
static steady-state temperature \cite{huang2009a, juan2011, juan2012, lee2013},
exclusive focus on the maximum temperature \cite{juan2011}, and \emph{a priori}
chosen distributions of power and temperature \cite{bhardwaj2006,
srivastava2010, juan2012}. The designer's toolbox does not yet include a tool
for transient analysis and a tool for dynamic steady-state analysis under
process variation, which are of high importance for certain classes of
applications. Furthermore, the state-of-the-art reliability models lack a
flexible approach to taking the effect of process variation on power and
temperature into consideration. This chapter addresses the aforementioned
concerns.
