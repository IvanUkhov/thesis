In order to achieve the established goal, we make use of indirect measurements.
Specifically, instead of \g, we measure an auxiliary parameter \h, which we
refer to as the quantity of measurement. The observations of \h are then
processed via Bayesian inference introduced in \xref{bayesian-statistics} in
order to derive the on-wafer distribution of \g. The quantity \h is chosen such
that \one~\h is convenient and cheap to be measured; \two~\h depends on \g,
which is signified by $\h = \f(\g)$; and \three~there is a way to compute \h
given \g. The last means that \f should be known. However, \f does not have to
be specified analytically since our framework treats \f as a ``black box.'' For
example, \f can be a piece of code.

Without loss of generality, we adhere to the following convention. Each die is a
potential measurement site, and $\hnd < \nd$ denotes the number of those sites
that are actually measured. Each site comprises \np measurement points, and
there are \ns data instances per each point. For instance, in the example given
in \sref{bayes-motivation}, an observation at a site is a temperature profile
$\mq \in \real^{\np \times \ns}$, which is a matrix capturing temperature of \np
processing elements at \ns time moments as defined in
\eref{temperature-profile}. Denote the collected data by $H$ and assume that the
corresponding locations are recorded along with $H$.

It is worth noting that, if \f is the identity function, that is, $\h = \f(\g) =
\g$, the proposed technique will primarily focus on the reconstruction of any
missing observations in $H$, that is, on the unobserved sites on the wafer. From
this standpoint, our approach is a generalization of those developed in
\cite{reda2009, zhang2010}.

\inputfigure{bayes-overview}
In the rest of this section, we present our framework for characterizing process
variation. The technique revolves around \eref{bayes-theorem} and is divided
into four major stages depicted in \fref{bayes-overview}. Stage~1 is the
data-harvesting stage where the designer collects a set of observations of the
quantity of measurement \h and, thereby, forms the data set $H$. At Stage~2, we
undertake an optimization procedure that is to assist the sampling procedure at
Stage~3. The latter produces a collection of samples of the quantity of interest
\g, such as the effective channel length, denoted by $G$. This data set $G$ is
then processed at Stage~4 in order to estimate the desired statistics about \g
such as the probability of \g being smaller than a certain threshold; recall the
example in \sref{bayes-motivation}. It can be seen in \fref{bayes-overview} that
Stage~2 and Stage~3 actively communicate with the two models on the right-hand
side called the data model and statistical model, which is what we elaborate on
in the following two subsections.

\subsection{Data Model}
\slab{bayes-data-model}

The data model relates the quantity of interest \g with the quantity of
measurement \h as follows:
\[
  \h = \f(\g).
\]
The function \f depends on the choice of \h and is specified by the designer.
The data model is utilized in order to predict the values of \h at the same
measurement sites, at the same measurement points, and with the same number and
meaning of data instances as the ones in $H$ obtained at Stage~1. The resulting
data are stacked into a single vector denoted by $\vh \in \real^{\hnd \np \ns}$.
Let also $\hvh \in \real^{\hnd \np \ns}$ be a stacked version of the data in $H$
such that the respective elements of \vh and \hvh correspond to the same
locations on the wafer.

In order to acquire a better understanding of the data model, let us return to
the example given in \sref{bayes-motivation}. In this case, \g stands for the
effective channel length, and \h stands for the temperature profile \mq
corresponding to a fixed workload. The data model can be roughly divided into
two transitions: \one~from the effective channel length \g to the static power
$\p_\static$ and \two~from the static power $\p_\static$ to the corresponding
temperature profile \h. At this point, it is worth recalling the power model
presented in \sref{power-model}. The first transition is due to the dependence
of the leakage current on the effective channel length, which is implicitly
present in \eref{static-power}. Then the transition can be made by means of the
model in \eref{static-power} or any of its variations; see, for instance,
\cite{chandrakasan2000, srivastava2010, juan2012}. In particular, an adequate
model of the static power can be constructed via a fitting procedure applied to
a data set of \up{SPICE} simulations of reference electrical circuits. The only
requirement to such a model is that it should be parameterized by \g. In
addition, it can also be parameterized by temperature in order to take account
of the interdependence between power and temperature described in
\sref{power-model}. The second transition is made by combining the static power
$\p_\static$ with the dynamic power $\p_\dynamic$ that corresponds to the
considered workload. The obtained total power along with the relevant
temperature-related information---such as the floorplan and thermal parameters
of the platform at hand---are fed to a temperature model in order to acquire the
corresponding temperature \h, which is a topic of \cref{certainty-development}.

\subsection{Statistical Model}
\slab{bayes-statistical-model}

\inputfigure{bayes-statistical-model}
Once the wafer has been fabricated, the values of \g across the wafer are fixed;
however, they remain unknown to the designer. In order to infer them, we employ
the procedure developed in the current subsection, which can also be seen in
\fref{bayes-statistical-model}. The procedure consists of the five steps
described below.

Step~1 is to assign an adequate model to the unknown \g. We model \g as a
Gaussian process \cite{rasmussen2006} since \one~it is flexible in capturing the
correlation patterns induced by the fabrication process; \two~it is
computationally efficient; and \three~Gaussian distributions are often adequate
models of the uncertainty due to process variation \cite{reda2009,
srivastava2010, juan2012}. The model is denoted by
\begin{equation} \elab{bayes-prior-quantity}
  \g | \u_\g \sim \mathrm{Gaussian\ Process}(\mu, v)
\end{equation}
where $\mu: \real^2 \to \real$ and $v: \real^2 \times \real^2 \to \real$ are the
mean function and covariance function of \g, respectively, which take locations
on the wafer as arguments. The notation also indicates that \g depends on a set
of parameters $\u_\g$, which we shall identify later on. Prior to taking any
measurements, \g is assumed to be spatially unbiased; therefore, we let $\mu$ be
a single location-independent parameter $\mu_\g$, that is, $\mu(\v{r}) = \mu_\g$
for any $\v{r} \in \real^2$. The covariance function $v$ is chosen to be the
following:
\begin{equation} \elab{bayes-covariance}
  v(\v{r}_1, \v{r}_2) = \sigma_\g^2 \, k(\v{r}_1, \v{r}_2)
\end{equation}
for $\v{r}_1 \in \real^2$ and $\v{r}_2 \in \real^2$ where
\begin{equation} \elab{bayes-correlation}
  k(\v{r}_1, \v{r}_2) = w \, k_\SE(\v{r}_1, \v{r}_2) + (1 - w) k_\OU(\v{r}_1, \v{r}_2)
\end{equation}
is the correlation function, and
\begin{align*}
  k_\SE(\v{r}_1, \v{r}_2) & = \exp\left(-\frac{\norm[2]{\v{r}_1 - \v{r}_2}^2}{\ell_\SE^2}\right) \text{ and} \\
  k_\OU(\v{r}_1, \v{r}_2) & = \exp\left(-\frac{\absolute{\norm[2]{\v{r}_1} - \norm[2]{\v{r}_2}}}{\ell_\OU}\right)
\end{align*}
are the squared-exponential and Ornstein--Uhlenbeck kernels
\cite{rasmussen2006}, respectively. In these formulae, $\sigma_\g^2$ is the
variance of \g; $w \in [0, 1]$ is a weight coefficient; $\ell_\SE > 0$ and
$\ell_\OU > 0$ are so-called length-scale parameters; and $\norm[2]{\cdot}$
stands for the Euclidean distance. The choice of $v$ is based on the
observations of the correlation structures induced by the fabrication process
\cite{chandrakasan2000, cheng2011}. Specifically, $k_\SE$ imposes similarities
on locations that are close to each other on the wafer, and $k_\OU$ imposes
similarities on locations that are at the same distance from the center of the
wafer. The parameters $\ell_\SE$ and $\ell_\OU$ control the extend of these
similarities, that is, the range where the correlation between two locations is
significant. Although all the above parameters of \g can be inferred from data,
for simplicity, we focus only on $\mu_\g$ and $\sigma_\g^2$. The rest of the
parameters---namely, $w$, $\ell_\SE$, and $\ell_\OU$---are assumed to be
determined prior to our analysis based on the knowledge of the correlation
patterns typical for the fabrication process considered; see \cite{marzouk2009}
and the references therein.

Step~2 is to make the above model of \g computationally tractable. The model is
an infinite-dimensional object as it characterizes a continuum of locations. For
practical computations, it should be reduced to a finite-dimensional one. First,
\g is discretized with respect to the union of two sets of locations. The first
one is composed of the $\hnd \np$ points where the observations in $H$ are made
(\hnd measurement sites with \np measurement points each), and the other one of
the locations where the designer wishes to characterize \g. For simplicity,
assume that the designer is interested in all the sites, which is $\nd \np$
locations in total. Let $\vg \in \real^{\nd \np}$ store the values of \g at
these locations. Second, the dimensionality is reduced further via the technique
described in \xref{probability-transformation}. Concretely, we perform the
transformation in \eref{model-order-reduction} with respect to the correlation
matrix of \vg computed via \eref{bayes-covariance}. The result is
\begin{equation} \elab{bayes-reduction}
  \vg = \mu_\g \v{1} + \sigma_\g \m{U} \tm{\Lambda}^\frac{1}{2} \vz
\end{equation}
where $\v{1} = (\range{1}{1}) \in \real^{\nd \np}$, and $\vz = (\z_i) \in
\real^\nz$ is a vector of independent random variables that obey the standard
Gaussian distribution. The number \nz is the final dimensionality of the model
of \g; typically, $\nz \ll \nd \np$. In addition, the parameters $\u_\g$ in
\eref{bayes-prior-quantity} are now known: $\u_\g = \{ \vz, \mu_\g, \sigma^2_\g
\}$; see \fref{bayes-statistical-model}. The model is now ready for practical
computations.

Step~3 is to define a likelihood function, which is where the observed
information is taken into account; see \xref{bayesian-statistics}. In our case,
the observed information is the measurements $H$ stacked into \hvh as described
in \sref{bayes-data-model}. Since the measurement process is not perfect, we
also have to take into consideration measurement noise. To this end, the
observed \hvh is assumed to deviate from the data model's prediction \vh as
follows:
\[
  \hvh = \vh + \v{\epsilon}
\]
where $\v{\epsilon}$ is an $\hnd \np \ns$-dimensional vector of noise, which is
typically assumed to be a white Gaussian noise \cite{rasmussen2006,
marzouk2009}. Without loss of generality, the noise is assumed to be independent
of \g and to have the same magnitude for all measurements. Hence, the model of
the noise is as follows:
\[
  \v{\epsilon} | \sigma^2_\epsilon \sim \mathrm{Gaussian}(\v{0}, \sigma^2_\epsilon \m{I})
\]
where $\sigma^2_\epsilon$ is the variance of the noise. At this point, all the
parameters of the inference are identified, and they are $\u = \u_\g \cup \{
\sigma_\epsilon^2 \} = \{ \vz, \mu_\g, \sigma_\g^2, \sigma_\epsilon^2 \}$; see
\fref{bayes-statistical-model}. Taking the above into account, we obtain
\begin{equation} \elab{bayes-likelihood}
  \hvh | \u \sim \mathrm{Gaussian}(\vh, \sigma_\epsilon^2 \m{I}).
\end{equation}
The density function of this distribution is the likelihood $p(H | \u)$ of our
statistical model, which is the first element of the posterior shown in
\eref{bayes-theorem}.

Step~4 is to decide on the second element of the posterior in
\eref{bayes-theorem}, that is, on the prior $p(\u)$. We put the following priors
on the parameters in \u:
\begin{align} \elab{bayes-prior}
  \begin{split}
    \vz               & \sim \mathrm{Gaussian}(\v{0}, \m{I}), \\
    \mu_\g            & \sim \mathrm{Gaussian}(\mu_0, \sigma^2_0), \\
    \sigma^2_\g       & \sim \mathrm{Scaled\ Inverse\ } \chi^2(\nu_\g, \tau^2_\g), \text{ and} \\
    \sigma^2_\epsilon & \sim \mathrm{Scaled\ Inverse\ } \chi^2(\nu_\epsilon, \tau^2_\epsilon).
  \end{split}
\end{align}
The prior of \vz is due to the decomposition in \eref{bayes-reduction}. The
other three priors, that is, a Gaussian and two scaled inverse chi-squared
distributions, are a common choice for a Gaussian model whose mean and variance
are unknown. The parameters $\mu_0$, $\tau^2_\g$, and $\tau^2_\epsilon$
represent the presumable values of $\mu_u$, $\sigma^2_\g$, and
$\sigma^2_\epsilon$, respectively, and are set by the designer based on the
prior knowledge of the technological process and measurement equipment. The
parameters $\sigma_0$, $\nu_\g$, and $\nu_\epsilon$ reflect the precision of
this information. When the prior knowledge is weak, less specific priors can be
considered \cite{gelman2013}. Finally, $p(\u)$ in \eref{bayes-theorem} is
obtained by multiplying the densities of the priors in \eref{bayes-prior}.

Step~5 is to calculate the posterior $p(\u | H)$ in \eref{bayes-theorem}. To
this end, the likelihood function $p(H | \u)$, which is the density of the
distribution shown in \eref{bayes-likelihood}, and the prior $p(\u)$, which is
the product of the densities of the distributions shown in \eref{bayes-prior},
are put together. The density of the resulting posterior distribution is as
follows:
\begin{equation} \elab{bayes-posterior}
  p(\u | H) \propto p(\hvh | \vz, \mu_\g, \sigma^2_\g, \sigma^2_\epsilon) p(\vz) p(\mu_\g) p(\sigma^2_\g) p(\sigma^2_\epsilon).
\end{equation}
Provided that there is a way to draw samples from \eref{bayes-posterior}, \g can
be readily analyzed as we shall see in \sref{bayes-post-processing}. The
problem, however, is that direct sampling of the posterior is difficult due to
the data model involved in the likelihood function via \vh; see
\eref{bayes-likelihood}. In order to circumvent this problem, we utilize the
Metropolis--Hastings algorithm \cite{gelman2013} outlined in
\xref{bayesian-statistics}, which operates on an auxiliary distribution called
the proposal distribution. The construction of an adequate proposal is discussed
next.

\subsection{Optimization Procedure}
\slab{bayes-optimization}

In this subsection, we describe the objective of Stage~2 in
\fref{bayes-overview}. Although the requirements for the proposal distribution
are mild, it is often difficult to pick an efficient proposal, that is, such a
proposal that would yield a good approximation with as few evaluations of the
posterior and, thus, of the data model in \sref{bayes-data-model} as possible.
This choice is especially difficult in high-dimensional problems, and our
problem---which involves around 30 parameters as we shall see in
\sref{bayes-results}---is one them. Therefore, a careful construction of the
proposal is an essential component of the proposed framework.

A common technique for constructing a high-quality proposal is to optimize the
posterior in \eref{bayes-posterior}. Specifically, we seek such a value $\u^*$
of \u that maximizes \eref{bayes-posterior} and, hence, has the maximum
posterior probability. In addition, we calculate the negative of the Hessian
matrix at $\u^*$, which is called the observed information matrix and denoted by
$\m{J}$; see the output of Stage~2 in \fref{bayes-overview}. Using $\u^*$ and
$\m{J}$, we can construct such a proposal that allows the Metropolis--Hastings
algorithm \one~to start producing samples directly from the desired regions of
high probability and \two~to explore those regions more rapidly. The usage of
$\u^*$ and $\m{J}$ is explained next.

\subsection{Sampling Procedure}

Let us turn to Stage~3 in \fref{bayes-overview}. In order to construct an
adequate proposal and utilize it for sampling, we have at our disposal $\u^*$
and $\m{J}$ from Stage~2. A commonly used proposal is a multivariate Gaussian
distribution where the mean is the current location of the chain of samples
started at $\u^*$, and the covariance matrix is the inverse of $\m{J}$
\cite{gelman2013}. In order to speed up the sampling process, we would like to
make use of parallel computing. The aforementioned proposal, however, is purely
sequential since the mean for the next sample draw is dependent on the previous
sample. Therefore, we appeal to a variation of the Metropolis--Hastings
algorithm known as the independence sampler \cite{gelman2013}. In this case, a
typical choice of the proposal is a multivariate t-distribution independent of
the current position of the chain as follows:
\begin{equation} \elab{bayes-proposal}
  \u \sim t_\nu(\u^*, \alpha^2 \m{J}^{-1})
\end{equation}
where $\u^*$ and $\m{J}$ are as in \sref{bayes-optimization}; \nu is the number
of degrees of freedom; and $\alpha$ is a tuning constant controlling the
standard deviation of the proposal. Now sampling the proposal in
\eref{bayes-proposal} and evaluating the posterior in \eref{bayes-posterior} can
be done in parallel. The obtained samples are then accepted or rejected
subsequently as in the usual Metropolis--Hastings algorithm.

Having completed the sampling procedure, we obtain a collection of samples of
the parameterization $\u = \{ \vz, \mu_\g, \sigma^2_\g \}$. The first portion of
the drawn samples is typically discarded as being unrepresentative; this portion
is known as the burn-in period. The preserved samples of \u are then passed
through \eref{bayes-reduction} in order to compute samples of \g, which are $\nd
\np$-dimensional. Denote the corresponding set of samples by $G$ and let its
cardinality be \no.

\subsection{Post-Processing}
\slab{bayes-post-processing}

At Stage~4 in \fref{bayes-overview}, using $G$, the designer computes the
desired statistics about the quantity of interest \g such as the most probable
value of \g at some location on the wafer and the probability of a certain area
on the wafer being defective. These computations are undertaken in the same way
as it is typically done when \ac{MC} sampling is utilized. Specifically, they
reduce to the estimation of expected values with respect to the posterior
distribution of \u given in \eref{bayes-posterior}: in order to calculate a
certain quantity dependent on \g, one evaluates it for each sample in $G$ and
then takes the average value.

The strength of the Bayesian approach to inference starts to shine when one is
interested in assessing the trustworthiness of the measured data and, therefore,
the credibility of estimates and decisions based on these data. Such an
assessment can be readily undertaken using our framework since the delivered
posterior contains all the needed information about the quantity of interest \g.
This is especially helpful in decision-making as noted in
\sref{bayes-motivation}.
