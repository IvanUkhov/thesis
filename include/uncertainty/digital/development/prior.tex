As we note in \sref{prior}, a sampling method would be a reasonable solution to
probabilistic analysis of electronic systems if these systems were
computationally inexpensive to evaluate. In order to eliminate or reduce the
costs associated with direct sampling, a number of techniques have been
introduced.

We begin by noting that our work presented in
\cref{uncertainty-analog-development} and, therefore, the prior studies
described in \sref{chaos-prior} are of relevance in this section. However, since
the corresponding techniques are designed for tackling process variation, we do
not discuss them further, which is motivated in \sref{frame-motivation}.

In the case of the uncertainty stemming from workload, timing analysis has drawn
the major attention \cite{quinton2012}. A seminal work on response time analysis
of periodic tasks with random execution times on uniprocessor systems is
reported in \cite{diaz2002}. A novel analytical solution to this problem is
given in \cite{tanasa2015}, which makes milder assumptions and allows for
addressing larger, previously unsolvable problems. The framework proposed in
\cite{santinelli2011} uses real-time calculus in order to facilitate task
scheduling by providing probabilistic bounds on the resource given to a task
flow and the resource needed by that task flow.

Studying the literature on probabilistic analysis of electronic systems related
to \cref{uncertainty-analog-development} and this chapter, one can note a
pronounced trend: the generality and straightforwardness of sampling methods
tend to be lost. The proposed techniques typically \one~require restrictive
assumptions to be fulfilled, such as the absence of correlation, \two~are
tailored to one concrete quantity of interest, such as the response time, and
\three~require substantial effort in order to be deployed. However, one should
also keep in mind what is practical. First of all, although additional
assumptions might make the mathematics analytically solvable, they often do not
hold in reality and oversimplify the model. An exact analytical solution might
also be extremely complex, requiring a lot of computational resources upon
evaluation. Furthermore, it is often the case that there has already been
developed a robust simulator evaluating the quantity under consideration in the
deterministic scenario. Switching to probabilistic analysis based on analytical
approaches might mean discarding this battle-tested code and implementing
something else from scratch, which is wasteful.

Some of the techniques mentioned earlier, in fact, preserve the generality and
straightforwardness of sampling methods. An example is our probabilistic
framework presented in \cref{uncertainty-analog-development}. The reason is that
the construction of \ac{PC} expansions in the framework is undertaken by means
of nonintrusive spectral projections \cite{xiu2010}, which, similarly to
sampling methods, do not need to look inside the ``black box.'' However, as
motivated in \sref{frame-motivation}, nonsmoothness is a serious problem for
global approximation based on polynomials. In particular, the convergence rate
of \ac{PC} expansions deteriorates substantially in such cases, requiring
partitioning the stochastic space in order to alleviate the problem. Therefore,
it is not straightforward to apply such techniques as the one given in
\cref{uncertainty-analog-development} to sources of uncertainty that exhibit
nonsmoothness.

To conclude, the available techniques for probabilistic analysis of electronic
systems are restricted in use. A flexible and easy-to-deploy framework capable
of tackling nonsmooth uncertainty-quantification problems is needed.
