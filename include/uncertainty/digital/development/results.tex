In this section, we evaluate our framework. All the experiments are conducted on
a \up{GNU}/Linux machine equipped with 16 processors, each being an Intel Xeon
E5520 2.27~\up{GH}z, and 24~\up{GB} of \up{RAM}. All the source code,
configuration files, and input data used in the experiments are available online
at \cite{eslab2017a}.

We consider three platform sizes \np, two application sizes \nt, and three
quantities of interest \g. Specifically, \np is in $\{ 2, 4, 8 \}$, \nt is in
$\{ 10, 20 \}$, and \g is the end-to-end delay, total energy consumption, or
maximum temperature; the quantities of interest are defined in
\eref{frame-delay}, \eref{frame-energy}, and \eref{frame-temperature},
respectively. Therefore, we address 18 problems in total, which correspond to
different combinations of \np, \nt, and \g. At this point, it might be helpful
to recall the illustrative application depicted in \fref{frame-application}.

Each problem is configured as follows. A platform with \np processing elements
and an application with \nt tasks are generated randomly by \up{TGFF}
\cite{dick1998}. In particular, the tool generates a table for each processing
element that specifies certain properties of the tasks when they are mapped to
that processing element. Namely, each table assigns two numbers to each task: a
reference execution time---which is chosen uniformly between 10 and 50~ms---and
a power consumption---which is chosen uniformly between 5 and 25~W. The
application is scheduled using a list scheduler \cite{adam1974}. The mapping of
the application is fixed and obtained by scheduling the tasks based on their
reference execution times and assigning them to the earliest available
processing elements.

Similar to the previous chapters, the construction of thermal \up{RC} circuits,
which are needed for temperature analysis, is delegated to HotSpot
\cite{skadron2003}. The floorplan of each platform is a regular grid where each
processing element occupies 2~×~2~mm\textsuperscript{2} on the die. The modeling
of the static power is based on a linear fit to a data set of \up{SPICE}
simulations of a series of \up{CMOS} invertors. The sampling interval of power
and temperature profiles is equal to 1~ms.

The uncertain parameters \vu are the execution times of the tasks, and their
marginal distributions and correlation matrix are as described in
\sref{frame-formulation}; all other parameters are deterministic. Regarding the
marginal distributions---which are beta distributions as shown in
\eref{beta-distribution}---the left $c$ and right $d$ endpoints of their
supports are set to 80\% and 120\%, respectively, of the reference execution
times generated by \up{TGFF} as described earlier. The parameter $a$ and $b$ are
set to two and five, respectively, for all tasks, which skews the distributions
toward the left endpoints. The model-order-reduction parameter $\eta$ in
\eref{model-order-reduction} is set to 0.9, which results in $\nz = 2$ and $\nz
= 3$ independent variables for applications with $\nt = 10$ and $\nt = 20$
tasks, respectively.

The configuration of the interpolation algorithm---including the collocation
nodes, basis functions, and adaptation strategy with stopping
conditions---follows closely the description given in \sref{frame-construction}.
The parameters \error{a}, \error{r}, and \error{s} are around
10\textsuperscript{3}, 10\textsuperscript{2}, and 10\textsuperscript{4},
respectively, depending on the problem.

The performance of our framework with respect to each problem is assessed as
follows. First, as in \cref{uncertainty-analog-development}, the true
probability distribution of \g is estimated by sampling \g directly and
extensively. Direct sampling means that there is no any intermediate
representation or any intermediate model order reduction involved. Second, we
construct an interpolant for \g and estimate \g's distribution by sampling this
interpolant, which is discussed in \sref{frame-construction} and
\sref{frame-processing}. In both cases, we draw 10\textsuperscript{5} samples;
let us remind, however, that, unlike the cost of direct sampling, the cost of
sampling the interpolant is practically negligible. Third, we perform another
round of direct sampling. This time we draw as many samples as many times the
calculation of \g is invoked during the interpolation process. In each of the
three cases, sampling is undertaken in accordance with a Sobol sequence, which
is a quasi-random low-discrepancy sequence featuring much better convergence
properties than those of the classical \ac{MC} sampling \cite{joe2008}. As a
result, we obtain three estimates of \g's distribution: reference (the one
considered true), proposed (the one based on interpolation), and direct (the one
equal in terms of the number of \g's evaluations to the proposed solution). The
last two are compared with the first one. For computing the proximity between
two distributions, we use the well-known \ac{KS} statistic \cite{rao2002}, which
is the supremum over the distance (pointwise) between two empirical distribution
functions and hence is a rather unforgiving error indicator.

\inputfigure{frame-accuracy-delay}
\inputfigure{frame-accuracy-energy}
\inputfigure{frame-accuracy-temperature}
The results are given in \fref{frame-accuracy-delay},
\fref{frame-accuracy-energy}, and \fref{frame-accuracy-temperature}, which
correspond to the end-to-end delay, total energy, and maximum temperature,
respectively. Each figure contains six plots arranged in a three-by-two grid.
The three rows of the grid correspond to the three platform sizes, and the two
columns correspond to the two application sizes. The horizontal axis of each
plot shows the number of points, that is, the number of evaluations of \g, and
the vertical one shows the \ac{KS} statistic on a logarithmic scale. Each plot
has two lines. The blue line represents our technique. The circles on this line
correspond to the steps of the interpolation process shown in
\eref{frame-sparse}. They illustrate how the \ac{KS} statistic computed with
respect to the reference solution changes as the interpolation process takes
steps (and adds more and more nodes into the interpolant) until a stopping
condition is satisfied. Note that only a subset of the actual steps is displayed
in order to make the figure legible. Synchronously with the blue line---that is,
for the same numbers of evaluations of the quantity of interest---the orange
line shows the error of direct sampling, which is also calculated with respect
to the reference solution.

We begin by describing one particular problem shown in the figures. Consider,
for instance, the one labeled with $\bigstar$ in \fref{frame-accuracy-energy}.
It can be seen that, at the very beginning, our solution and the solution
delivered by direct sampling are poor. The \ac{KS} statistic tells us that there
are substantial mismatches between the estimates and the reference solution.
However, as the interpolant is being adaptively refined, our solution approaches
rapidly the reference one and, by the end of the interpolation process, leaves
the solution of naïve sampling approximately an order of magnitude behind.

Studying \fref{frame-accuracy-delay}, \fref{frame-accuracy-energy}, and
\fref{frame-accuracy-temperature}, one can make a number of observations. Our
interpolation-powered approach (blue lines) to probabilistic analysis
outperforms direct sampling (orange lines) in all the cases. This means that,
given a fixed budget of the computation time, the probability distributions
delivered by our framework are closer to the true ones than those delivered by
sampling \g directly, despite the fact that the latter relies on Sobol
sequences, which are a sophisticated sampling strategy. Since direct sampling
methods try to cover the probability space impartially, the figures are a
salient illustration of the difference between being adaptive and nonadaptive.

It can also be seen in the figures that, as the number of evaluations increases,
the solutions computed by our technique approach the true ones. The error of our
framework decreases generally steeper than the one of direct sampling. The
decrease, however, tends to plateau toward the end of the interpolation process
(when a stopping condition is satisfied). This behavior can be explained by the
following two reasons. First, the algorithm has been instructed to satiate
certain accuracy requirements (\error{a}, \error{r}, and \error{s}), and it
reasonably does not do more than what has been requested. Second, since model
order reduction is performed in the case of interpolation, the quantity being
interpolated is not \g strictly speaking; it is a lower-dimensional
representation of \g, which already implies an information loss. Therefore,
there is a limit on the accuracy that can be achieved, which depends on the
amount of reduction.

The message of the above observations is that the designer of an electronic
system can benefit substantially in terms of accuracy per computation time by
switching from direct sampling to the proposed technique. If the designer's
current workhorse is the classical \up{MC} sampling, the switch might lead to
even more dramatic savings than those shown in the figures. It is worth
mentioning that the gain is especially prominent in situations where the
analysis needs to be performed many times such as for the purpose of
design-space exploration.

\begin{remark}
The wall-clock time taken by the experiments is not reported here as this time
is irrelevant: since the evaluation of \g is time consuming (see
\sref{frame-problem}), the number of \g's evaluations is the most apposite
expense indicator. For the curious reader, however, let us give an example by
considering the problem labeled with $\clubsuit$ in
\fref{frame-accuracy-temperature}. Obtaining a reference solution with
10\textsuperscript{5} simulations in parallel on 16 cores takes around two
hours. Constructing an interpolant with 383 collocation nodes takes around 30
seconds (this is also the time of direct sampling with 383 simulations of \g).
Evaluating the interpolant 10\textsuperscript{5} times takes less than a second.
The relative computation cost of sampling an interpolant readily diminishes as
the complexity of \g increases; contrast it with direct sampling, whose cost
grows proportional to \g's evaluation time.
\end{remark}

Last but not least, we investigate the viability of deploying the proposed
framework in a real environment. It means that we are to couple the framework
with a battle-proven simulator, used in both academia and industry, and then let
this simulator evaluate a real application running on a real platform.

The scenario that we consider is similar to the one depicted in
\fref{frame-application}. In this case, an industrial-standard simulator is put
in place of the ``black box'' on the left-hand side of the figure, and the
quantity of interest \g is now the total energy. Unlike the synthetic examples
discussed earlier, there is no true solution to compare with due to the
prohibitive expense of running the simulator, which is exactly why our framework
is needed in such cases.

The chosen simulator is the well-known and widely used combination of Sniper
\cite{carlson2011} and \up{McPAT} \cite{li2009}. The architecture that we
simulate is Intel's Nehalem-based Gainestown series. Sniper is distributed with
a configuration file for this architecture, and we utilize it without any
changes. The simulated platform is set up to have three \up{CPU}s sharing one L3
cache. Regarding the application chosen for this example, it is \up{VIPS}, which
is an image-processing piece of software taken from the \up{PARSEC} benchmark
suite \cite{bienia2011}. In this scenario, \up{VIPS} applies a fixed set of
operations to a given image. The width and height of the image to process are
considered as the uncertain parameters \vu, which are assumed to be distributed
uniformly within certain ranges.

The deployment of the real-life example has fulfilled our expectations. The
interpolation process has successfully finished and delivered an interpolant
after 78 invocations of the simulator. Each invocation takes 40 minutes on
average. The probability distribution of the total energy has been estimated by
sampling the constructed surrogate 10\textsuperscript{5} times. These many
samples would take around 6 months to obtain on our machine if we sampled the
simulator directly in parallel on 16 cores; using the technique presented in
this chapter, the whole procedure has taken approximately 9 hours.
