We develop a framework for probabilistic analysis of electronic systems that is
efficient in characterizing the impact of workload uncertainty on a design. The
proposed framework is also straightforward to use in practice. The effectiveness
of our approach is due to the powerful approximation engine that the framework
features. Specifically, we make use of the hierarchical interpolation with
hybrid adaptivity developed in \cite{klimke2006, ma2009, jakeman2012}, which
enables tackling diverse design problems while keeping the associated
computational costs low. The usage of the framework is streamlined---which is
also the case with the framework presented in
\cref{uncertainty-analog-development}---because it has the same low entrance
requirements as sampling techniques: one only has to be able to evaluate the
quantity of interest given a set of deterministic parameters. Moreover, it can
be utilized in scenarios with limited knowledge of the joint probability
distribution of the uncertain parameters, which are common in practice.

The general solution strategy here is similar to the one outlined in
\sref{chaos-solution}. First, we note that making use of a sampling method is a
compelling approach to uncertainty quantification, and we would readily apply
such a method to study the quantity \g if only evaluating \g had a small cost,
which it does not. Our solution to this quandary is to construct a light
representation of the heavy \g and study this representation instead of \g.
Given the setting of this chapter, the surrogate that we build is based on
adaptive interpolation: \g is evaluated at a number of strategically chosen
collocation nodes, and any other values of \g are reconstructed on demand
(without involving \g) using a set of basis functions mediating between the
collected values of \g. The benefit of this approach is in the number of
invocations of the quantity \g: only a few evaluations of \g are needed, and the
rest of probabilistic analysis is powered by the constructed interpolant, which,
in contrast to \g, has a negligible cost.

The solution process has four stages, and they reflect the ones depicted in
\fref{chaos-overview}. At Stage~1, the quantity of interest \g and the uncertain
parameters \vu are decided upon by the designer. At Stage~2, \g is
reparameterized in terms of an auxiliary random vector \vz extracted from \vu;
the stage is described in \sref{frame-transformation}. At Stage~3, an
interpolant of \g is constructed by considering \g as a deterministic function
of \vz and evaluating \g at a small set of carefully chosen points; the stage is
detailed in \sref{frame-construction}. At Stage~4, the constructed interpolant
of \g is post-processed in order to calculate the desired statistics about \g;
the stage is detailed in \sref{frame-processing}. In particular, the probability
distribution of \g is estimated by applying an arbitrary sampling method to the
interpolant.

The first stage is problem specific, and it is to be elaborated on in
\sref{frame-application}. In what follows, we proceed directly to the second
stage, which together with the third should be approached with a great care
since interpolation of multivariate functions is a challenging undertaking.
