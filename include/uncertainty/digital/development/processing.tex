At Stage~4 of the proposed framework, the constructed hierarchical interpolant
$\interpolant{\nz}{\ls}(\g)$ of the quantity of interest \g is processed
according to the designer's needs. The post-processing in this chapter is
similar to the one described in \sref{chaos-processing}. Namely, the
probabilistic analysis of \g is carried out on $\interpolant{\nz}{\ls}(\g)$
instead, and \g is never invoked again. Since $\interpolant{\nz}{\ls}(\g)$ is a
lightweight representation of the heavy \g, the analysis has a negligible
computational cost.

In particular, the distribution of \g can be efficiently estimating via a
sampling method of choice. In this case, one draws independent samples from the
distribution of \vz and evaluates $\interpolant{\nz}{\ls}(\g)$ at those points
in accordance with \aref{frame-evaluation}. Having collected a large enough set
of samples denoted by $G$, the density of \g can be estimated by employing such
techniques as kernel density estimation \cite{hastie2013}. Having $G$ at one's
disposal, other statistics about \g, such as probabilities of various events,
can be straightforwardly estimated.

In addition, the expectation and variance of \g can be computed without
sampling. Using an adequate $\transform$ discussed in
\sref{frame-transformation}, \g can be reparameterized in terms of independent
variables that are uniformly distributed on $[0, 1]^\nz$. This means that the
density function of \vz equals unity. Therefore, using \eref{frame-sparse}, we
have the following analytical expression for the expectation:
\[
  \expectation{\g} \approx \expectation{\interpolant{\nz}{\ls}(\g)}
  = \int_{[0, 1]^\nz} \interpolant{\nz}{\ls}(\g)(\vz) \d \vz
  = \sum_{\vi \in \sparseindex{\nz}{\ls}} \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} \Delta(\g \circ \transform)(\vx_{\vi \vj}) w_{\vi \vj}
\]
where
\[
  w_{\vi \vj}
  = \int_{[0, 1]^\nz} e_{\vi \vj}(\vz) \d \vz
  = \prod_{k = 1}^\nz \int_0^1 e_{i_k j_k}(\z_k) \d \z_k
  = \prod_{k = 1}^\nz w_{i_k j_k}.
\]
In the above equation, $w_{ij}$ is calculated as shown in
\eref{frame-basis-volume}.

Regarding the variance, it can be seen in \eref{variance} that $\variance{\g}$
can be assembled from two components: the expectation of \g, which we already
have, and the expectation of $\g^2$, which is missing. The solution is to let
$\h = (\g, \g^2)$ be the quantity of interest instead of \g. Then the
expectations of both \g and $\g^2$ become available in analytical forms, and
$\variance{\g}$ can be computed using \eref{variance}. This approach can be
generalized to probabilistic moments of higher orders.

The careful reader might have noted a problem with the calculation of
$\variance{\g}$ presented above: \h is vector valued. More generally, \g has
been depicted so far as having a one-dimensional codomain. The proposed
framework, however, does not have such a limitation as explained in
\rref{frame-multidimensional-output}.

\begin{remark} \rlab{frame-multidimensional-output}
All the mathematics and pseudocodes stay the same for vector-valued quantities.
The only difference is that, since a surplus $\Delta(\g \circ
\transform)(\vx_{\vi \vj})$ naturally inherits the output dimensionality of \g,
the operations that involve $\Delta(\g \circ \transform)(\vx_{\vi \vj})$ should
be adequately adjusted. If the outputs are on different scales or have different
accuracy requirements, one might want to have different \error{a} and \error{r}
in \eref{frame-stop} for different outputs. In that case, one also needs to
revisit \eref{frame-score} and devise a more sensible strategy for scoring
collocation nodes such as rescaling individual outputs and then calculating
$\norm[2]{\cdot}$ or $\norm[\infty]{\cdot}$.
\end{remark}

To summarize, once an interpolant of \g has been constructed, the distribution
of \g is estimated using versatile sampling methods applied to the interpolant.
The framework extends naturally to quantities with multiple outputs, and it
provides analytical formulae for the expectation and variance.

Lastly, let us remind that the evaluation of \g is an expensive operation. Our
technique is designed to keep this expense as low as possible by choosing
evaluation points adaptively, which is unlike traditional sampling methods.
Moreover, in contrast to \up{PC} expansions and similar techniques, the proposed
framework is well suited for nonsmooth response surfaces.
