At Stage~3, an approximation of the quantity of interest is constructed using
the interpolation algorithm presented in this section. The algorithm constitutes
the core of the framework proposed in this chapter, and it features a sparse
structure, hierarchical construction, and hybrid adaptivity. The benefits of
these characteristics are interconnected and can be summarized as follows:
\one~the ability to efficiently address multidimensional problems, \two~the
ability to progressively refine the approximation, and \three~the ability to
perform this refinement strategically by virtue of a fine-grained error control.

Hierarchical interpolation is introduced in \xref{sparse-interpolation}, and
here we rely heavily on the results given in that section. The mathematics
presented in \xref{sparse-interpolation} as well as below is based on the
development in \cite{klimke2006, ma2009, jakeman2012}.

Consider the quantity of interest \g as a function of \vz via $\transform$ as
shown in \sref{frame-transformation}. Assume that \g belongs to $\continuous{[0,
1]^\nz}$, the space of continuous functions on $[0, 1]^\nz$; the assumption is
not limiting in practice. As shown in \xref{sparse-interpolation}, \g can be
approximated by means of the following interpolant:
\begin{equation} \elab{frame-interpolant}
  \g \approx \interpolant{\nz}{\ls}(\g)
  = \interpolant{\nz}{\ls - 1}(\g) + \sum_{\vi \in \Delta\sparseindex{\nz}{\ls}}
  \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} \Delta(\g \circ \transform)(\vx_{\vi \vj}) e_{\vi \vj} \end{equation}
where $\vi \in \natural^\nz$ is called a level index; $\vj \in \natural^\nz$ is
called an order index; $\set{\vx_{\vi \vj}}$ and $\set{e_{\vi \vj}}$ are
collocation nodes and basis functions, respectively; $\set{\Delta(\g \circ
\transform)(\vx_{\vi \vj})}$ are hierarchical surpluses defined in
\eref{interpolant-sparse-surplus}; and $\Delta\sparseindex{\nz}{\ls}$ and
$\Delta\tensorindex{\nz}{\vi}$ are index sets defined in
\eref{interpolant-sparse-index-delta} and \eref{interpolant-tensor-index-delta},
respectively.

Due to the reasons clarified in \sref{frame-adaptivity}, the interpretation of
\eref{frame-interpolant} used in this chapter is different from the one used in
\xref{sparse-interpolation}. Specifically, $\ls \in \natural$ no longer
represents an interpolation level but rather an interpolation step. Accordingly,
$\interpolant{\nz}{\ls}(\g)$ represents the interpolant obtained by a certain
interpolation step. In addition, all the index sets discussed here are generally
subsets of their full-fledged counterparts defined in
\xref{sparse-interpolation}.

Let us now turn to the choice of collocation nodes and basis functions.

\subsection{Collocation Nodes}
\slab{frame-grid}

\inputfigure{frame-grid}
In order to gain computational efficiency, the integration grid should be fully
nested, which is explained in \xref{sparse-interpolation}. Such a grid can be
constructed using the family of Newton--Cotes rules \cite{ma2009}. In one
dimension, a Newton--Cotes rule is a set of equidistant nodes on $[0, 1]$. There
are two types of Newton--Cotes rules: open and closed. The only difference
between the two types is that the latter includes the endpoints, that is, 0 and
1, while the former does not.

Technically, in order to be able to proceed to hierarchical interpolation, the
chosen rules have to fulfill a certain condition, which is discussed in
\xref{sparse-interpolation} and can be seen in
\eref{interpolant-tensor-exactness}. Closed Newton--Cotes rules satisfy this
condition, and they are the ones used in the original version of local
adaptivity presented in \cite{ma2009}. The open rules, on the other hand, do not
fulfill the condition close to the boundaries of the unit interval. However,
according to our experience, open Newton--Cotes rules are a viable option since
they perform well in practice, which is also noted in \cite{klimke2006}. In
fact, we are able to obtain better results with open rules and, therefore,
present them here.

The open Newton--Cotes rule of level $i \in \natural$ is
\[
  \X^1_i = \set{\x_{ij}}{j \in \tensorindex{1}{i}}
\]
where
\begin{align*}
  & \x_{ij} = \frac{j + 1}{n_i + 1}, \\
  & \tensorindex{1}{i} = \set{i - 1}_{i = 1}^{n_i}, \text{ and} \\
  & n_i = 2^{i + 1} - 1.
\end{align*}
\fref{frame-grid} depicts the first three levels of this rule. It can be seen
that the number of nodes grows as 1, 3, 7, and so on, and that the rule is fully
nested. In multiple dimensions, collocation nodes are formed as shown in
\eref{interpolant-tensor}.

\subsection{Basis Functions}
\slab{frame-basis}

\inputfigure{frame-basis}
The basis functions that go hand in hand with open Newton--Cotes rules are
piecewise linear functions. For $i = 0$ and $j = 0$,
\[
  e_{00}(\x) = 1.
\]
For $i > 0$ and $j = 0$ (close to the left endpoint),
\[
  e_{i0}(\x) =
  \begin{cases}
    2 - \left(n_i + 1\right) \x, & \text{if } \x < \frac{2}{n_i + 1}; \\
    0, & \text{otherwise}.
  \end{cases}
\]
For $i > 0$ and $j = n_i - 1$ (close to the right endpoint),
\[
  e_{i, n_i - 1}(\x) =
  \begin{cases}
    \left(n_i + 1\right) \x - n_i + 1, & \text{if } \x > \frac{n_i - 1}{n_i + 1}; \\
    0, & \text{otherwise}.
  \end{cases}
\]
In other cases,
\[
  e_{ij}(\x) =
  \begin{cases}
    1 - \left(n_i + 1\right)|\x - \x_{ij}|, & \text{if } |\x - \x_{ij}| < \frac{1}{n_i + 1}; \\
    0, & \text{otherwise}.
  \end{cases}
\]
The basis functions that correspond to the first three levels of one-dimensional
interpolation are depicted in \fref{frame-basis}. In multiple dimensions, basis
functions are formed as shown in \eref{interpolant-tensor}, which results in
\[
  e_{\vi \vj}(\vx) = \prod_{k = 1}^n e_{i_k j_k}(x_k).
\]

Additionally, let us calculate the volumes (the integrals over the whole domain)
of the aforementioned piecewise linear functions; these volumes are required in
the continuation. For $i = 0$ and $j = 0$,
\[
  w_{00} = 1.
\]
For $i > 0$ and $j \in \set{0, n_i - 1}$,
\[
  w_{ij} = \frac{2}{n_i + 1}.
\]
In other cases,
\[
  w_{ij} = \frac{1}{n_i + 1}.
\]
The volumes of multidimensional basis functions are products of the
corresponding one-dimensional volumes as follows:
\begin{equation} \elab{frame-volume}
  w_{\vi \vj} = \prod_{k = 1}^\nz w_{i_k j_k}.
\end{equation}

Imagine now a function that is nearly flat on the first half of $[0, 1]$ and
rather irregular on the other. Under these circumstances, it is natural to
expect that, in order to attain the same accuracy, the first half should require
much fewer collocation nodes than the other one; recall the example given in
\fref{frame-motivation}. However, if we followed the usual construction
procedure described in \xref{sparse-interpolation}, we would not be able to
benefit from this idiosyncrasy. Both sides would be treated equally, and all the
nodes of each interpolation level would be added to the interpolant, which is
wasteful. The solution to this problem is to make the interpolation algorithm
adaptive, which we discuss next.

\subsection{Hybrid Adaptivity}
\slab{frame-adaptivity}

In order to make the algorithm adaptive, we first need to decide on a criterion
used for measuring the accuracy of the interpolant $\interpolant{\nz}{\ls}(\g)$
in \eref{frame-interpolant} at any point in $[0, 1]^\nz$. Then, when refining
the interpolant, instead of evaluating the quantity of interest \g at all
possible nodes, we choose only those that are located in the regions with poor
accuracy as indicated by the criterion.

We already have a good foundation for building the above-mentioned criterion.
Hierarchical surpluses, which are introduced in \xref{sparse-interpolation} and
defined in \eref{interpolant-sparse-surplus}, are natural indicators of the
interpolation error: they are the difference between the true values of \g and
those estimated by $\interpolant{\nz}{\ls}(\g)$ at the nodes of the underlying
integration grid. Therefore, hierarchical surpluses can be recycled in order to
effectively identify problematic regions.

We proceed as follows. First, a score is assigned to each node $\vx_{\vi \vj}$
or, equivalently, to each pair of a level index and an order index $(\vi, \vj)$
as follows:
\begin{equation} \elab{frame-score}
  s_{\vi \vj} = \absolute{\Delta(\g \circ \transform)(\vx_{\vi \vj}) w_{\vi \vj}}
\end{equation}
where $\Delta(\g \circ \transform)(\vx_{\vi \vj})$ is the surplus at the node as
defined in \eref{interpolant-sparse-surplus}, and $w_{\vi \vj}$ is the volume of
the corresponding basis function as shown in \eref{frame-volume}. The above
score is utilized for guiding the algorithm as explained below.

Each hierarchical interpolant $\interpolant{\nz}{\ls}$ is characterized by a set
of level indices $\sparseindex{\nz}{\ls}$, and each level index $\vi \in
\sparseindex{\nz}{\ls}$ by a set of order indices
$\Delta\tensorindex{\nz}{\vi}$. At each interpolation step $\ls \in \natural$, a
single level index
\[
  \vi_{\ls} \in \sparseindex{\nz}{\ls - 1}
\]
is chosen where $\sparseindex{\nz}{-1} = \set{\v{0}}$. This index gives birth to
$\Delta\sparseindex{\nz}{\ls}$ and $\set{\Delta\tensorindex{\nz}{\vi}}{\vi \in
\Delta\sparseindex{\nz}{\ls}}$, forming the increment given on the right-hand
side of \eref{frame-interpolant}.

The level index set $\Delta\sparseindex{\nz}{\ls}$ contains so-called admissible
forward neighbors of the chosen $\vi_{\ls}$. The forward neighbors of an index
\vi are given by
\[
  \set{\vi + \v{1}_k}_{k = 1}^\nz
\]
where $\v{1}_k \in \set{0, 1}^\nz$ is a vector whose elements are zero except
for element $k$ that is equal to unity. An index \vi is called admissible if its
inclusion into the index set $\sparseindex{\nz}{\ls}$ under consideration keeps
the set admissible. Finally, $\sparseindex{\nz}{\ls}$ is admissible if it
satisfies the following condition \cite{klimke2006}:
\[
  \vi - \v{1}_k \in \sparseindex{\nz}{\ls}
\]
for $\vi \in \sparseindex{\nz}{\ls}$ and $k \in \set{\range{1}{\nz}}$ where the
cases with $i_k = 0$ need no check.

Let us now turn to the content of $\Delta\tensorindex{\nz}{\vi}$ where $\vi =
\vi_{\ls} + \v{1}_k$ for some $k$. It also contains admissible forward
neighbors; however, they are order indices, and their construction is different
from the one used in the case of $\Delta\sparseindex{\nz}{\ls}$. These indices
are identified by inspecting the backward neighborhood of \vi, which is
analogous to the forward one but in the other direction. For each backward
neighbor $\vi - \v{1}_m$ and each $\vj \in \Delta\tensorindex{\nz}{\vi -
\v{1}_m}$, we first check the condition
\[
  s_{\vi - \v{1}_m, \vj} \geq \error{s}
\]
where \error{s} is a user-defined constant referred to as the score error. If
the condition holds, the forward neighbors of \vj in dimension $k$ are added to
$\Delta\tensorindex{\nz}{\vi}$. This procedure is illustrated in
\fref{frame-grid} for open Newton--Cotes rules in one dimension. The arrows
emerging from a collocation node connect the node with its forward neighbors. In
general, each node has two forward neighbors in each dimension. The order
indices of these nodes are
\begin{align*}
  & (j_1, \dots, 2 j_k \phantom{{} + 2}, \dots, j_{\nz}) \text{ and} \\
  & (j_1, \dots, 2 j_k + 2,              \dots, j_{\nz}).
\end{align*}
The above refinement procedure should be undertaken for each level index $\vi
\in \Delta\sparseindex{\nz}{\ls}$ with respect to each dimension $k \in
\set{\range{1}{\nz}}$.

The choice of $\vi_{\ls} \in \sparseindex{\nz}{\ls - 1}$ at each step \ls in
\eref{frame-interpolant} is made as follows. First, each index can be picked at
most once. The rest is resolved by prioritizing the candidates. It is reasonable
to compute the priority of a level index \vi based on the scores of the order
indices associated with this level index, that is, based on the scores of
$\tensorindex{\nz}{\vi}$. We set the priority to the average score
\[
  s_{\vi} = \frac{1}{\cardinality{\Delta\tensorindex{\nz}{\vi}}} \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} s_{\vi \vj}.
\]
Thus, at each step \ls, the index \vi with the highest $s_{\vi}$ is promoted to
$\vi_{\ls}$.

The final question to answer is the stopping condition of the approximation
process in \eref{frame-interpolant}. Apart from the natural constraints on the
maximum number of function evaluations and the maximum interpolation level (the
original \ls in \xref{sparse-interpolation}), we rely on the following
criterion. Given two additional user-defined constants \error{a} and \error{r},
which are referred to as the absolute and relative errors, respectively, the
process is terminated as soon as
\begin{equation} \elab{frame-stop}
  \max_{\vi \vj} \absolute{\Delta(\g \circ \transform)(\vx_{\vi \vj})} \leq \max \set{\error{a}, \error{r} (\g_\maximum - \g_\minimum)}
\end{equation}
where $\g_\minimum$ and $\g_\maximum$ are the minimum and maximum observed
values of \g, respectively. The left-hand side of \eref{frame-stop} corresponds
the largest surplus whose level index has not been refined yet, that is, has not
been considered as $\vi_{\ls}$ at some step \ls. The above criterion is an
adequate technique for curtailing the interpolation process since it is based on
the actual progress.

The adaptivity presented in this subsection is referred to as hybrid since it
combines features of global and local adaptivity \cite{jakeman2012}. Local
adaptivity operates on the level of individual nodes \cite{ma2009}, and it is
motivated in \sref{frame-motivation}. By contrast, global adaptivity operates on
the level of individual dimensions \cite{klimke2006}. The intuition behind
global adaptivity is that, in general, the input variables manifest themselves
(impact \g) differently, and the interpolation algorithm is likely to benefit by
prioritizing those variables that are the most influential.

\conclusioncut
So far, we have formalized an efficient algorithm for adaptive hierarchical
interpolation in multiple dimensions. The main equation is
\eref{frame-interpolant} where $\Delta(\g \circ \transform)(\vx_{\vi \vj})$,
$\vx_{\vi \vj}$, and $e_{\vi \vj}$ are the ones in
\eref{interpolant-sparse-surplus}, \sref{frame-grid}, and \sref{frame-basis},
respectively; and the interpolation is undertaken according to the rules in
\sref{frame-adaptivity}. Next, we discuss a few implementation details.

\subsection{Implementation}

The life cycle of interpolation has roughly two stages: construction and usage.
The construction stage invokes \g at a set of collocation nodes and produces
certain artifacts. The usage stage estimates the values of \g at a set of
arbitrary points by manipulating these artifacts. In this subsection, we provide
the pseudocodes of the two stages in order to give a better sense of the
technique.

Let us first make a general note. According to our experience, it is beneficial
to the clarity and ease of implementation to collapse the two sums in
\eref{frame-interpolant} into one. This requires storing a level index $\vi =
(i_k) \in \natural^\nz$ and an order index $\vj = (j_k) \in \natural^\nz$ for
each node. It is also advantageous to encode each pair $(i_k, j_k)$ as a single
unsigned integer, which, in particular, eliminates excessive memory usage. In
multiple dimensions, this results in a vector $\v{\iota} = (\iota_k) \in
\natural^\nz$, which we simply call an index. Our encoding is
\[
  \iota_k = i_k \lor (j_k \ll \n{\mathrm{bits}})
\]
where $\lor$ and $\ll$ stand for the bitwise \up{OR} and logical left shift,
respectively, and \n{\mathrm{bits}} is the number of bits reserved for storing
level indices, which can be adjusted according to the maximum permitted depth of
interpolation.

\inputalgorithm{frame-construction}
The pseudocode of the construction stage is given in \aref{frame-construction}.
The input is a subroutine called Algorithm~G that evaluates $\g \circ
\transform$. The output is a structure \texttt{interpolant} that contains the
artifacts of the interpolation. These artifacts are a set of tuples
$\set{(\v{\iota}_k, \Delta\g(\vx_{\v{\iota}_k}))}$, which is a comprehensive
description of a hierarchical interpolant. The pseudocode works as follows.

Line 1: Each iteration of the loop corresponds to an interpolation step \ls in
\eref{frame-interpolant}. The progress is captured by a structure
\texttt{state}. The \texttt{strategy} object represents the adaptation strategy
utilized, and it operates in accordance with \sref{frame-adaptivity}. The
\texttt{Continue?} method of \texttt{strategy} checks if any of the stopping
conditions is satisfied, in which case the process is terminated.

Line 2: The \texttt{Next} method of \texttt{strategy} consumes the previous
state and returns the initial state of the ongoing interpolation step. In
particular, it populates the \texttt{indices} field of \texttt{state} with the
indices of the step. The rest of the loop's body populates the rest of
\texttt{state}'s fields so that \texttt{strategy} can adequately execute its
functionality at the beginning of the next iteration.

Line 3: The \texttt{grid} object represents the utilized interpolation grid, and
its \texttt{Compute} method calculates the collocation nodes that correspond to
the given indices, that is, $\set{\vx_{\v{\iota}_k}}$ based on
$\set{\v{\iota}_k}$; see \sref{frame-grid} for more details.

Line 4: Algorithm~G evaluates $\g \circ \transform$ at the collocation nodes.
This is by far the most time consuming operation of the algorithm as \g is
generally expensive to execute. This operation is also a prominent candidate for
parallelization since the algorithm does not impose any particular evaluation
order.

Line 5: \aref{frame-evaluation} is a subroutine that exercises the interpolant
constructed so far at the collocation nodes and, thereby, approximates the
values obtained on line~4. This algorithm is to be discussed separately later
on.

Line 6: The \texttt{Subtract} subroutine computes the difference between the
actual and approximated values of \g, which yields the hierarchical surpluses
$\set{\Delta(\g \circ \transform)(\vx_{\v{\iota}_k})}$ (see
\eref{interpolant-sparse-surplus}) of the current interpolation step.

Line 7: The \texttt{Score} method of \texttt{strategy} calculates the scores of
the collocation nodes based on their surpluses as described in
\sref{frame-adaptivity}.

Line 8: The \texttt{Append} method of \texttt{interpolant} refines the
interpolant by extending it with the indices and surpluses of the completed
iteration.

\inputalgorithm{frame-evaluation}
We now turn our attention to the usage stage of an interpolant. The pseudocode
is given in \aref{frame-evaluation}. This subroutine is also used in
\aref{frame-construction} on line~5. Let us make a couple of observations
regarding this subroutine.

Line 3: The inner loop corresponds to an unfolded version of
\eref{frame-interpolant}; that is, there is no separation into the individual
interpolation steps taken.

Line 4: The \texttt{basis} object represents the utilized interpolation basis,
and its \texttt{Compute} method evaluates the basis functions that correspond to
the given indices, that is, $\set{e_{\v{\iota}_k}}$ based on
$\set{\v{\iota}_k}$, at arbitrary points; see \sref{frame-basis}.

It is worth noting that the \texttt{strategy}, \texttt{grid}, and \texttt{basis}
objects conform to certain interfaces and can be easily swapped out. This makes
the two algorithms very general and reusable with different configurations. In
particular, the adaptation strategy can be fine-tuned for each particular
problem.

\conclusioncut
To recapitulate, in this section, the approximation engine of our framework for
probabilistic analysis of electronic systems under workload variation has been
presented. It is consolidated in \aref{frame-construction} and
\aref{frame-evaluation}.
