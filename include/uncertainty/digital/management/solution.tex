\inputfigure{network-overview}
Our workflow is illustrated in \fref{network-overview}. First, we note that the
available data generally need to be processed prior to learning since they are
likely to be given in a format that is not efficient or convenient for the
subsequent calculations. With this in mind, our foremost task is to extract $G$
in \eref{network-dataset} from given raw data. This processing part can be seen
at the top of \fref{network-overview} and is explained in \sref{network-data}.
Next, the resulting profiles are used in order to obtain an adequately trained
predictive model. The modeling part is covered in \sref{network-model} while the
learning part is explained in \sref{network-learning}; the latter can also be
seen at the bottom of \fref{network-overview}. The above operations are to be
undertaken offline while the trained model is supposed to be used by the
resource manager at runtime in order to make predictions and, thereby, account
for workload uncertainty.

\subsection{Data Pipeline}
\slab{network-data}

In this subsection, we describe our pipeline for working with large data sets
that makes the data readily accessible for machine-learning undertakings. In
order to make the exposition clearer, the pipeline is described by applying it
to a real-life data set of resource-usage traces collected in a large computer
cluster. To this end, we begin by giving a brief introduction to this data set.

The data set that we work with is the Google cluster-usage traces
\cite{reiss2011}. The traces were collected over 29 days in May 2011 and
encompass more than 12 thousand machines serving requests from more than 900
users. In the data set's parlance, a user request is a job; a job comprises one
or several tasks; and a task is a Linux program to be run on a single machine.
Each job is ascribed a unique \up{ID}, and each task is given an \up{ID} that is
unique in the scope of the corresponding job. Apart from other tables, the data
set contains a table that records the resource usage of individual tasks with
the granularity of five minutes. Each record corresponds to a specific task and
a specific five-minute interval, and it provides such measurements as the
average and maximum values of the \up{CPU}, memory, and disk usage. There are
more than 1.2 billion records in the table in question, which correspond to more
than 24 million tasks or, equivalently, resource-usage profiles and to more than
670 thousand jobs.

The resource-usage table is provided in the form of 500 archives. Each archive
contains a single file with measurements over a certain time window. Such a
format is inconvenient and inefficient to work with, which is addressed in what
follows. Consider now the three topmost boxes in \fref{network-overview}.

As the first step (the leftmost orange box in \fref{network-overview}), the data
from the 500 archives are distributed into separate databases so that each such
database contains all the data points that belong to a particular job, resulting
in as many databases as there are jobs. In order to reduce the disk-space
requirements, only the used columns of the table are preserved. In our
experiments, these columns are the start time stamp, job \up{ID}, task \up{ID},
and average \up{CPU} usage.

As the second step (the middle orange box in \fref{network-overview}), an index
of the profiles is created in order to be able to efficiently navigate the
catalog of the databases created at the previous step. Each record in the index
contains metadata about a single task, the most important of which are the task
\up{ID} and the path to the corresponding database. We also include the length
of the profile into the index in order to be able to efficiently filter the
profiles by length.

As the last step (the rightmost orange box in \fref{network-overview}), a subset
of the resource-usage profiles is selected using the index according to the
needs of a particular training session (to be discussed in
\sref{network-results}) and then stored on disk. Concretely, the profiles are
fetched from the databases and stored in the native format of the
machine-learning library utilized; we refer to a file in such a format as a
binary file. Instead of writing all the selected profiles into one binary file,
we distribute them across several files. Such a catalog of binary files is
created for each of the three commonly considered parts \cite{hastie2013} of the
data at hand: one is for training, one is for validation, and one is for
testing. We denote these parts by $G_1$, $G_2$, and $G_3$, respectively; see
also \fref{network-overview}.

Lastly, it is common practice to standardize data before feeding them into a
learning algorithm \cite{hastie2013}. In our case, it is done along with
creating the aforementioned three catalogs, which requires a separate pass over
the training set.

In conclusion, the benefit of the data pipeline describe above is in the
streamlined access to the data during one or multiple training sessions. The
binary files can be read efficiently as many times as needed, and they can be
straightforwardly regenerated when the selection criteria change. It is worth
noting that the artifacts of the procedures undertaken at the grouping and
indexing steps stay the same. The presence of multiple binary files also allows
for shuffling the training data at the beginning of each training epoch.

\subsection{Predictive Model}
\slab{network-model}

As emphasized in \sref{network-introduction} and \sref{network-problem}, our
goal is to assess the applicability of the latest advancements in neural
networks \cite{goodfellow2016} to fine-grained long-range prediction of the
resource usage in computer systems. The architectures of neural networks are
very diverse. However, since the data that we study are inherently sequential,
it is natural to found our model on the basis of recurrent neural networks
\cite{goodfellow2016}, which are designed for such cases.

\inputfigure{network-model}
A schematic representation of our model can be seen in \fref{network-model}.
Note that many of the actual connections between the model's parts are
simplified or not shown at all in order to make the figure legible. In what
follows, we describe each part of the model in detail. A number of important
operational aspects of the model are to be covered in the next subsection,
\sref{network-learning}.

The input to the model is a single \ng-dimensional data point, which can be seen
on the left-hand side of \fref{network-model}. Similarly, the output is a single
\ng-dimensional data point, which is depicted on the right-hand side of
\fref{network-model}. The input $\vg_{ij}$ is the value of the resource usage of
an individual task at step $j$, and the output $\hat{\vg}_{i,j + 1}$ is a
one-step-ahead prediction of this usage.

The core of the model is formed by a number of recurrent layers, which are also
called cells. They are shown as a group of blue boxes in \fref{network-model}.
The network can be made as many layers deep as needed. Each cell is composed of
a number of units, which are depicted as double circles in \fref{network-model}.
The number of cells and units per cell are denoted by \nc and \nu, respectively.

A unit is the smallest processing elements. The key characteristics of a unit
are that it has internal memory, and that it has access to its previous output,
which makes it recurrent. There are different types of units; each one defines
how a unit lets data flow through it and updates its memory. One notable unit is
the \ac{LSTM} unit \cite{hochreiter1997}. It has been designed to overcome the
problems of traditional recurrent networks---such as vanishing gradient when
training---and it is now one of the most widely used types. The recurrent layers
of our model are \ac{LSTM} cells, cells composed of \ac{LSTM} units.

In addition, each cell is enhanced by a dropout mechanism \cite{zaremba2014},
which gives control over the regularization of the model and is to prevent
potential overfitting \cite{hastie2013}. The mechanism is active only during
training. For future reference, denote by $\rho_\drop$ the probability of
dropping an output of a cell.

The output of the last cell is typically a large tensor, which is proportional
in size to the number of units in the cell. Each entry of such a tensor can be
considered as a feature that the network has extracted and activated in
accordance with the profile that is currently being fed into the model. The task
now is to combine these features in order to produce a single prediction. To
this end, we mount a regression layer on top of the last cell, which is depicted
by an orange box in \fref{network-model}. Unlike the recurrent layers, which
feature highly nonlinear transformations, this layer performs an affine
transformation.

To summarize, we have described a predictive model that is composed of a number
of \ac{LSTM} cells and a linear regression layer. Due to its internal memory,
the presented model is capable of efficiently taking into account the entire
past of the resource-usage profile under consideration when making predictions.
Let us now discuss how the model is supposed to be used.

\subsection{Learning Pipeline}
\slab{network-learning}

Recall that the output of the data pipeline described in \sref{network-data} is
the data set $G$, which is split into three parts: $G_1$ is for training, $G_2$
is for validation, and $G_3$ is for testing. We now elaborate on the operations
that take place during these three steps, which are also displayed at the bottom
of \fref{network-overview}.

Let us discuss training first. The model depicted in \sref{network-model} has a
large number of parameters that have to be learned; they are primarily various
weights and biases inside the layers. For this purpose, $G_1$ is utilized. The
training is undertaken via backpropagation through time using stochastic
gradient descent \cite{goodfellow2016} whose objective is to minimize a certain
loss function, which we shall specify shortly. There are two aspects that need
to be discussed first.

\inputfigure{network-feed}
The first concerns the way a single profile is fed into the model. To begin
with, the internal memory of the model is nullified. Next, note that each
profile---which can be seen in \eref{network-profile}---has multiple data points
($\nsi{i} > 1$), and that two profiles generally have different lengths
($\nsi{i} \neq \nsi{j}$) since the execution times of two tasks are likely to
differ. With this in mind, all the points of a profile are fed in one pass using
a technique called dynamic unrolling. An illustration is given in
\fref{network-feed}, in which the schematic representation in
\fref{network-model} has been simplified even further and rotated 90${}^\circ$
counterclockwise. It can be seen that the model has been replicated as many
times as there are data points in the profile. However, it is still the same
model, and all the replicas share the same parameters and the same internal
memory. It can also be seen in \fref{network-feed} how information flows from
one time step to the next, which implicitly gives the model access to the whole
history at each time step.

Now, it is not efficient to feed only one profile at a time due to the
inevitable overhead imposed by the computations involved. These computations
should be performed in batches when possible, that is, \nb profiles should be
fed simultaneously where \nb is referred to as the batch size. Since $\nsi{i}
\neq \nsi{j}$ in general, it is not possible to stack multiple arbitrary
profiles into a single tensor directly. In order to circumvent this problem, we
reside to bucketing. Specifically, each profile is put into one of many queues
depending on its length. When a queue that is accumulating profiles of length
from some \nsi{l} to \nsi{r} has collected \nb profiles, it pads the profiles
that are shorter than \nsi{r} with zeros and emits a tensor of size $\nb \times
\nsi{r} \times \ng$ to be further consumed by the model.

The loss function that we minimize during training is the \ac{MSE} of one-step
predictions over the whole batch. The correct prediction for the very last time
step, which goes beyond the time window of the profiles in question, is assumed to
be zero. For instance, $\hat{\vg}_{i, \nsi{i} + 1}$ in \fref{network-feed} has
no $\vg_{i, \nsi{i} + 1}$ to be compared with, and this $\vg_{i, \nsi{i} + 1}$
is assumed to be zero.

Let us now discuss validation, which corresponds to the middle blue box in
\fref{network-overview}. As it is the case with arguably any nontrivial model,
the one presented in \sref{network-model} has a number of hyperparameters.
Examples include the number of cells \nc, number of units per cell \nu, and
probability of dropout $\rho_\drop$. Unlike ordinary parameters---which are to
be optimized during training as discussed earlier---hyperparameters are to be
set prior to training and kept unchanged thereafter. From the examples given, it
is apparent that the impact of hyperparameters is profound; they should be
tuned with great care.

The validation set $G_2$ is used to assess the performance of the model trained
(using $G_1$ as usual) under different configurations of the model's
hyperparameters. Similar to training, the error metric utilized in this case is
the \ac{MSE}, and it is beneficial to perform these computations in batches. The
trained model that has the best performance on $G_2$ is then chosen as the one
to use.

Despite all the techniques employed to speed up training, it is still a
time-consuming operation. This means that a brute-force search in the space of
hyperparameters for the best configuration is impractical; a certain intelligent
strategy should be followed. In our workflow, we use the Hyperband algorithm
\cite{li2016}. Instead of adaptively choosing new configurations to
evaluate---which is the case with many algorithms of this kind---it adaptively
allocates resources to configurations chosen at random, which has been shown to
be a very efficient strategy. In particular, the algorithm allows one for saving
a lot of compute power, which otherwise would be burnt in vain evaluating
overtly inadequate configurations of hyperparameters. In this context,
\emph{resources} refers a user-defined measure of how extensively a
configuration is exercised. For instance, it can be the amount of wall-clock
time spent or the number of training steps taken; in our experiments in
\sref{network-results}, we use the latter.

Let us now turn to testing; see the rightmost blue box in
\fref{network-overview}. After a trained model has been selected during the
validation step, it has to be assessed anew \cite{hastie2013}: one cannot aver
that the error with respect to $G_2$ is a good estimate of the generalization
error due to the selection bias---we have deliberately chosen the configuration
that has the best performance on $G_2$.

In order to have an unbiased evaluation, the testing set $G_3$ is utilized.
Similar to training and validation, the \ac{MSE} is considered as a measure of
quality, and bucketing is utilized. However, unlike training and validation, the
error is calculated in a more elaborate way. To elaborate, recall first that our
objective is making long-range predictions of resource usage; see
\sref{network-problem}. Note also that, in training and validation, we are only
concerned with what happens one time step ahead. The reason is that we would
like to have as high of a throughput as possible during training and validation
since they are to be performed many times. Testing, on the other hand, is done
only once, and it is during testing we make and subsequently assess
multiple-step-ahead predictions.

In order to calculate long-rage predictions, we use refeeding: at time step $j$,
the predicted value $\hat{\vg}_{i,j + 1}$ of the resource usage of task $i$ is
fed into the model as if it was the actual resource usage $\vg_{i,j + 1}$ at
step $j + 1$, which is not yet known at step $j$. At this point, it might be
helpful to consider the example given in \fref{network-example}. The process
continues until all the desired $h$ future values are estimated. It is natural
to expect that the more accurate the one-step-ahead prediction is, the more
accurate the multiple-step-ahead one becomes.

There is more to testing. Consider now how a trained model might be used in
practice. Potentially at each step $j$, the resource manager might want to
predict the next $h$ values of the resource usage of task $i$, that is,
$\range{\hat{\vg}_{i,j + 1}}{\hat{\vg}_{i,j + h}}$. Therefore, in order to test
the model properly, we have to sweep over all the time steps of the profiles in
question while making $h$ predictions at each step. An important aspect to note
is that the state of the model's memory should be saved before computing
$\range{\hat{\vg}_{i,j + 1}}{\hat{\vg}_{i,j + h}}$ at step $j$ and restored
before feeding $\vg_{i,j + 1}$ in order to advance to step $j + 1$. The memory
becomes contaminated when one feeds predictions instead of observations into the
model.

At this point, the main aspects of our workflow, which is illustrated in
\fref{network-overview}, have been discussed. The output of the workflow is a predictive
model that has been trained on $G_1$, validated on $G_2$, and tested on $G_3$.
