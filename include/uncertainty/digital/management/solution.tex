\inputfigure{network-overview}
Our workflow is illustrated in \fref{network-overview}. First, we note that the
available data generally need to be processed prior to learning since they are
likely to be given in a format that is not efficient or convenient for the
subsequent calculations. With this in mind, our foremost task is to extract $G$
in \eref{network-dataset} from given raw data. This processing part can be seen
at the top of \fref{network-overview} and is explained in \sref{network-data}.
Next, the resulting profiles are used in order to obtain an adequately trained
predictive model. The modeling part is covered in \sref{network-model} while the
learning part is explained in \sref{network-learning}; the latter can also be
seen at the bottom of \fref{network-overview}. The above operations are to be
undertaken offline while the trained model is supposed to be used by the
resource manager at runtime in order to make predictions and, thereby, account
for workload uncertainty.

\subsection{Data Pipeline}
\slab{network-data}

In this subsection, we describe our pipeline for working with large data sets
that makes the data readily accessible for machine-learning undertakings. In
order to make the exposition clearer, the pipeline is described by applying it
to a real-life data set of resource-usage traces collected in a large computer
cluster. To this end, we begin by giving a brief introduction to this data set.

The data set that we work with is the Google cluster-usage traces
\cite{reiss2011}. The traces were collected over 29 days in May 2011 and
encompass more than 12 thousand machines serving requests from more than 900
users. In the data set's parlance, a user request is a job; a job comprises one
or several tasks; and a task is a Linux program to be run on a single machine.
Each job is ascribed a unique \up{ID}, and each task is given an \up{ID} that is
unique in the scope of the corresponding job. Apart from other tables, the data
set contains a table that records the resource usage of individual tasks with
the granularity of five minutes. Each record corresponds to a specific task and
a specific five-minute interval, and it provides such measurements as the
average and maximum values of the \up{CPU}, memory, and disk usage. There are
more than 1.2 billion records in the table in question, which correspond to more
than 24 million tasks or, equivalently, resource-usage profiles and to more than
670 thousand jobs.

The resource-usage table is provided in the form of 500 archives. Each archive
contains a single file with measurements over a certain time window. Such a
format is inconvenient and inefficient to work with, which is addressed in what
follows. Consider now the three topmost boxes in \fref{network-overview}.

As the first step (the leftmost orange box in \fref{network-overview}), the data
from the 500 archives are distributed into separate databases so that each such
database contains all the data points that belong to a particular job, resulting
in as many databases as there are jobs. In order to reduce the disk-space
requirements, only the used columns of the table are preserved. In our
experiments, these columns are the start time stamp, job \up{ID}, task \up{ID},
and average \up{CPU} usage.

As the second step (the middle orange box in \fref{network-overview}), an index
of the profiles is created in order to be able to efficiently navigate the
catalog of the databases created at the previous step. Each record in the index
contains metadata about a single task, the most important of which are the task
\up{ID} and the path to the corresponding database. We also include the length
of the profile into the index in order to be able to efficiently filter the
profiles by length.

As the last step (the rightmost orange box in \fref{network-overview}), a subset
of the resource-usage profiles is selected using the index according to the
needs of a particular training session (to be discussed in
\sref{network-results}) and then stored on disk. Concretely, the profiles are
fetched from the databases and stored in the native format of the
machine-learning library utilized; we refer to a file in such a format as a
binary file. Instead of writing all the selected profiles into one binary file,
we distribute them across several files. Such a catalog of binary files is
created for each of the three commonly considered parts \cite{hastie2013} of the
data at hand: one is for training, one for validation, and one for testing; see
also \fref{network-overview}. We denote these parts by $G_1$, $G_2$, and $G_3$,
respectively.

Lastly, it is common practice to standardize data before feeding them into a
learning algorithm \cite{hastie2013}. In our case, it is done along with
creating the aforementioned three catalogs, which requires a separate pass over
the training set.

In conclusion, the benefit of the data pipeline describe above is in the
streamlined access to the data during one or multiple training sessions. The
binary files can be read efficiently as many times as needed, and they can be
straightforwardly regenerated when the selection criteria change. It is worth
noting that the artifacts of the procedures undertaken at the grouping and
indexing steps stay the same. The presence of multiple binary files allows also
for shuffling the training data at the beginning of each training epoch.

\subsection{Predictive Model}
\slab{network-model}

As emphasized in \sref{network-introduction} and \sref{network-problem}, our
goal is to assess the applicability of the latest advancements in neural
networks \cite{goodfellow2016} to fine-grained long-range prediction of the
resource usage in computer systems. The architectures of neural networks are
very diverse. However, since the data that we study are inherently sequential,
it is natural to found our model on the basis of recurrent neural networks
\cite{goodfellow2016}, which are designed for such cases.

\inputfigure{network-model}
A schematic representation of our model can be seen in \fref{network-model}.
Note that many of the actual connections between the model's parts are
simplified or not shown at all in order to make the figure legible. In what
follows, we describe each part of the model in detail. A number of important
operational aspects of the model are to be covered in the next subsection,
\sref{network-learning}.

The input to the model is a single \ng-dimensional data point, which can be seen
on the left-hand side of \fref{network-model}. Similarly, the output is a single
\ng-dimensional data point, which is depicted on the right-hand side of
\fref{network-model}. The input $\vg_{ij}$ is the value of the resource usage of
an individual task at step $j$, and the output $\hat{\vg}_{i,j + 1}$ is a
one-step-ahead prediction of this usage.

The core of the model is formed by a number of recurrent layers, which are also
called cells. They are shown as a group of blue boxes in \fref{network-model}.
The network can be made as many layers deep as needed. Each cell is composed of
a number of units, which are depicted as double circles in \fref{network-model}.
The number of cells and units per cell are denoted by \nc and \nu, respectively.

A unit is the smallest processing elements. The key characteristics of a unit
are that it has internal memory, and that it has access to its previous output,
which makes it recurrent. There are different types of units; each one defines
how a unit lets data flow through it and updates its memory. One notable unit is
the \ac{LSTM} unit \cite{hochreiter1997}. It has been designed to overcome the
problems of traditional recurrent networks---such as vanishing gradient when
training---and it is now one of the most widely used types. The recurrent layers
of our model are \ac{LSTM} cells, cells composed of \ac{LSTM} units.

In addition, each cell is enhanced by a dropout mechanism \cite{zaremba2014},
which gives control over the regularization of the model and is to prevent
potential overfitting \cite{hastie2013}. The mechanism is active only during
training. For future reference, denote by $\rho_\drop$ the probability of
dropping an output of a cell.

The output of the last cell is typically a large tensor, which is proportional
in size to the number of units in the cell. Each entry of such a tensor can be
considered as a feature that the network has extracted and activated in
accordance with the profile that is currently being fed into the model. The task
now is to combine these features in order to produce a single prediction. To
this end, we mount a regression layer on top of the last cell, which is depicted
by an orange box in \fref{network-model}. Unlike the recurrent layers, which
feature highly nonlinear transformations, this layer performs an affine
transformation.

To summarize, we have described a predictive model that is composed of a number
of \ac{LSTM} cells and a linear regression layer. Due to its internal memory,
the presented model is capable of efficiently taking into account the entire
past of the resource-usage profile under consideration when making predictions.
Let us now discuss how the model is supposed to be used.

\subsection{Learning Pipeline}
\slab{network-learning}

As mentioned in \sref{introduction} and \sref{problem}, a part of our goal is to
assess the applicability of the latest advancements in neural networks
\cite{goodfellow2016} to modeling and prediction of fine-grained resource-usage
data. The architectures of neural networks are very diverse: one network can be
nothing like another. Since the data that we study are inherently sequential, it
is natural to found our model on the basis of recurrent neural networks
\cite{goodfellow2016}, which are specifically designed for such cases.

A schematic representation of our model can be seen in \fref{model}; many of the
actual connections between the model's parts are simplified or not shown at all
in order to make the figure legible. In the following subsections, we shall
describe each part in detail. A number of important operational aspects of the
model will be covered in the next section, \sref{learning}.

The input to the model is a single $d$-dimensional data point, which can be seen
on the left-hand side of \fref{model}. Similarly, the output is a single
$d$-dimensional data point, which is depicted on the right-hand side of
\fref{model}. The input $x_{ik}$ is the value of the resource usage of an
individual task at step $k$, and the output $\hat{x}_{i,k + 1}$ is a
one-step-ahead prediction of the usage.

The core of the model is formed by a number of recurrent layers, which are also
called cells. They are shown as a group of blue boxes in \fref{model}. The
network can be made as many layers deep as needed. Each cell is composed of a
number of units, which are depicted by double circles in \fref{model}. We let
$c$ and $u$ be the number of cells and units per cell, respectively.

A unit is the smallest processing elements. The key characteristics of a unit
are that it has internal memory, and that it has access to its previous output,
which makes it recurrent. There are different types of units; each one defines
how a unit lets data flow through it and updates its memory. One notable type is
called \up{LSTM} \cite{hochreiter1997}, which stands for \emph{long short-term
memory}. It has been designed to overcome the problems of traditional recurrent
networks---such as vanishing gradient when training---and it is now one of the
most widely used types. The recurrent layers of our model are \up{LSTM} cells.

In addition, during training, each cell is enhanced by a dropout mechanism
\cite{zaremba2014}, which gives control over the regularization of the model and
is to prevent potential overfitting \cite{hastie2013}. We let $p$ be the
probability of dropping an output of a cell.

The output of the last cell is typically a large tensor, which is proportional
in size to the number of units in the cell. Each entry of such a tensor can be
considered as a feature that the network has extracted and activated in
accordance with the trace that is currently being fed into the model. The task
now is to combine these features in order to produce a single prediction. To
this end, we mount a regression layer on top of the last cell, which is depicted
by an orange box in \fref{model}. Unlike the recurrent layers in
\sref{recurrent}, which feature highly nonlinear transformations, this layer
performs an affine transformation.

To summarize, we have described a predictive model that is composed of a number
of \up{LSTM} cells and a linear regression layer. Due to its internal memory,
the model is capable of efficiently taking into account the entire past of the
resource-usage trace under consideration when making predictions. Let us now
discuss how the model is supposed to be used.
