\inputfigure{network-overview}
Our workflow is illustrated in \fref{network-overview}. First, we note that the
available data generally need to be processed prior to learning since they are
likely to be given in a format that is not efficient or convenient for the
subsequent calculations. With this in mind, our foremost task is to extract $G$
in \eref{network-dataset} from given raw data. This processing part can be seen
at the top of \fref{network-overview} and is explained in \sref{network-data}.
Next, the resulting profiles are used in order to obtain an adequately trained
predictive model. The modeling part is covered in \sref{network-model} while the
learning part is explained in \sref{network-learning}; the latter can also be
seen at the bottom of \fref{network-overview}. The above operations are to be
undertaken offline while the trained model is supposed to be used by the
resource manager at runtime in order to make predictions and, thereby, account
for workload uncertainty.

\subsection{Data Pipeline}
\slab{network-data}

In this subsection, we describe our pipeline for working with large data sets
that makes the data readily accessible for machine-learning undertakings. In
order to make the exposition clearer, the pipeline is described by applying it
to a real-life data set of resource-usage traces collected in a large computer
cluster. To this end, we begin by giving a brief introduction to this data set.

The data set that we work with is the Google cluster-usage traces
\cite{reiss2011}. The traces were collected over 29 days in May 2011 and
encompass more than 12 thousand machines serving requests from more than 900
users. In the data set's parlance, a user request is a job; a job comprises one
or several tasks; and a task is a Linux program to be run on a single machine.
Each job is ascribed a unique \up{ID}, and each task is given an \up{ID} that is
unique in the scope of the corresponding job. Apart from other tables, the data
set contains a table that records the resource usage of individual tasks with
the granularity of five minutes. Each record corresponds to a specific task and
a specific five-minute interval, and it provides such measurements as the
average and maximum values of the \up{CPU}, memory, and disk usage. There are
more than 1.2 billion records in the table in question, which correspond to more
than 24 million tasks or, equivalently, resource-usage profiles and to more than
670 thousand jobs.

The resource-usage table is provided in the form of 500 archives. Each archive
contains a single file with measurements over a certain time window. Such a
format is inconvenient and inefficient to work with, which is addressed in what
follows. Consider now the three topmost boxes in \fref{network-overview}.

As the first step (the leftmost orange box in \fref{network-overview}), the data
from the 500 archives are distributed into separate databases so that each such
database contains all the data points that belong to a particular job, resulting
in as many databases as there are jobs. In order to reduce the disk-space
requirements, only the used columns of the table are preserved. In our
experiments, these columns are the start time stamp, job \up{ID}, task \up{ID},
and average \up{CPU} usage.

As the second step (the middle orange box in \fref{network-overview}), an index
of the profiles is created in order to be able to efficiently navigate the
catalog of the databases created at the previous step. Each record in the index
contains metadata about a single task, the most important of which are the task
\up{ID} and the path to the corresponding database. We also include the length
of the profile into the index in order to be able to efficiently filter the
profiles by length.

As the last step (the rightmost orange box in \fref{network-overview}), a subset
of the resource-usage profiles is selected using the index according to the
needs of a particular training session (to be discussed in
\sref{network-results}) and then stored on disk. Concretely, the profiles are
fetched from the databases and stored in the native format of the
machine-learning library utilized; we refer to a file in such a format as a
binary file. Instead of writing all the selected profiles into one binary file,
we distribute them across several files. Such a catalog of binary files is
created for each of the three commonly considered parts \cite{hastie2013} of the
data at hand: one is for training, one for validation, and one for testing; see
also \fref{network-overview}. We denote these parts by $G_1$, $G_2$, and $G_3$,
respectively.

Lastly, it is common practice to standardize data before feeding them into a
learning algorithm \cite{hastie2013}. In our case, it is done along with
creating the aforementioned three catalogs, which requires a separate pass over
the training set.

In conclusion, the benefit of the data pipeline describe above is in the
streamlined access to the data during one or multiple training sessions. The
binary files can be read efficiently as many times as needed, and they can be
straightforwardly regenerated when the selection criteria change. It is worth
noting that the artifacts of the procedures undertaken at the grouping and
indexing steps stay the same. The presence of multiple binary files allows also
for shuffling the training data at the beginning of each training epoch.

\subsection{Predictive Model}
\slab{network-model}

As emphasized in \sref{network-introduction} and \sref{network-problem}, our
goal is to assess the applicability of the latest advancements in neural
networks \cite{goodfellow2016} to fine-grained long-range prediction of the
resource usage in computer systems. The architectures of neural networks are
very diverse. However, since the data that we study are inherently sequential,
it is natural to found our model on the basis of recurrent neural networks
\cite{goodfellow2016}, which are designed for such cases.

\inputfigure{network-model}
A schematic representation of our model can be seen in \fref{network-model}.
Note that many of the actual connections between the model's parts are
simplified or not shown at all in order to make the figure legible. In what
follows, we describe each part of the model in detail. A number of important
operational aspects of the model are to be covered in the next subsection,
\sref{network-learning}.

The input to the model is a single \ng-dimensional data point, which can be seen
on the left-hand side of \fref{network-model}. Similarly, the output is a single
\ng-dimensional data point, which is depicted on the right-hand side of
\fref{network-model}. The input $\vg_{ij}$ is the value of the resource usage of
an individual task at step $j$, and the output $\hat{\vg}_{i,j + 1}$ is a
one-step-ahead prediction of this usage.

The core of the model is formed by a number of recurrent layers, which are also
called cells. They are shown as a group of blue boxes in \fref{network-model}.
The network can be made as many layers deep as needed. Each cell is composed of
a number of units, which are depicted as double circles in \fref{network-model}.
The number of cells and units per cell are denoted by \nc and \nu, respectively.

A unit is the smallest processing elements. The key characteristics of a unit
are that it has internal memory, and that it has access to its previous output,
which makes it recurrent. There are different types of units; each one defines
how a unit lets data flow through it and updates its memory. One notable unit is
the \ac{LSTM} unit \cite{hochreiter1997}. It has been designed to overcome the
problems of traditional recurrent networks---such as vanishing gradient when
training---and it is now one of the most widely used types. The recurrent layers
of our model are \ac{LSTM} cells, cells composed of \ac{LSTM} units.

In addition, each cell is enhanced by a dropout mechanism \cite{zaremba2014},
which gives control over the regularization of the model and is to prevent
potential overfitting \cite{hastie2013}. The mechanism is active only during
training. For future reference, denote by $\rho_\drop$ the probability of
dropping an output of a cell.

The output of the last cell is typically a large tensor, which is proportional
in size to the number of units in the cell. Each entry of such a tensor can be
considered as a feature that the network has extracted and activated in
accordance with the profile that is currently being fed into the model. The task
now is to combine these features in order to produce a single prediction. To
this end, we mount a regression layer on top of the last cell, which is depicted
by an orange box in \fref{network-model}. Unlike the recurrent layers, which
feature highly nonlinear transformations, this layer performs an affine
transformation.

To summarize, we have described a predictive model that is composed of a number
of \ac{LSTM} cells and a linear regression layer. Due to its internal memory,
the presented model is capable of efficiently taking into account the entire
past of the resource-usage profile under consideration when making predictions.
Let us now discuss how the model is supposed to be used.

\subsection{Learning Pipeline}
\slab{network-learning}

The output of the data-processing pipeline in \sref{data} is the data set $X$,
which is split into three parts: $X_1$ is for the training stage, $X_2$ for the
validation stage, and $X_3$ for the testing stage. We now elaborate on the
operations that take place during these three stages, which are also displayed
in \fref{workflow}.

The model in \sref{model} has a large number of parameters that have to be
learned during training; they are primarily various weights and biases inside
the layers. For this purpose, $X_1$ is utilized. The training is undertaken via
backpropagation through time using stochastic gradient descent
\cite{goodfellow2016} whose objective is to minimize a certain loss function,
which we shall specify shortly. There are two aspects to be discussed first.

The first concerns the way a single resource-usage trace is fed into the model.
To begin with, the internal memory is nullified before feeding a trace. Next,
note that a trace, as in \eref{trace}, has multiple data points ($l_i > 1$), and
that two traces are likely to have different lengths ($l_i \neq l_j$) since the
execution times of two tasks are likely to differ. With this in mind, all the
points of a trace are fed in one pass using a technique called dynamic
unrolling. An illustration for $l_i = 4$ is given in \fref{unroll}, in which the
representation in \fref{model} has been simplified even further and rotated
$90^\circ$ counterclockwise. It can be seen that the model has been replicated
as many times as there are data points in the trace. However, it is still the
same model, and all the replicas share the same parameters and internal memory.
It can also be seen in \fref{unroll} how information flows from one time step to
the next, which is what makes the model recurrent.

Now, it is not efficient to feed only one trace at a time due to the inevitable
overhead imposed by the computations involved. Therefore, these computations
should be performed in batches whenever possible. Since $l_i \neq l_j$ in
general, it is not possible to stack multiple arbitrary traces into a single
tensor directly. In order to circumvent this problem, we reside to bucketing.
Specifically, each read trace is put into one of many queues depending on its
length. When a queue, accumulating traces of length from some $l'$ to $l''$, has
collected the desired number of traces---denoted by $b$ and referred to as the
batch size---it pads traces shorter than $l''$ with zeros and emits a tensor of
size $b \times l'' \times d$ to be further consumed by the model.

The loss function that we minimize during training is the mean squared error
(\up{MSE}) \cite{hastie2009} of one-step predictions over the whole batch. The
correct prediction for the very last time step, which goes beyond the time
window of the traces in question, is assumed to be zero. For instance, in
\fref{unroll}, $\hat{x}_{i4}$ has no $x_{i4}$ to be compared with; $x_{i4}$ is
assumed to be zero.

As it is the case with arguably all nontrivial machine-learning models, the one
presented in \sref{model} has a number of hyperparameters. They include the
number of cells $c$, number of units per cell $u$, and probability of dropout
$p$, which are introduced in \sref{recurrent}. Unlike ordinary parameters, which
are to be optimized during training (see \sref{training}), the hyperparameters
are to be set prior to training and kept unchanged thereafter. The impact of the
hyperparameters is profound; therefore, they should be carefully selected.

The validation set $X_2$ is used to assess the performance of the model trained
(using $X_1$ as usual) under different configurations of the hyperparameters of
the model. As before, the error metric utilized is the \up{MSE}, and it is
beneficial to perform these computations in batches. The trained model that has
the best performance on $X_2$ is then chosen as the one to be used.

Despite all the techniques employed to speed up training, it is still a
time-consuming operation. As a result, brute-force search in the space of
hyperparameters for the best configuration is impractical; a certain intelligent
strategy should be followed.

In our workflow, we use the Hyperband algorithm \cite{li2016}. Instead of
adaptively choosing new configurations to evaluate---which is the case with many
algorithms of this kind---it adaptively allocates resources to configurations
chosen at random, which has been shown to be a very efficient strategy. In
particular, the algorithm allows one for saving a lot of compute power, which
otherwise would be burnt in vain evaluating overtly inadequate configurations of
hyperparameters. In this context, \emph{resources} refers a user-defined measure
of how extensively a configuration is exercised. For instance, it can be the
amount of wall-clock time spent or the number of training steps taken; in our
experiments in \sref{result}, we use the latter.

After a trained model has been selected during the validation stage, it has to
be assessed anew \cite{hastie2009}: one cannot aver that the error with respect
to $X_2$ is a good estimate of the generalization error because the selection
was biased (we deliberately chose the configuration that had the best
performance on $X_2$).

In order to have an unbiased evaluation, the testing set $X_3$ is utilized. As
it is with training and validation, the \up{MSE} is considered as a quality
metric, and the bucketing mechanism is used (see \sref{training}). However,
unlike training and validation, the error is calculated in a more elaborate way
as follows.

Recall first that our objective is making long-range predictions of resource
usage (see \sref{problem}). Note also that, in \sref{training} and
\sref{validation}, we are only concerned with what happens one time step ahead.
The reason is that we would like to have as high throughput as possible since
the training and validation operations are to be performed many times. Testing,
on the other hand, is done only once, and it is during testing we make and
assess multiple-step-ahead predictions.

In order to compute long-rage predictions, we use refeeding: at time step $k$,
the predicted value $\hat{x}_{i,k + 1}$ is fed into the model as if it was the
actual resource usage $x_{i,k + 1}$ at step $k + 1$, which is not yet known at
step $k$. It might be helpful to consider the example given in \fref{example}.
The process continues until all the desired $h$ future values are estimated. It
is natural to expect that the more accurate the one-step-ahead prediction is,
the more accurate the multiple-step-ahead one will be.

There is more to it. Consider how a trained model will be used in practice.
Potentially at each step $k$, one might want to predict the next $h$ values of
the resource usage of task $i$, that is, $\hat{x}_{i,k + 1}, \dots, \hat{x}_{i,k
+ h}$. Therefore, in order to test the model properly, we have to sweep over all
the time steps of the trace in question while making $h$ predictions at each
step. An important aspect to note here is that the state of the model's memory
should be saved before computing $\hat{x}_{i,k + 1}, \dots, \hat{x}_{i,k + h}$
at step $k$ and restored before feeding $x_{i,k + 1}$ in order to advance to
step $k + 1$. The memory becomes contaminated when one feeds predictions instead
of observations into the model.

At this point, the main aspects of our workflow, which is illustrated in
\fref{workflow}, have been discussed. The output of the workflow is a predictive
model that has been trained on $X_1$, validated on $X_2$, and tested on $X_3$.
