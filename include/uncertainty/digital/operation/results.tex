In this section, we evaluate the performance of our approach to mitigating
workload uncertainty at runtime, which is the tool for making fine-grained
long-range predictions of the resource usage in a computer system described in
\sref{brain-solution}. All the experiments are conducted on a \up{GNU}/Linux
machine equipped with 8 processors Intel Core i7-3770 3.4~\up{GH}z, 24~\up{GB}
of \up{RAM}, and an \up{HDD} of 500~\up{GB}. The machine has no modern
\up{GPU}s; therefore, there is an immense room for improvement. All the source
code, configuration files, and input data used in the experiments are available
online at \cite{eslab2017b}.

\hiddensubsection{Data Pipeline}

Recall that the considered data set is the Google cluster-usage traces
\cite{reiss2011} introduced in \sref{brain-data}. Without loss of generality, we
focus on the consumption of one particular resource, namely, \up{CPU}; thus,
$\ng = 1$ in \eref{brain-profile}. In this regard, the data set provides two
apposite pieces of information: the average and maximum \up{CPU} usage over
five-minute intervals; we extract the former.

The grouping and indexing steps of our data pipeline---described in
\sref{brain-data} and depicted in \fref{brain-overview}---take approximately 57
hours to finish (no parallelism). Since they have to be done only once, their
computational cost can be safely considered negligible. Regarding the selection
step, we filter those resource-usage profiles that contain 5--50 data points;
hence, $\nsi{i} \in [5, 50]$ in \eref{brain-profile}. Such profiles constitute
around 74\% of the total number of profiles (around 18 out of 24 million). We
experiment with a random subset of two million profiles, which is around 11\% of
the 5--50 resource-usage profiles; hence, $\no = 2 \times 10^6$ in
\eref{brain-dataset}. The data sets $G_1$, $G_2$, and $G_3$ constitute 70\%,
10\%, and 20\% of $G$, respectively. Fetching and storing on disk these many
profiles take around four hours. Recall that this operation has to be repeated
only when the selection criteria change, which happens rarely.

\hiddensubsection{Learning Pipeline}

The machine-learning library that is used for constructing, training,
validating, and testing the model described in \sref{brain-model} is TensorFlow
\cite{abadi2015}.

The training step (see the leftmost blue box in \fref{brain-overview}) is
configured as follows. Ten buckets or queues are used according to the following
rule:
\[
  \nsi{i} < 6 < 7 < 8 < 9 < 10 < 15 < 20 < 30 < 40 \leq 50.
\]
The batch size \nb is set to 64. The optimization algorithm that is employed for
minimizing the loss function is Adam \cite{kingma2014}, which is an adaptive
technique. The algorithm is applied with its default, recommended settings.

Let us now discuss the validation step, which is the middle blue box in
\fref{brain-overview}. The considered hyperparameters are the number of cells
\nl (the blue boxes in \fref{brain-model}), number of units per cell \nu (the
double circles in \fref{brain-model}), and probability of dropout $\rho_\drop$
introduced in \sref{brain-model}. More concretely, we let $\nl \in \{ 1, 2, 3,
4, 5 \}$, $\nu \in \{ 100, 200, 400, 800, 1600 \}$, and $\rho_\drop \in \{ 0,
0.25, 0.5 \}$, which yields 75 different combinations in total. The candidates
are explored by means of the Hyperband algorithm introduced in
\sref{brain-learning} with its default settings. The maximum budget for one
configuration is set to four training epochs, which correspond to $4 \times 0.7
\times 2 \times 10^6 = 5.6 \times 10^6$ resource-usage profiles or $5.6 \times
10^6 \div 64 = 87~500$ training steps.

The above exploration, which encompasses both the training and validation steps,
takes approximately 15 days to finish. During this process, we run up to four
training sessions in parallel, which typically keeps all the eight cores busy.
It should be noted that, since the training, validation, and testing data sets
have been cached on disk as a result of our data pipeline described in
\sref{brain-data}, individual training sessions do not have any overhead in this
regard.

\inputtable{brain-validation}
The results of the validation step are reported in \tref{brain-validation}. The
table shows the \ac{MSE} of the top 10 configurations of the hyperparameters as
measured using $G_2$ ($0.1 \times 2 \times 10^6 = 2 \times 10^5$ profiles). The
best trained predictive model is found to have the following hyperparameters:
$\nl = 3$, $\nu = 1600$, and $\rho_\drop = 0$. In general, deeper and wider
architectures tend to outperform shallower and narrower ones---the depth and
breadth are measured by \nl and \nu, respectively---which is expected. In these
experiments, the dropout mechanism does not have much impact on the resulting
accuracy, which could be due to the amount of training data being enough for
regularizing the model.

\tref{brain-validation} also shows an estimate of the memory required by each
configuration, including the model's trainable parameters and internal state. It
can be seen that, if the memory usage is a concern, one could trade a small
decrease in accuracy for a considerable memory saving. For example, the fourth
best configuration requires 75\% less memory than the first one.

After the validation step, the best trained model is taken to the testing step
(see \fref{brain-overview}), which is undertaken using $G_3$ ($0.2 \times 2
\times 10^6 = 4 \times 10^5$ profiles). The model is extensively assessed by
predicting the resource usage of individual tasks multiple steps ahead at each
step of the profiles in $G_3$. In this experiment, we predict four steps into
the future; hence, $h = 4$ in \sref{brain-problem}. This elaborate sequential
testing procedure takes around 18 hours.

In order to assess the accuracy of our model better, we employ an alternative
model referred to as the reference model. This model is based on random walk. It
postulates that the best prediction of what will happen tomorrow is what happens
today plus an optional random offset, which we set to zero. In other words, the
next value of a resource-usage profile is estimated to be the current one, which
results in four identical predictions at each time step.

\inputfigure{brain-testing}
The results of the testing step can be seen in \fref{brain-testing}, which shows
the \ac{MSE} of our model (the blue line) as well as the one of the reference
model (the orange line) with respect to $G_3$. The magnitude of our model's
errors suggests that the amount of regularity present in the data is not
sufficient to make the resource-usage predictions highly accurate. Nevertheless,
it can also be seen in \fref{brain-testing} that, relative to the reference
model, our model provides an error reduction of approximately 47\% at each of
the four future time moments. This observation indicates that a certain
structure does exist, and that it can be identified and utilized in order to
make educated predictions.
