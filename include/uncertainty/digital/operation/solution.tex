\inputfigure{brain-overview}
Our workflow for resource-usage prediction is illustrated in
\fref{brain-overview}. First, we note that the available data generally need to
be processed prior to learning since they are likely to be given in a format
that is not efficient or convenient to the subsequent calculations. With this in
mind, our foremost task is to extract $G$ defined in \eref{brain-dataset} from
the given raw data. This processing part can be seen at the top of
\fref{brain-overview} and is explained in \sref{brain-data-pipeline}. Next, the
resulting profiles are utilized in order to obtain an adequately trained
predictive model. The modeling part is covered in \sref{brain-predictive-model}
while the learning one is explained in \sref{brain-learning-pipeline}; the
latter can also be seen at the bottom of \fref{brain-overview}. The above
operations are to be undertaken offline while the trained model is supposed to
be used by the resource manager at runtime in order to make predictions and
subsequently take account of workload uncertainty.

\subsection{Data Pipeline}
\slab{brain-data-pipeline}

In this subsection, we describe our pipeline for working with large data sets,
which makes the data readily accessible for machine-learning undertakings. In
order to make the exposition clearer, the pipeline is described by applying it
to a real-life data set of resource-usage traces collected in a large computer
cluster. To this end, we begin by giving a brief introduction to this data set.

The data set that we work with is the Google cluster-usage traces
\cite{reiss2011}. The traces were collected over 29 days in May 2011 and
encompass more than 12 thousand machines serving requests from more than 900
users. In the data set's parlance, a user request is a job, a job comprises one
or several tasks, and a task is a Linux program to be run on a single machine.
Each job is ascribed a unique \up{ID}, and each task of a job is given an
\up{ID} that is unique in the scope of the job. Apart from other tables, the
data set contains a table that records the resource usage of individual tasks
with the granularity of five minutes. Each record corresponds to a specific task
and a specific five-minute interval, and it provides such measurements as the
average and maximum values of \up{CPU}, memory, and disk usage. There are more
than 1 billion records in the resource-usage table, which correspond to more
than 24 million tasks or, equivalently, 24 million resource-usage profiles and
to more than 670 thousand jobs.

The resource-usage table is provided in the form of 500 archives. Each archive
contains a single file with measurements over a certain time window. This format
is inconvenient and inefficient to work with, which is addressed as described
below and shown by the three topmost boxes in \fref{brain-overview}.

As the first step (the leftmost orange box in \fref{brain-overview}), the data
from the 500 archives are distributed into separate databases so that each such
database contains all the data points that belong to a particular job, resulting
in as many databases as there are jobs. In order to reduce the disk-space
requirements, only the actually used columns of the table are preserved. In our
experiments, these columns are the start time stamp, job \up{ID}, task \up{ID},
and average \up{CPU} usage.

As the second step (the middle orange box in \fref{brain-overview}), an index of
the profiles is created in order to be able to efficiently navigate the catalog
of the databases created at the previous step. Each record in the index contains
metadata about a single task, the most important of which are the task \up{ID}
and the path to the corresponding database. We also include the length of the
profile into the index in order to be able to efficiently filter the profiles by
length.

As the last step (the rightmost orange box in \fref{brain-overview}), a subset
of the resource-usage profiles is selected using the index according to the
needs of a particular learning session (to be discussed in \sref{brain-results})
and then stored on disk. Concretely, the profiles are fetched from the databases
and stored in the native format of the machine-learning library utilized; we
refer to a file in such a format as a binary file. Instead of writing all the
selected profiles into one binary file, we distribute them across several files.
Such a catalog of binary files is created for each of the three parts of the
commonly considered partition of the available data \cite{hastie2013}: one is
for training, one for validation (model selection or development), and one for
testing. In what follows, these three parts of $G$ are denoted by $G_\training$,
$G_\validation$, and $G_\testing$, respectively; see also \fref{brain-overview}.

Lastly, it is common practice to standardize data before feeding them into a
learning algorithm \cite{hastie2013}. In our case, it is done along with
creating the aforementioned three catalogs, which requires a separate pass over
the training set.

In conclusion, the benefit of the data pipeline describe above is in the
streamlined access to the data during one or multiple learning sessions. The
binary files can be read efficiently as many times as needed, and they can be
straightforwardly regenerated whenever the selection criteria change. It is
worth noting that the artifacts of the procedures undertaken at the grouping and
indexing steps stay the same. The presence of multiple binary files also allows
for shuffling the training data at the beginning of each training epoch.

\subsection{Predictive Model}
\slab{brain-predictive-model}

As emphasized in \sref{brain-introduction} and \sref{brain-problem}, our
objective is to assess the applicability of the latest advancements in neural
networks \cite{goodfellow2016} to fine-grained long-range prediction of resource
usage in computer systems. The architectures of neural networks are very
diverse. However, since the data that we study are inherently sequential, it is
natural to found our predictive model on the basis of recurrent neural networks
\cite{goodfellow2016}, which are designed for such cases.

\inputfigure{brain-predictive-model}
A schematic representation of our model can be seen in
\fref{brain-predictive-model}. Note that many of the actual connections between
the parts of the model are simplified or not shown at all in order to make the
figure legible. In the rest of this subsection, we describe each of the parts. A
number of important operational aspects of the model are to be covered in the
next subsection, \sref{brain-learning-pipeline}.

The input to the predictive model is a single \ng-dimensional data point, which
can be seen on the left-hand side of \fref{brain-predictive-model}. Similarly,
the output is a single \ng-dimensional data point, which is depicted on the
right-hand side of \fref{brain-predictive-model}. The input $\vg_{ij}$ is the
value of the resource usage of a single task at time step~$j$, and the output
$\hat{\vg}_{i, j + 1}$ is a one-step-ahead prediction of this usage.

The core of the model is formed by a number of recurrent layers. In
\fref{brain-predictive-model}, these layers are represented by a group of blue
boxes. The network can be made as many layers deep as needed. Each layer is
composed of a number of units, which are depicted as double circles in
\fref{brain-predictive-model}. The number of layers and the number of units per
layer are denoted by \nl and \nu, respectively.

Units are the smallest processing elements. The key characteristics of a unit of
a recurrent layer are that the unit has internal memory, and that it has access
to its previous output. There are different types of recurrent units, defining
how these units let data flow through them and update their memory. One notable
unit is the \ac{LSTM} unit \cite{hochreiter1997}, which has been designed to
overcome the problems of traditional recurrent neural networks, such as the
vanishing gradient during training. The recurrent layers of our predictive model
are \ac{LSTM} layers, that is, layers composed of \ac{LSTM} units.

Each recurrent layer is enhanced by a dropout mechanism \cite{zaremba2014},
which is active only during training. This mechanism gives control over the
regularization of the model and is to prevent potential overfitting
\cite{hastie2013}. For future reference, denote the probability of dropping an
output of a layer by $\rho_\drop$.

The output of the last recurrent layer is typically a large tensor, which is
proportional in size to the number of units in the layer. Each entry of such a
tensor can be considered as a feature that the network has extracted and
activated in accordance with the resource-usage profile that is currently being
fed to the model. The task now is to combine these features in order to produce
a single prediction. To this end, we mount a feedforward layer with a linear
activation function on top of the last recurrent layer, which is depicted as an
orange box in \fref{brain-predictive-model}. Unlike the recurrent layers, which
feature highly nonlinear transformations, this layer performs an affine
transformation.

To summarize, we have described a predictive model that is composed of a number
of recurrent layers and a feedforward layer. Due to its internal memory, the
model is capable of efficiently taking into account the entire past of the
resource-usage profile in question when predicting the future of this profile.
Let us now discuss how the predictive model is supposed to be used.

\subsection{Learning Pipeline}
\slab{brain-learning-pipeline}

Recall that the output of the data pipeline described in
\sref{brain-data-pipeline} is the resource-usage data set $G$, which is split
into three parts: $G_\training$, $G_\validation$, and $G_\testing$. The three
parts are utilized during the following three steps of what we refer to as the
learning pipeline: training, validation, and testing. This learning pipeline is
discussed below and illustrated at the bottom of \fref{brain-overview}.

We begin with training. The model depicted in \fref{brain-predictive-model} has
a large number of parameters that have to be learned; they are primarily various
weights and biases inside the layers. For this purpose, $G_\training$ is
utilized. The training is undertaken via backpropagation through time using
stochastic gradient descent \cite{goodfellow2016} whose objective is to minimize
a certain loss function, which we shall specify shortly. There are two aspects
that need to be discussed first.

\inputfigure{brain-dynamic-unroll}
The first concerns the way a single profile is fed to the predictive model. To
begin with, the internal memory of the model is nullified. Next, recall that
each profile contains several data points (see \eref{brain-profile}), and note
that two profiles generally have different lengths ($\nsi{i} \neq \nsi{j}$)
since the execution times of two tasks are likely to differ. With this in mind,
each profile is fed to the model using a technique called dynamic unrolling. An
illustration is depicted in \fref{brain-dynamic-unroll}, in which the schematic
representation given in \fref{brain-predictive-model} has been simplified even
further and rotated 90${}^\circ$ counterclockwise. It can be seen that the model
has been replicated as many times as there are data points in the profile.
However, it is still the same model, and all the replicas share the same
parameters and the same internal memory. It can also be observed in
\fref{brain-dynamic-unroll} how information flows from one time step to the next
one, which implicitly gives the model access to the whole history at each time
step.

Now, it is not efficient to feed only one resource-usage profile at a time due
to the inevitable overhead imposed by the computations involved. These
computations should be performed in batches, that is, \nb profiles should be fed
simultaneously where \nb is referred to as the batch size. Since, in general,
$\nsi{i} \neq \nsi{j}$, it is not possible to stack multiple arbitrary profiles
into a single tensor. In order to circumvent this problem, we reside to
bucketing. Specifically, each profile is put into one of many buckets chosen
based on the profile's length. When a bucket collecting profiles of length from
some \nsi{l} to \nsi{r} has received \nb profiles, it pads the ones that are
shorter than \nsi{r} with zeros and emits a tensor of size $\nb \times \nsi{r}
\times \ng$ to be further consumed by the model.

The loss function that is minimized during training is the \ac{MSE} of one-step
predictions over the whole batch. The correct prediction for the very last time
step, which goes beyond the time window of the profiles in question, is assumed
to be zero. For instance, $\hat{\vg}_{i, \nsi{i} + 1}$ in
\fref{brain-dynamic-unroll} has no $\vg_{i, \nsi{i} + 1}$ to be compared with,
and this $\vg_{i, \nsi{i} + 1}$ is assumed to be zero.

Let us now discuss validation, which corresponds to the middle blue box in
\fref{brain-overview}. Similarly to other nontrivial models, the one presented
in \sref{brain-predictive-model} has a number of hyperparameters. Examples
include the number of recurrent layers \nl, number of units per layer \nu, and
probability of dropout $\rho_\drop$. Unlike ordinary parameters---which are to
be optimized during training as discussed earlier---hyperparameters are to be
set prior to training and kept unchanged thereafter. From the examples given, it
is apparent that the impact of hyperparameters is profound. Hence, they should
be tuned with great care.

The validation set $G_\validation$ is used to assess the performance of the
model trained (using $G_\training$ as usual) with different configurations of
the model's hyperparameters. Similarly to training, the error metric in this
case is the \ac{MSE}, and it is beneficial to perform validation in batches. The
trained model that has the best performance on $G_\validation$ is then chosen as
the one to use.

Despite all the techniques employed to speed up training, it is still a
time-consuming operation. This means that a brute-force search in the space of
the hyperparameters for the best configuration is impractical; therefore, a
certain intelligent strategy should be followed. In our workflow, we use the
Hyperband algorithm \cite{li2016}. Instead of adaptively choosing new
configurations to evaluate---which is the case with many algorithms of this
kind---Hyperband adaptively allocates resources to configurations chosen at
random, which is shown to be a highly efficient strategy. In particular, the
algorithm allows one to reduce computation time by promptly eliminating overtly
inadequate configurations of the hyperparameters. In this context,
\emph{resources} refers to a user-defined measure of how extensively a
configuration is to be exercised. For instance, it can be the amount of
wall-clock time spent or the number of training steps taken; in our experiments
given in \sref{brain-results}, we use the latter.

Let us now turn to testing; see the rightmost blue box in \fref{brain-overview}.
After a trained model has been selected during the validation step, it has to be
assessed anew \cite{hastie2013}: one cannot aver that the error with respect to
$G_\validation$ is a good estimate of the generalization error due to the
selection bias---we have deliberately chosen the configuration that has the best
performance on $G_\validation$.

In order to have an unbiased evaluation, the testing set $G_\testing$ is
utilized. Similarly to training and validation, the \ac{MSE} is considered as a
measure of quality, and bucketing is utilized. However, unlike training and
validation, the error is calculated in a more elaborate way as follows. Recall
first that our objective is making long-range predictions of resource usage; see
\sref{brain-problem}. Note also that, in training and validation, we are only
concerned with what happens one step ahead. The reason is that we would like to
have as high of a throughput as possible during training and validation since
they are to be performed many times. Testing, on the other hand, is done only
once, and it is during testing that we make and assess multiple-step-ahead
predictions.

In order to calculate long-rage predictions, we use refeeding: at time step~$j$,
the predicted value $\hat{\vg}_{i, j + 1}$ of the resource usage of task~$i$ is
fed to the model as if it was the actual resource usage $\vg_{i, j + 1}$ at time
step $j + 1$, which is not yet known at time step~$j$. At this point, it might
be helpful to consider the example given in \fref{brain-application}. The
process continues until all the desired $h$ future values are estimated. It is
natural to expect that the more accurate the one-step-ahead prediction is, the
more accurate the multiple-step-ahead one becomes.

Consider now how a trained predictive model might be used in practice.
Potentially at each time step~$j$, the next $h$ values of the resource usage of
task~$i$, that is, $\range{\hat{\vg}_{i, j + 1}}{\hat{\vg}_{i, j + h}}$, might
have to be estimated. Therefore, in order to test the model properly, we have to
consider all the time steps of the profile in question and make $h$ predictions
at each time step. An important aspect to note is that the state of the model's
memory should be saved before computing $\range{\hat{\vg}_{i, j +
1}}{\hat{\vg}_{i, j + h}}$ at time step~$j$ and restored before feeding $\vg_{i,
j + 1}$ in order to advance to time step $j + 1$. The reason is that the memory
becomes contaminated when one feeds predictions instead of observations into the
model.

At this point, the main aspects of our workflow, which is illustrated in
\fref{brain-overview}, have been discussed. The output of the workflow is a
predictive model that has been trained on $G_\training$, validated on
$G_\validation$, and tested on $G_\testing$.
