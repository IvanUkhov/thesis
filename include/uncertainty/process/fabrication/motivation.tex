Consider the distribution of the effective channel length, which we denote by \g
in this section, across a silicon wafer. As noted in \sref{power-model}, \g has
one of the strongest effects on the leakage current and consequently on power
and temperature \cite{juan2012}. At the same time, \g is well known to be
severely affected by process variation \cite{chandrakasan2000, srivastava2010}.
Therefore, the distribution of \g is not uniform across the wafer. For
concreteness, let this distribution be the one depicted on the left-hand side of
\fref{bayes-motivation-distribution}. The gradient from blue to yellow
represents the transition of \g from low to high values, and the scale is given
in terms of the number of standard deviations away from the mean value; the
exact experimental setup is to be described in detail in \sref{bayes-results}.
Hence, the blue regions have a high level of power consumption and heat
dissipation.

Assume that the technological process imposes a lower bound $\g_\minimum$ on \g.
It separates defective dies ($\g < \g_\minimum$) from those that are acceptable
($\g \geq \g_\minimum$). In order to reduce costs, the manufacturer is
interested in detecting the faulty dies and taking them out of the production
process at the earliest possible stage. The possible actions with respect to a
die on the wafer are then \one~to keep the die if it conforms to the
specification or \two~to dispose of it otherwise.

\inputfigure{bayes-motivation-distribution}
In order to analyze the variability of the effective channel length \g across
the wafer, one could remove the top layer of the dies (hence destroying them)
and measure \g directly. Alternatively, despite the fact that the knowledge of
\g is more preferable, one could step back and decide to quantify process
variation using an auxiliary parameter \h that can be measured without damaging
the dies; for instance, \h could be the leakage current. It should be noted
that, in this second case, \h is the final product of the analysis, and \g
remains unknown.

In either case, adequate test structures have to be present on the dies in order
to take measurements at a sufficient number of locations with a sufficient level
of granularity. Such a sophisticated test structure might not always be readily
available, and its deployment might significantly increase production expenses.
Moreover, as noted earlier, the first approach implies that the measured dies
have to be disposed of afterwards, and the second approach implies that further
design decisions will be based on a surrogate quantity \h instead of the primary
target of process variation \g, which could compromise these decisions. The
latter concern is particularly prominent in situations where the production
process is not yet completely stable, and design decisions based on the primary
subjects of process variation are consequently preferable.

Our technique operates differently. In this example, in order to characterize
the effective channel length \g, we begin by measuring an auxiliary quantity \h.
The quantity is required to depend on \g, and it can be chosen to be
straightforward from the measurement perspective. The distribution of \g across
the whole wafer is then obtained by inferring it from the collected measurements
of \h. Our technique permits these measurements to be taken only at a small
number of locations on the wafer and to be corrupted by noise, which could be
due to the imperfection of the measurement equipment that is used.

Let us consider one particular \h that can be used to study the effective
channel length \g. Specifically, let \h be temperature, which will be discussed
further in \sref{bayes-results}. We can then apply a fixed workload---for
instance, we can run the same application under the same conditions---to a few
dies on the wafer and measure the corresponding temperature profiles. Since
temperature does not require extra equipment to be deployed on the wafer and can
be tracked using infrared cameras \cite{mesa-martinez2007} or built-in
components of the dies, our approach can reduce the costs associated with
analysis of process variation. The results of our framework applied to a set of
noisy temperature profiles measured on only 7\% of the dies are shown on the
right-hand side of \fref{bayes-motivation-distribution}, and the locations of
the measured dies are depicted on the left-hand side of
\fref{bayes-motivation-location}. It can be seen that the two maps in
\fref{bayes-motivation-distribution} closely match each other, implying that the
distribution of \g is reconstructed with a high level of accuracy.

\inputfigure{bayes-motivation-location}
Another characteristic of the proposed framework is that probabilities of
various events, for instance, $\probability{\g \geq \g_\minimum}$, can be
readily estimated. This is important, since the true values are unknown in
reality; otherwise, we would not need to infer them. Therefore, we can rely on
our decisions only up to a certain probability. We can then reformulate the
decision rule given earlier as follows: \one~to keep the die if $\probability{\g
\geq \g_\minimum}$ is larger than a certain threshold or \two~to dispose of it
otherwise. An illustration of following this rule is given on the right-hand
side of \fref{bayes-motivation-location} where $\g_\minimum$ is set to two
standard deviations below the mean value of \g, the probability threshold of the
first action is set to 0.9, the crosses mark both the true and inferred
defective dies (they coincide), and the gradient from white to orange
corresponds to the inferred probability of defect. It can be seen that the
inference accurately detects the faulty dies.

In addition, we can introduce a trade-off action between action \one and action
\two as follows: \three~to expose the die to a thorough inspection (for
instance, via a test structure) if $\probability{\g \geq \g_\minimum}$ is
smaller than the threshold of action \one but larger than another threshold. For
instance, action \three can be taken if $0.1 < \probability{\g \geq \g_\minimum}
< 0.9$. In this scenario, we can reduce expenses by examining only those dies
for which there is no sufficiently strong evidence of their satisfactory or
unsatisfactory condition. Furthermore, one can take into consideration a
so-called utility function, which, for each combination of an outcome of \g and
an action that is taken, returns the gain that the decision maker obtains. For
example, such a function could favor a rare omission of malfunctioning dies over
a frequent inspection of correct dies, as the latter might involve much higher
costs. The optimal decision is given by the action that maximizes the expected
utility with respect to both the observed data and prior knowledge regarding \g.
Consequently, all possible values of \g weighted by their probabilities are
taken into account in the final decision while also incorporating the
preferences of the designer via the utility function.
