The key building block of the solutions that we develop in the subsequent
sections is the uncertainty-quantification technique presented in this section.
The task of this technique is to propagate uncertainty through the system, from
a set of inputs to a set of outputs. The inputs are the uncertain parameters
\vu, and the outputs are the quantities that the designer is interested in
studying. The former could be, for instance, the effective channel length, gate
oxide thickness, and threshold voltage, and the latter could be, for instance,
the temperature profile, energy consumption, and maximum temperature of the
system.

\inputfigure{chaos-overview}
The major stages of our general approach are depicted in \fref{chaos-overview}.
At Stage~1, the quantity of interest \g and the uncertain parameters \vu are
specified. The quantity is given as a ``black-box'' function that operates on a
particular outcome of the parameters. In order to evaluate \g, the designer is
implicitly required to specify the system model being considered, which also
includes a power model and a temperature model. At Stage~2, the uncertain
parameters \vu are to be transformed into independent random variables \vz,
since independence is a prerequisite of the subsequent calculations. At Stage~3,
a surrogate for \g is constructed by means of the \ac{PC} decomposition. At
Stage~4, the computed expansion is processed in order to obtain the desired
statistics about \g.

\subsection{\problemtitle}
\slab{chaos-formulation}

The problem formulation is problem specific. Thus, in this general section, we
operate on unspecified \g and \vu; more concrete discussions about Stage~1 are
postponed to later sections where we start to apply the proposed methodology to
particular problems. In addition, for convenience, \g is assumed to take values
in $\real$ here, which is generalized to $\real^n$ later on. Lastly, there is
one recurring aspect about \vu that is worth discussing before going any
further.

A description of \vu that is supplied by the designer is an input to our
probabilistic analysis. A proper way to describe a set of random variables is to
specify their joint distribution function; see \xref{probability-theory}. In
practice, however, such exhaustive information is often unavailable, which is
due mainly to the high dimensionality and intricate dependencies inherent to
real-life problems.

A more realistic assumption is the knowledge of the marginal distributions
$\set{F_i}_{i = 1}^\nu$ and correlation matrix $\correlation{\vu}$ of \vu.
However, these are not sufficient to reconstruct the joint distribution of \vu
in general. Nevertheless, it can be approximated well by accompanying the
available marginals with a carefully constructed copula \cite{nelsen2006}, which
makes \vu fully specified; see \xref{probability-theory}. Specifically, one
constructs a Gaussian copula based on $\set{F_i}_{i = 1}^\nu$ and
$\correlation{\vu}$ as a part of the Nataf transformation \cite{liu1986}, which
is introduced in \xref{probability-transformation}. This Gaussian copula is
defined in terms of an auxiliary correlation matrix.

Without loss of generality, we target the above practical scenario in our
experiments in this section and the next one. However, it should be understood
that, even though the marginal distributions and the above copula prescribe a
joint distribution for \vu, this distribution is an approximation rather than
the true one. If the joint distribution is available, it should be used instead.

\subsection{Probability Transformation}
\slab{chaos-transformation}

Mutual independence of random variables is required by \ac{PC} expansions. In
general, however, the individual variables in $\vu: \Omega \to \real^\nu$ are
dependent. Therefore, our foremost task is to transform \vu into a vector with
independent components in order to fulfill the requirement; see Stage~2 in
\fref{chaos-overview}. To this end, an adequate transformation should be
performed, depending on the available information \cite{eldred2008}. Denote such
a transformation by
\begin{equation} \elab{chaos-transformation}
  \vu = \transform{\vz}
\end{equation}
where $\vz: \Omega \to \real^\nz$ is a random vector with \nz independent
components, and $\transform: \real^\nz \to \real^\nu$. The quantity of interest
\g can now be computed as
\[
  \g(\vu) = (\g \circ \transform)(\vz) = \g(\transform(\vz)).
\]

Correlated random variables can be transformed into linearly uncorrelated ones
via the \ac{KL} decomposition described in \xref{probability-transformation} and
shown in \eref{karhunen-loeve}. In addition, if the correlated variables form a
Gaussian vector, the uncorrelated variables are mutually independent. In the
general case (non-Gaussian), the most prominent solutions for attaining
independence are the Rosenblatt transformation \cite{rosenblatt1952} and the
Nataf transformation mentioned earlier. Rosenblatt's approach is suitable when
the joint distribution of \vu is known. However, as emphasized in
\sref{chaos-formulation}, such information is rarely available. Marginal
distributions and a correlation matrix are more likely to be given, which are
sufficient for the Nataf transformation; see \xref{probability-transformation}.

Apart from the extraction of the independent variables \vz, an essential
operation at this stage is model order reduction, since the number of stochastic
dimensions---that is, the dimensionality of \vz---directly impacts the
complexity of the rest of the computations. This operation is frequently treated
as a part of the \ac{KL} decomposition, and it is also elaborated on in
\xref{probability-transformation}.

\subsection{Surrogate Construction}
\slab{chaos-construction}

In order to obtain a computationally efficient and convenient characterization
of \g, we utilize the \ac{PC} decomposition with nonintrusive spectral
projections \cite{xiu2010}. The corresponding mathematical foundation is given
in \xref{polynomial-chaos}.

Assume that \g as a function of \vz belongs to $\L{2}{\Omega, \F,
\probability}$; see \xref{probability-theory}. Then \g is expanded into the
following series at Stage~3 in \fref{chaos-overview}:
\begin{equation} \elab{chaos-expansion}
  \g \approx \chaos{\nz}{\lc}{\g}
  = \sum_{\vi \in \sparseindex{\nz}{\lc}} \hat{\g}_{\vi} \psi_{\vi}
\end{equation}
where $\lc \in \natural$ is the level of the expansion, $\vi = (i_k) \in
\natural^\nz$ is an index, $\sparseindex{\nz}{\lc}$ is an index set, and
$\set{\psi_{\vi}}{\vi \in \sparseindex{\nz}{\lc}}$ are orthonormal polynomials
in \nz variables whose orders are specified by the corresponding elements of
\vi.

It is clear that the first step toward a polynomial expansion is the choice of a
suitable polynomial basis, which is typically made based on the Askey scheme of
orthogonal polynomials \cite{xiu2010}. This step is crucial, as the rate of
convergence of \ac{PC} expansions depends on it. Although there are no rules
that guarantee the optimal choice \cite{knio2006}, there are best practices
suggesting that one should be guided by the probability distributions of the
random variables that drive the stochastic system at hand. For instance, when a
random variable follows a beta distribution, it is worth trying the Jacobi basis
first; on the other hand, the Hermite basis is preferable for Gaussian
distributions.

As shown in \eref{chaos-inner-product} and \eref{chaos-projection}, each
coefficient $\hat{\g}_{\vi}$ is an \nz-dimensional integral of the product of \g
and $\psi_{\vi}$. In general, this integral should be computed numerically as
described in \xref{numerical-integration}. Specifically, an adequate
\nz-dimensional quadrature $\quadrature{\nz}{\lq}$, which is a set of
\nz-dimensional points accompanied by a set of scalar weights, is utilized. The
result is
\begin{equation} \elab{chaos-coefficient}
  \hat{\g}_{\vi} \approx \quadrature{\nz}{\lq}{\g \psi_{\vi}}
  = \sum_{j \in \tensorindex{\nz}{\lq}} (\g \circ \transform)(\vz_j) \psi_{\vi}(\vz_j) w_j
\end{equation}
where $\lq \in \natural$ is the quadrature's level, and $\set{\vz_j} \subset
\real^\nz$ and $\set{w_j} \subset \real$ are the corresponding points and
weights, respectively, indexed by $\tensorindex{\nz}{\lq} \subset \natural$. The
operator $\quadrature{\nz}{\lq}$ is constructed via the Smolyak algorithm
\cite{smolyak1963} as shown in \eref{quadrature-sparse}. An important aspect to
note about this construction is that it is a combination of a number of
cherry-picked operators identified by a certain index set denoted by
$\sparseindex{\nz}{\lq} \subset \natural^n$. Let us discuss the content of
$\sparseindex{\nz}{\lc}$ and $\sparseindex{\nz}{\lq}$.

The standard choice of $\sparseindex{\nz}{\lc}$ in \eref{chaos-expansion} is the
isotropic total-order index set, which can be seen in
\eref{index-total-order-isotropic}. \emph{Isotropic} refers to constraining all
dimensions identically, and \emph{total-order} to the criterion used for
constraining each dimension. Since $\psi_{\vi}$ is a polynomial of total order
at most \lc, and \g is approximated by such a polynomial, the integrand in
\eref{chaos-coefficient} can be assumed to be a polynomial of total order at
least $2 \lc$. With this in mind, one typically constructs such a quadrature
that is exact for polynomials of total order up to at least $2 \lc$
\cite{eldred2008}. More generally, the index set $\sparseindex{\nz}{\lc}$, which
is used in \eref{chaos-expansion}, and the index set $\sparseindex{\nz}{\lq}$,
which implicitly determines the content of the index set
$\tensorindex{\nz}{\lq}$ used in \eref{chaos-coefficient}, should be related as
$\sparseindex{\nz}{\lc} \subseteq \sparseindex{\nz}{\lq}$.

In the case of Gaussian quadratures, which is a broad and potent class of
quadratures introduced in \xref{numerical-integration}, a quadrature of level
\lq is exact for polynomials of total order up to $2 \lq + 1$ \cite{heiss2008}.
Therefore, in this very common case, an adequate quadrature can be constructed
by ensuring that $\lq \geq \lc$.

An important generalization of the isotropic Smolyak algorithm in
\eref{quadrature-sparse} is the anisotropic Smolyak algorithm \cite{nobile2008}.
The difference between the isotropic and anisotropic versions lies in the
content of $\sparseindex{\nz}{\lq}$. In particular, the anisotropic total-order
index set is defined as follows:
\begin{equation} \elab{index-total-order-anisotropic}
  \sparseindex{\nz}{\lq} = \set{\vi}{\vi \in \natural^\nz, \innerproduct{\v{c}}{\vi} \leq \lq \, \min_{i = 1}^\nz c_i}
\end{equation}
where $\v{c} = (c_i) \in \real^\nz$ with $c_i \geq 0$ for $i = \range{1}{\nz}$
is a vector assigning importance weights to the dimensions, and
$\innerproduct{\cdot}{\cdot}$ is the standard inner product in $\real^\nz$.
\eref{index-total-order-anisotropic} plugged into \eref{quadrature-sparse}
results in a sparse grid that is exact for the polynomial subspace that is
obtained using the same index set.

The above approach allows one to exploit anisotropic behaviors that are present
in many practical problems \cite{nobile2008}. It provides fine control over the
computational cost associated with the construction of \ac{PC} expansions: a
carefully chosen importance vector $\v{c}$ in
\eref{index-total-order-anisotropic} can significantly reduce the number of
polynomial terms in \eref{chaos-expansion} and the number of quadrature points
in \eref{chaos-coefficient}, which are needed for calculating the coefficients
of those polynomial terms. The question, then, is in the choice of $\v{c}$. When
the \ac{KL} decomposition is utilized as a part of $\transform$ in
\eref{chaos-transformation}, a viable option in this regard is to rely on the
variance contributions of the dimensions given by $\set{\lambda_i}_{i = 1}^\nu$
in \eref{karhunen-loeve}. Specifically, we let
\begin{equation} \elab{chaos-anisotropic-weight}
  c_i = \left(\frac{\lambda_i}{\sum_{j = 1}^\nu \lambda_j}\right)^\gamma
\end{equation}
for $i = \range{1}{\nz}$ where $\gamma \in [0, 1]$ is a tuning parameter. The
isotropic scenario can be recovered by setting $\gamma = 0$; other values of
$\gamma$ correspond to various levels of anisotropy with the maximum attained by
setting $\gamma = 1$.

Once \vz has been identified, and \lc, \lq, and $\v{c}$ have been chosen, the
corresponding polynomial basis and quadrature stay the same for all quantities
that one might be interested in studying. This observation is highly important,
as a lot of preparatory work can and should be done only once and then reused as
needed. In particular, the construction in \eref{chaos-expansion} can be reduced
to one matrix multiplication with a precomputed matrix, which we show next.

\inputalgorithm{chaos-construction}
Let $\nc = \cardinality{\sparseindex{\nz}{\lc}}$ be the cardinality of
$\sparseindex{\nz}{\lc}$, which is the number of polynomial terms and
coefficients in \eref{chaos-expansion}. Let also $\nq =
\cardinality{\tensorindex{\nz}{\lq}}$ be the cardinality of
$\tensorindex{\nz}{\lq}$, which is the number of quadrature points and weights
in \eref{chaos-coefficient}. Furthermore, assume that the index sets
$\sparseindex{\nz}{\lc}$ and $\tensorindex{\nz}{\lq}$ are given certain
orderings so that one is able to refer to their elements using one-dimensional
indices $i = \range{1}{\nc}$ and $j = \range{1}{\nq}$, respectively. Now, let
\begin{equation} \elab{chaos-projection-matrix}
  \Pi = (\psi_i(\vz_j) w_j)_{i = 1, j = 1}^{i = \nc, j = \nq}.
\end{equation}
The element of $\Pi$ on row~$i$ and column~$j$ is polynomial~$i$ evaluated at
quadrature point~$j$ and multiplied by quadrature weight~$j$. The matrix $\Pi$
is referred to as the projection matrix. Using $\Pi$, the coefficients
$\set{\hat{g}_{\vi}}{\vi \in \sparseindex{\nz}{\lc}}$ in \eref{chaos-expansion}
can now be trivially computed as follows:
\begin{equation} \elab{chaos-coefficients}
  \hat{\vg} = \Pi \vg
\end{equation}
where
\begin{align*}
  & \hat{\vg} = (\hat{\g}_i)_{i = 1}^\nc \text{ and} \\
  & \vg = ((\g \circ \transform)(\vz_i))_{i = 1}^\nq.
\end{align*}
It can be seen that this formula is a matrix version of
\eref{chaos-coefficient}. The matrix $\Pi$ is the one that should be precomputed
and stored for future use.

The pseudocode of a procedure that computes \ac{PC} expansions by leveraging the
projection matrix $\Pi$ is given in \aref{chaos-construction} where Algorithm~G
stands for a subroutine that calculates $\g \circ \transform$ for a given \vz,
which is problem specific.

Let us summarize this subsection. In order to give a probabilistic
characterization of the quantity of interest \g, we construct a polynomial
expansion of this quantity as shown in \eref{chaos-expansion}. The coefficients
of this expansion are found by means of a suitable multivariate quadrature as
shown in \eref{chaos-coefficient}. The quadrature is constructed via the Smolyak
formula given in \eref{quadrature-sparse}. The index set used in both
\eref{chaos-expansion} and \eref{quadrature-sparse} is the one given in
\eref{index-total-order-anisotropic} where the anisotropic weights are set
according to \eref{chaos-anisotropic-weight}. For computational efficiency, the
projection matrix defined in \eref{chaos-projection-matrix} is to be calculated,
stored, and used as illustrated in \aref{chaos-construction}.

\subsection{Post-Processing}
\slab{chaos-processing}

Due to the properties of the \ac{PC} decomposition---in particular, the
orthogonality of the basis discussed in \xref{polynomial-chaos}---the obtained
polynomial representation in \eref{chaos-expansion} allows various statistics
about \g to be estimated with little effort, which is the subject of Stage~4 in
\fref{chaos-overview}. The reason that this estimation is straightforward is
that the function given in \eref{chaos-expansion} is nothing more than a
polynomial; hence, it is easy to interpret and easy to evaluate.

Let us find, for example, the expectation and variance of \g. Since the first
polynomial $\psi_{\v{0}}$ in a normalized polynomial basis is unity by
definition \cite{xiu2010},
\[
  \expectation{\psi_{\v{0}}} = 1.
\]
Hence, using the orthogonality property in \eref{chaos-orthogonality}, we
conclude that
\[
  \expectation{\psi_{\vi}} = 0
\]
for $\vi \in \sparseindex{\nz}{\lc} \setminus \set{\v{0}}$. Consequently, the
expected value and variance of \g have the following straightforward
expressions:
\begin{equation} \elab{chaos-moments}
  \begin{split}
    & \expectation{\g} = \hat{\g}_{\v{0}} \text{ and} \\
    & \variance{\g} = \sum_{\vi \in \sparseindex{\nz}{\lc} \setminus \set{\v{0}}} \hat{\g}_{\vi}^2,
  \end{split}
\end{equation}
respectively. It can be seen that the \ac{PC} decomposition provides analytical
formulae, solely based on the coefficients, for these probabilistic moments.

The \ac{CDF} and \ac{PDF} of \g can be estimated by a sampling method applied to
\eref{chaos-expansion}, which is typically followed by kernel density estimation
\cite{hastie2013} or a similar technique. The sampling in this context can be
better understood by rewriting \eref{chaos-expansion}, which is given in terms
of operators, as follows:
\[
  (\g \circ \transform)(\vz) \approx \chaos{\nz}{\lc}{\g}(\vz)
  = \sum_{\vi \in \sparseindex{\nz}{\lc}} \hat{\g}_{\vi} \psi_{\vi}(\vz).
\]
Here the aforementioned operators are applied to an outcome of \vz drawn from
the corresponding distribution. Consequently, each sample is a trivial
evaluation of a polynomial; therefore, sampling methods are computationally
cheap in this case. Furthermore, probabilities of various events can be
estimated in a similar way, and global and local sensitivity analysis of
deterministic and stochastic quantities can be readily conducted on the
expansion.

\begin{remark} \rlab{chaos-multidimensional-output}
The development given in this section remains valid even when \g is a
multidimensional quantity from the standpoint of the number of outputs. In this
case, it is convenient to consider \g as a row vector with an appropriate number
of elements. Then all the operations that involve \g---such as those given in
\eref{chaos-expansion}, \eref{chaos-coefficient}, and
\eref{chaos-moments}---should be undertaken elementwise. In
\eref{chaos-coefficients} and \aref{chaos-construction}, \vg and $\hat{\vg}$ are
treated as matrices with \nc rows, and $\g_i$ as a row vector. The output of
Algorithm~G in \aref{chaos-construction} is assumed to be automatically reshaped
into a row vector.
\end{remark}

In the following, we apply the uncertainty analysis presented here to a number
of concrete problems, namely transient and dynamic steady-state power and
temperature analysis as well as reliability analysis and optimization.
