In this section, we evaluate the performance of our approach to reliability
analysis and optimization presented in \sref{chaos-reliability-analysis} and
\sref{chaos-optimization} considering the illustrative application described in
\sref{chaos-optimization-application}. The technique for dynamic steady-state
analysis under process variation delineated in
\sref{chaos-dynamic-steady-analysis} is also a part of the assessment, since it
is included in the reliability model. All the experiments are conducted on a
\up{GNU}/Linux machine equipped with 16 Intel Xeon E5520 2.27~\up{GH}z
processors and 24~\up{GB} of \up{RAM}. All the configuration files used in the
experiments are available online at \cite{eslab2015}.

We consider a 45-nm technological process and rely on the 45-nm open cell
library by NanGate \cite{nangate} as explained in
\sref{chaos-optimization-application}. The effective channel length and gate
oxide thickness are assumed to have nominal values equal to 22.5~nm and 1~nm,
respectively. Based on the International Technology Roadmap for Semiconductors
\cite{itrs}, each parameter is assumed to deviate by up to 12\% of its nominal
value; the percentage is treated as three standard deviations, and the
assumption should be understood accordingly. Regarding the correlation function
in \eref{bayes-correlation}, the weight coefficient $w$ is set to 0.5, and the
length-scale parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of
the die. The model order reduction described in
\xref{probability-transformation} is set to preserve 95\% of the variance of the
problem. The parameter $\gamma$ used in \eref{chaos-anisotropic-weight}, which
controls the anisotropy of \ac{PC} expansions and integration grids, is set to
0.25.

Heterogeneous platforms and periodic applications are randomly generated via
\up{TGFF} \cite{dick1998} in such a way that the execution time of each task is
uniformly distributed between 10 and 30~ms, and its dynamic power between 6 and
20~W. The floorplans of the platforms being considered are regular grids with
each processing element occupying 4~\power{mm}{2}. The sampling interval \dt of
power and temperature profiles is set to 1~ms. The stopping condition used in
\aref{chaos-dynamic-steady-solution-iterative} is that the \ac{NRMSE} between
two successive temperature profiles becomes smaller than 1\%, which typically
requires 3--5 iterations.

\hiddensubsection{Approximation Accuracy}

Our first task is to evaluate the accuracy of the proposed framework. To this
end, in this subsection, the quantity of interest given in
\eref{chaos-optimization-quantity}---which encompasses the total energy
consumption, maximum temperature, and parameterization of the reliability model
of the system---is considered in isolation. This quantity plays the key role in
the subsequent optimization, since the optimization objective and constraints
depend on it as discussed previously.

We compare the performance of our technique with that of direct \ac{MC} sampling
applied to the quantity shown in \eref{chaos-optimization-quantity}. The
operations performed by direct sampling for one sample are the same as those
performed by our framework for one quadrature point. The only difference is that
no model order reduction of any kind is undertaken prior to \ac{MC} sampling,
which ensures that the resulting accuracy is not compromised. The number of
\ac{MC} samples is set to \power{10}{4}, which is a practical assumption based
not only on our experience but also on the literature as discussed in
\sref{chaos-transient-results}.

\inputtable{chaos-optimization-accuracy}
The results are displayed in \tref{chaos-optimization-accuracy} where we
consider a quad-core platform ($\np = 4$) with 10 randomly generated
applications and vary the level of polynomial expansions \lc from one to five.
The errors for the three components of $\vg = (E, Q, \life)$ are denoted by
\error{E}, \error{Q}, and \error{\life}, respectively. Each error metric shows
the distance between the empirical probability distributions produced by our
approach and the ones produced by direct sampling. The measure of this distance
is the \ac{KLD} \cite{gelman2013, hastie2013} where the results of direct
sampling are treated as the true ones. The \ac{KLD} takes non-negative values
and reaches zero only when two distributions are equal almost everywhere
\cite{durrett2010}. In general, the errors decrease as \lc increases. This
trend, however, is not monotonic for \ac{PC} expansions of high levels; see
\error{Q} and \error{\life} for $\lc = 5$ in \tref{chaos-optimization-accuracy}.
This observation can be ascribed to the random nature of sampling and to the
reduction procedures that we undertake in order to gain speed; they reasonably
impose limitations on the accuracy that can be attained by polynomial
surrogates. \tref{chaos-optimization-accuracy} additionally contains the number
of polynomial terms \nc and the number of quadrature points \nq that correspond
to each value of \lc. We have performed the above experiment for platforms with
both fewer and more processing elements and have observed similar results.

Based on the figures reported in \tref{chaos-optimization-accuracy}, we consider
the results delivered by third-level \ac{PC} expansions, where the \ac{KLD}
drops to the third decimal place for all the three quantities, sufficiently
accurate. Therefore, we fix \lc---and hence \lq, as they are kept
synchronized---to three for the rest of the experiments.

\hiddensubsection{Computational Speed}

\inputtable{chaos-optimization-speed}
Our task now is to assess the speed of the proposed solution. To this end, we
consider the same setup as the one outlined in the previous subsection.
\tref{chaos-optimization-speed} displays the time needed to perform one
characterization of \vg for the number of processing elements \np increasing
from 2 to 32. Note that, in this experiment, no parallel computing is utilized.
It can be seen that the computation time ranges from a fraction of a second to
around two seconds. More importantly, \tref{chaos-optimization-speed} provides
information about a number of complementary quantities that are of great
interest to the designer, which we discuss below.

The primary quantity to pay heed to is the number of random variables \nz
preserved after the reduction procedure noted in \sref{chaos-transformation} and
described in \xref{probability-transformation}. Without this reduction, \nz
would be $2 \np$, since there are two process parameters per processing element;
recall \sref{chaos-optimization-application}. It can be seen in
\tref{chaos-optimization-speed} that there is no reduction in the case of the
dual-core platform, whereas around 80\% of the stochastic dimensions are
eliminated in the case of the platform with 32 processing elements. One can also
note that \nz is the same for the last two platforms. The magnitude of reduction
is determined solely by the assumed correlation structure (see
\sref{chaos-optimization-application}) and the floorplan of the platform at
hand, which we also observe and discuss in \sref{chaos-transient-results}.

Another important quantity displayed in \tref{chaos-optimization-speed} is the
number of quadrature points \nq. This number is the main indicator of the
computational expense of our probabilistic analysis: it is equal to the number
of times Algorithm~G in \aref{chaos-construction} must be executed in order to
construct a \ac{PC} expansion of the quantity of interest \g, which, in this
case, is the one in \eref{chaos-optimization-quantity}. Note that \nq is very
low. In order to substantiate this, the last column of
\tref{chaos-optimization-speed} shows the speedup of our approach with respect
to \power{10}{4} \ac{MC} samples. The proposed solution is 100--200 times faster
while delivering highly accurate results as discussed earlier. It should be
noted that the comparison has been drawn based on the number of evaluation
points rather than on the wall-clock time, since the relative cost of other
computations is negligible.

To summarize, the proposed framework for probabilistic analysis of electronic
systems under process variation has been assessed using the composite quantity
given in \eref{chaos-optimization-quantity}. The results shown in
\tref{chaos-optimization-accuracy} and \tref{chaos-optimization-speed} indicate
that our approach is both accurate and computationally efficient.

\hiddensubsection{Optimization Effectiveness}

\inputtable{chaos-optimization-objective}
In this subsection, the results of the optimization procedure formulated in
\sref{chaos-optimization} are reported. To reiterate, the objective is to
minimize the expected energy consumption as shown in
\eref{chaos-optimization-objective} while satisfying a set of constraints on the
maximum end-to-end delay, maximum temperature, and minimum lifetime as shown in
\eref{chaos-optimization-constraints}. For optimization, we employ a genetic
algorithm and evaluate candidate solutions in parallel using 16 cores.

The goal of this experiment is to justify the following assertion: reliability
analysis has to take account of the effect of process variation on temperature.
To this end, for each problem (a pair of a platform and an application), we run
the optimization procedure twice: the first run assumes the setup discussed in
this section so far, and the second run treats the objective in
\eref{chaos-optimization-objective} and the constraints in
\eref{chaos-optimization-constraints} as deterministic. Specifically, the second
run assumes that temperature is deterministic and can be computed using the
nominal values of the process parameters. Therefore, in this deterministic case,
only one execution of the system is needed in order to evaluate a chromosome's
fitness. \eref{chaos-optimization-objective} and
\eref{chaos-optimization-constraints} become, respectively,
\[
    \min_{\schedule} E(\schedule)
\]
and
\begin{align*}
  & \period(\schedule) \leq \period_\maximum, \\
  & Q(\schedule) \geq \q_\maximum, \text{ and} \\
  & \life(\schedule) \leq \life_\minimum.
\end{align*}

We consider five platforms with the number of processing elements \np taking
values in $\set{2^i}_{i = 1}^5$ and 10 applications with the number of tasks \nt
equal to $20 \, \np$; therefore, there are 50 problems in total. In
\eref{chaos-optimization-constraints}, $\rho_\burn$ and $\rho_\wear$ are set to
0.01. Due to the diversity of the addressed problems, $\period_\maximum$,
$\q_\maximum$, and $\life_\minimum$ are individually found for each problem in
order to ensure that their values are sensible to the subsequent optimization.
For instance, $\q_\maximum$ ranges from \celsius{90} to \celsius{120}, depending
on the problem. Note, however, that these three parameters stay the same in both
the probabilistic and deterministic cases.

The obtained results are reported in \tref{chaos-optimization-objective}. The
most important message is in the last column. \emph{Failure} refers to the
percentage of the solutions produced by the deterministic optimization that,
after being re-evaluated using our probabilistic approach (that is, after taking
process variation into account), have been found to be violating the
probabilistic constraints given in \eref{chaos-optimization-constraints}. To
give an example, for the quad-core platform, 6 out of 10 schedules proposed by
the deterministic approach violate the constraint on the maximum temperature or
minimum lifetime (or both) when process variation is taken into consideration.
The more complex the problem becomes, the higher values the failure rate
attains: with 16 and 32 processing elements (320 and 640 tasks, respectively),
all deterministic solutions violate the imposed constraints. Moreover, the
difference between the acceptable 1\% of burn and wear ($\rho_\burn = \rho_\wear
= 0.01$) and the actual probability of burn and wear is found to be as high as
80\% in some cases, which is unacceptable.

In addition, we take a close look at those few deterministic solutions that have
passed the probabilistic re-evaluation and observe that the reported reduction
in the maximum temperature and energy consumption as well as the reported
increase in the lifetime are overoptimistic. To elaborate, the predictions
produced by the deterministic optimization, which neglects process variation,
are compared with the expected values obtained when process variation is taken
into account. The comparison shows that the expected energy and temperature are
up to 5\% higher while the expected lifetime is up to 20\% shorter than the ones
estimated by the deterministic approach. This aspect of the deterministic
optimization can mislead the designer. Hence, when studying those
characteristics of electronic systems that are concerned with power,
temperature, and reliability, the ignorance of the damaging effect of process
variation can severely compromise the associated design decisions, making them
less profitable in the best case and dangerous in the worst scenario.

Let us now comment on the optimization time shown in
\tref{chaos-optimization-objective}. It can be observed that our framework takes
from around one minute to six hours (utilizing 16 cores) in order to perform the
optimization, and that the deterministic technique is 2--40 times faster.
However, the price to pay when relying on the latter is considerably high as
discussed above. The deterministic approach is blind-guessing with highly
unfavorable odds of succeeding. Therefore, the computation time of our framework
is considered reasonable and affordable.

Lastly, we perform an experiment targeted at investigating the impact of the
lifetime constraint in \eref{chaos-optimization-constraints} on the reduction in
the expected energy consumption. To this end, we run our probabilistic
optimization (all 50 problems) without the reliability constraint and compare
the corresponding results with those obtained when the lifetime constraint is
included. We observe that the expected energy consumption is higher when the
constraint is taken into account; however, the difference vanishes when the
complexity of the problem increases. On average, the cost of the lifetime
constraint is below 5\% in terms of the expected energy consumption. Without the
constraint, however, no (probabilistic) guarantees on the system's lifetime can
be given.
