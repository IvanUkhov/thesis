As mentioned \sref{past}, a sampling method would be a reasonable solution to
probabilistic analysis of electronic systems if these systems were
computationally inexpensive to evaluate. In order to eliminate or reduce the
costs associated with direct sampling, a number of techniques have been
introduced.

We begin by noting that our work presented in
\cref{uncertainty-process-development} and thus the prior studies discussed in
\sref{chaos-past} are of relevance to this section. However, since those
techniques are designed for a different source of uncertainty, namely process
variation, we do not discuss them separately here; see also
\sref{frame-motivation}.

In the case of workload variation, timing analysis has drawn the major attention
\cite{quinton2012}. A seminal work on response time analysis of periodic tasks
with random execution times on uniprocessor systems is reported in
\cite{diaz2002}. A novel analytical solution to this problem is presented in
\cite{tanasa2015}; the solution makes milder assumptions and allows for
addressing larger, previously unsolvable problems. The framework proposed in
\cite{santinelli2011} makes use of real-time calculus in order to facilitate
task scheduling by providing probabilistic bounds on the resource given to a
task flow and the resource required by this task flow. In
\cite{schranzhofer2009}, the authors consider applications characterized by
probabilities of execution and propose a heuristic that searches a design which
has the least expected average power consumption. Independent periodic tasks
with probabilistic execution times are analyzed in \cite{zhu2008}; the work
presents a scheme for optimistic reliability-aware power management targeted at
energy minimization.

Studying the literature on probabilistic analysis of electronic systems related
to this chapter (and \cref{uncertainty-process-development} for that matter),
one can note a pronounced trend: the generality and straightforwardness of
sampling methods tend to be lost. The proposed techniques typically \one~require
restrictive assumptions to be fulfilled, such as the absence of correlation;
\two~are tailored to one concrete quantity of interest, such as the response
time; and \three~require substantial effort in order to be deployed. However,
one should keep in mind what is practical. First, although additional
assumptions might make the mathematics analytically solvable, they often do not
hold in reality and oversimplify the model. An exact analytical solution might
also be extremely complex, requiring a lot of computational resources upon
evaluation. Furthermore, it is often the case that there is a robust simulator
capable of calculating the quantity under consideration in the deterministic
scenario. Switching to probabilistic analysis based on sophisticated techniques
might mean discarding this battle-tested code and starting from scratch, which
is wasteful.

Some of the techniques mentioned earlier, in fact, preserve the generality and
straightforwardness of sampling methods. An example is our probabilistic
framework presented in \cref{uncertainty-process-development}. The reason is
that the construction of \ac{PC} expansions in this framework is undertaken by
means of nonintrusive spectral projections \cite{xiu2010}, which, similarly to
sampling methods, do not need to look inside the ``black box.'' However, as
motivated in \sref{frame-motivation}, nonsmoothness is a serious problem for
global approximation based on polynomials. In particular, the convergence rate
of \ac{PC} expansions deteriorates substantially in such cases, requiring
partitioning the stochastic space in order to alleviate the problem. Therefore,
it is not straightforward to apply such techniques as the one in
\cref{uncertainty-process-development} to sources of uncertainty that exhibit
nonsmoothness.

To conclude, the available techniques for probabilistic analysis of electronic
systems are restricted in use. A flexible and easy-to-deploy framework capable
of addressing nonsmooth uncertainty-quantification problems is needed.
