In this section, we assess the proposed framework. All the experiments are
conducted on a \up{GNU}/Linux machine equipped with 16 Intel Xeon E5520
2.27~\up{GH}z processors and 24~\up{GB} of \up{RAM}. All the source code,
configuration files, and input data used in the experiments are available online
at \cite{eslab2017a}.

We consider three platform sizes \np, two application sizes \nt, and three
quantities of interest \g. Specifically, \np is 2, 4, or 8; \nt is 10 or 20; and
\g is the end-to-end delay, total energy consumption, or maximum temperature.
The three quantities of interest are defined in \eref{frame-delay},
\eref{frame-energy}, and \eref{frame-temperature}, respectively. Therefore, we
address 18 problems in total, which correspond to different combinations of \np,
\nt, and \g. At this point, it might be helpful to recall the illustrative
application depicted in \fref{frame-application}.

Each problem is configured as follows. A platform with \np processing elements
and an application with \nt tasks are randomly generated by \up{TGFF}
\cite{dick1998}. The tool generates a table for each processing element that
specifies certain properties of the tasks when they are mapped to that
processing element. Namely, each table assigns two numbers to each task: a
reference execution time and a power consumption value, which are chosen
uniformly between 10 and 50~ms and between 5 and 25~W, respectively. The
application is scheduled using a list scheduler \cite{adam1974}. The mapping of
the application is fixed and obtained by scheduling the tasks based on their
reference execution times and assigning them to the earliest available
processing elements (a shared ready list).

Similarly to the previous chapters, the construction of thermal \up{RC}
circuits, which are required for temperature analysis, is delegated to HotSpot
\cite{skadron2003}. The floorplan of each platform is a regular grid where each
processing element occupies 4~\power{mm}{2}. The modeling of the static
component of power consumption is based on a linear fit to a set of \up{SPICE}
simulations of a series of \up{CMOS} invertors. The sampling interval \dt of
power and temperature profiles is 1~ms.

The uncertain parameters \vu are the execution times of the tasks, and their
marginal distributions and correlation matrix are as described in
\sref{frame-application-problem}; all other parameters are deterministic.
Regarding the marginal distributions, which are beta distributions as shown in
\eref{beta-distribution}, the left $c$ and right $d$ endpoints of their supports
are set to 80\% and 120\%, respectively, of the reference execution times
generated by \up{TGFF} as described earlier. The parameters $a$ and $b$ are set
to two and five, respectively, for all tasks, which skews the distributions
toward the left endpoints. The model-order-reduction parameter $\eta$ in
\eref{model-order-reduction} is set to 0.9, which results in $\nz = 2$ and $\nz
= 3$ independent variables for applications with $\nt = 10$ and $\nt = 20$
tasks, respectively.

The configuration of the interpolation algorithm---including the collocation
nodes, basis functions, and adaptation strategy with stopping
conditions---closely follows the description given in \sref{frame-construction}.
The parameters \error{a}, \error{r}, and \error{s} are set to around
\power{10}{3}, \power{10}{2}, and \power{10}{4}, respectively, depending on the
problem.

The performance of our framework with respect to each problem is assessed as
follows. First, as in \cref{uncertainty-process-development}, the true
probability distribution of \g is estimated by sampling \g directly and
extensively. Direct sampling means that there is no intermediate representation
or model order reduction involved. Second, we construct an interpolant of \g and
estimate the distribution of \g by sampling this interpolant, which is discussed
in \sref{frame-construction} and \sref{frame-processing}. In both cases, we draw
\power{10}{5} samples; recall, however, that, unlike the cost of direct
sampling, the cost of sampling the interpolant is negligible. Third, we perform
another round of direct sampling. This time, the number of samples taken is
equal to the number of times \g is invoked during the interpolation process.

In each of the aforementioned three cases, sampling is performed in accordance
with a Sobol sequence, which is a low-discrepancy sequence featuring better
convergence properties than those of classical \ac{MC} sampling \cite{joe2008}.
As a result, we obtain three estimates of the probability distribution of the
quantity of interest: reference (the one considered to be exact), proposed (the
one based on our interpolation), and direct (the one equal in terms of the
number of evaluations of \g to the proposed solution). The last two are compared
with the first one. For computing the proximity between two distributions, we
use the well-known \ac{KS} statistic \cite{rao2002}, which is the supremum over
the distance (pointwise) between two empirical distribution functions and is
consequently a stringent error indicator.

\subsection{Approximation Accuracy}

\inputfigure{frame-accuracy-delay}
\inputfigure{frame-accuracy-energy}
\inputfigure{frame-accuracy-temperature}
The results are given in \fref{frame-accuracy-delay},
\fref{frame-accuracy-energy}, and \fref{frame-accuracy-temperature}, which
correspond to the end-to-end delay, total energy, and maximum temperature,
respectively. Each figure contains six plots arranged in a three-by-two grid.
The three rows of the grid correspond to the three platform sizes, and the two
columns to the two application sizes. The horizontal axis of each plot shows the
number of evaluations of the quantity of interest \g, and the vertical one shows
the \ac{KS} statistic on a logarithmic scale. Each plot has two lines. The blue
line represents our technique. The circles on this line correspond to the steps
of the interpolation process shown in \eref{frame-interpolant}. They illustrate
how the \ac{KS} statistic computed with respect to the reference solution
changes as the interpolation process adds nodes to the interpolant until a
stopping condition is satisfied. Note that, in order to make the figure legible,
only a subset of the actual steps is displayed. Synchronously with the blue
line, that is, for the same numbers of evaluations of \g, the orange line shows
the error of direct sampling, which is also calculated with respect to the
reference solution.

We begin by describing one particular problem chosen among those shown in the
three figures. Consider, for instance, the one labeled with $\bigstar$ in
\fref{frame-accuracy-energy}. It can be seen that, at the very beginning, when
the number of evaluations is very small, both the proposed solution and direct
sampling perform poorly. The \ac{KS} statistic indicates that there is a
substantial mismatch between each of these two solutions and the reference one.
However, as the hierarchical interpolant is being adaptively refined, our
solution rapidly approaches the reference one and, by the end of the
interpolation process, leaves the solution produced by direct sampling
approximately an order of magnitude behind.

Studying \fref{frame-accuracy-delay}, \fref{frame-accuracy-energy}, and
\fref{frame-accuracy-temperature}, one can make a number of observations. Our
interpolation-based approach (the blue lines) to probabilistic analysis
outperforms direct sampling (the orange lines) in all the cases. This means
that, given a fixed budget of computation time, the probability distributions
delivered by our framework are closer to the true ones than those delivered by
sampling \g directly, despite the fact that the latter relies on Sobol
sequences, which are a sophisticated sampling strategy. Since sampling methods
try to cover the probability space impartially, the figures are a salient
illustration of the difference between being adaptive and nonadaptive.

It can also be seen in the figures that, as the number of evaluations increases,
the solutions computed by our technique approach the true ones. The error of the
framework generally decreases more steeply than the error of direct sampling.
The decrease, however, tends to plateau toward the end of the interpolation
process (terminated by a stopping condition). The observed behavior has two
potential explanations. First, the algorithm is instructed to satisfy certain
accuracy requirements (given by \error{a}, \error{r}, and \error{s}), and it
reasonably does not do more than what is requested. Second, since model order
reduction is performed in the case of interpolation, the quantity being
interpolated is not \g strictly speaking; it is a low-dimensional representation
of \g, which already implies some information loss. Consequently, there is a
certain limit on the accuracy that can be achieved, which depends on the amount
of reduction.

The message of the above observations is that the designer of an electronic
system can benefit substantially in terms of accuracy per computation time by
switching from direct sampling to the proposed technique. If the designer's
current workhorse is classical \up{MC} sampling, the switch might lead to even
more dramatic savings than those shown in the figures. It is worth mentioning
that the gain is especially prominent in situations where the analysis needs to
be performed many times, such as for the purpose of design-space exploration.

\begin{remark}
The wall-clock time taken by the experiments is not reported here, as this time
is irrelevant: since the evaluation of \g is time consuming (see
\sref{frame-problem}), the number of these evaluations is the most apposite
expense indicator. For the curious reader, however, let us give an example by
considering the problem labeled with $\clubsuit$ in
\fref{frame-accuracy-temperature}. Obtaining a reference solution with
\power{10}{5} samples in parallel on 16 cores takes around two hours.
Constructing an interpolant with 383 collocation nodes takes around 30 seconds
(this is also the time of direct sampling with 383 samples). Sampling the
interpolant \power{10}{5} times takes less than a second. The relative
computation cost of sampling an interpolant readily diminishes as the complexity
of \g increases; contrast it with direct sampling, whose cost grows
proportionally to the evaluation time of \g.
\end{remark}

\subsection{Real-Life Deployment}

Last but not least, we investigate the viability of deploying the proposed
framework in a real environment. This means that we should couple the framework
with a battle-proven simulator, used in both academia and industry, and let this
simulator evaluate a real application running on a real platform.

The scenario that we consider is similar to the one depicted in
\fref{frame-application}. The difference is that an industry-standard simulator
is put in place of the ``black box'' on the left-hand side of the figure, and
that the quantity of interest \g is now the total energy. Unlike the synthetic
examples discussed earlier, there is no true solution against which to compare
due to the prohibitive expense of running the simulator, which is exactly why
our framework is needed.

The chosen simulator is the well-known and widely used combination of Sniper
\cite{carlson2011} and \up{McPAT} \cite{li2009}. The architecture that we
simulate is Intel's Nehalem-based Gainestown series. Sniper is distributed with
a configuration for this architecture, and we utilize this configuration without
any changes. The simulated platform is set up to have three \up{CPU}s sharing
one L3 cache. Regarding the application chosen for this example, it is
\up{VIPS}, which is a piece of image-processing software taken from the
\up{PARSEC} benchmark suite \cite{bienia2011}. In this scenario, \up{VIPS}
applies a fixed set of operations to a given image. The width and height of the
image to process are considered as the uncertain parameters \vu, and they are
assumed to be uniformly distributed within certain ranges.

The deployment of the real-life example has fulfilled our expectations. The
interpolation process has successfully finished and delivered an interpolant
after 78 invocations of the simulator. Each such invocation takes 40 minutes on
average. The probability distribution of the total energy consumption has been
estimated by sampling the constructed surrogate \power{10}{5} times. This number
of samples would take around six months to obtain on our machine if we sampled
the simulator directly in parallel on 16 cores; using the technique presented in
this chapter, the whole procedure has taken approximately nine hours.
