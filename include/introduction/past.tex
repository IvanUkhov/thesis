Since the appearance of the first digital computers in 1940s, \ac{MC} sampling
remains one of the most well-known and widely used methods for analyzing
stochastic systems. In this case, the system at hand is treated as an opaque
object, and one only has to evaluate this object a number of times in order to
start to draw conclusions about the system's behavior. This straightforwardness
of application is at the heart of the technique's popularity, but there are two
other prominent reasons: the independence of the number of stochastic dimensions
and the law of large numbers \cite{durrett2010} stating that the quantities
estimated using \ac{MC} sampling asymptotically approach the true values.

The major problem with sampling-based methods, however, is in sampling \perse:
one has to obtain sufficient many realizations of the quantity of interest in
order to accurately estimate the necessary statistics about this quantity, and
the number of required samples can be considerably large
\cite{diaz-emparanza2002}. In the case of \ac{MC} sampling, for instance, the
error can be halved by quadrupling the number of sample points. In other words,
an additional decimal point of accuracy requires hundred times more samples.
When the subject under analysis is computationally expensive---which is arguably
the case with all nontrivial quantities of interest, as they typically involve a
complete realization of the whole system---sampling methods are rendered slow
and often unfeasible.

There are sampling techniques that have better convergence rates than the one of
the classical \ac{MC} sampling mentioned above. Examples of such techniques
include quasi-\ac{MC} sampling and Latin hypercube sampling \cite{asmussen2007}.
However, these rates are still relatively low, and the corresponding techniques
often bring additional restrictions, which limit their applicability in practice
\cite{xiu2010}.

In order to circumvent the high computational costs of sampling methods and to
address other scenarios, a number of techniques for analysis and design of
electronic systems under uncertainty have been developed. Process variation has
been a topic of many lines of research; see, for instance, \cite{bhardwaj2006,
bhardwaj2008, chandra2010, juan2012, lee2013}. Similarly, workload uncertainty
has not been deprived of attention, especially in the real-time community; see,
for instance, \cite{diaz2002, santinelli2011, quinton2012, tanasa2015}. Aging
uncertainty has also been studied extensively in the literature; see, for
instance, \cite{coskun2006, huang2009b, das2014c}. However, certain problems
have not been addressed yet, and the solutions to those that have been
considered are restricted in use, which is frequently due the unrealistic
assumptions that these solutions make. These concerns are to be discussed in
detail in the relevant parts of the thesis.
