Since the appearance of the first digital computers in the 1940s, \ac{MC}
sampling remains one of the most well-known and widely used methods for
analyzing stochastic systems. With this technique, the system at hand is treated
as an opaque object, and one only has to evaluate this object a number of times
in order to start to draw conclusions about the system's behavior. This
straightforwardness of application is at the heart of the technique's
popularity, but there are two other reasons: the independence of the number of
stochastic dimensions and the law of large numbers \cite{durrett2010}, which
states that the quantities estimated using \ac{MC} sampling asymptotically
approach the true values.

The major problem with sampling-based methods, however, is in sampling \perse:
one has to obtain a sufficiently large number of readings of the quantity of
interest in order to accurately estimate the necessary statistics about this
quantity, and the number of required samples can be considerable
\cite{diaz-emparanza2002}. In the case of \ac{MC} sampling, for instance, the
error can be halved by quadrupling the number of sample points. In other words,
an additional decimal point of accuracy requires a hundred times more samples.
When the subject of the analysis is computationally expensive---which is
arguably the case with all nontrivial quantities of interest, as they typically
involve an evaluation of the whole system---sampling methods are rendered slow
and often unfeasible.

There are sampling techniques that have better convergence rates than that of
classical \ac{MC} sampling. Examples of such more sophisticated techniques
include quasi-\ac{MC} sampling and Latin hypercube sampling \cite{asmussen2007}.
However, these rates are still relatively low, and the corresponding techniques
often have additional restrictions, which limits their applicability in practice
\cite{xiu2010}.

In order to circumvent the high computational costs associated with sampling
methods and to address other scenarios, a number of alternative techniques have
been developed for uncertainty-aware analysis and design of electronic systems.
Process variation has been the topic of many lines of research; see, for
instance, \cite{bhardwaj2006, vrudhula2006, bhardwaj2008, chandra2010, juan2012,
lee2013}. Similarly, workload uncertainty has not been deprived of attention;
see, for instance, \cite{diaz2002, zhu2008, schranzhofer2009, santinelli2011,
quinton2012, tanasa2015}. Aging uncertainty has also been studied extensively in
the literature; see, for instance, \cite{coskun2006, huang2009b, oboril2012,
firouzi2013, kiamehr2013, das2014c}. However, certain important problems have
not been addressed yet, and in the case of the ones that have been considered,
the proposed solutions are often restricted in use, which is due in part to the
unrealistic assumptions that these solutions make. The aforementioned concerns
will be discussed in detail in the relevant parts of the thesis.
