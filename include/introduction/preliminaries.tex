Before we proceed any further, let us first cover a few concepts and notations
that are extensively utilized throughout the thesis.

\subsection{Linear Algebra}
\slab{linear-algebra}

Vectors are denoted by bold lowercase. An $n$-dimensional vector $\v{x}$ in a
vector space such as $\real^n$ can be defined in a number of equivalent ways,
depending on what is important to emphasize in a particular context, as follows:
\begin{align*}
  & \v{x} = (x_i), \\
  & \v{x} = (x_i)_{i = 1}^n, \text{ and} \\
  & \v{x} = (\range{x_1}{x_n}).
\end{align*}
Matrices are denoted by bold uppercase letters. An $n_1 \times n_2$ matrix in a
vector space such as $\real^{n_1 \times n_2}$ can also be defined in several
ways as follows:
\begin{align*}
  & \m{X} = (x_{ij}), \\
  & \m{X} = (x_{ij})_{i = 1, j = 1}^{n_1, n_2}, \text{ and} \\
  & \m{X} = \left(
    \begin{array}{lll}
      x_{1 1}   & \cdots & x_{1 n_2}   \\
      \cdots    & \cdots & \cdots      \\
      x_{n_1 1} & \cdots & x_{n_1 n_2}
    \end{array}
  \right).
\end{align*}

Any symmetric matrix $\m{X} \in \real^{n \times n}$ admits the
eigendecomposition \cite{press2007}, which, in this particular case, takes the
following form:
\begin{equation} \elab{eigendecomposition}
  \m{X} = \m{U} \m{\Lambda} \transpose{\m{U}}
\end{equation}
where $\m{U} \in \real^{n \times n}$ is an orthogonal matrix of the eigenvectors
of $\m{X}$, and
\[
  \m{\Lambda} = \diagonal{\lambda_1}{\lambda_n} = \left(
    \begin{array}{lll}
      \lambda_1 & \cdots & 0         \\
      \cdots    & \cdots & \cdots    \\
      0         & \cdots & \lambda_n
    \end{array}
  \right) \in \real^{n \times n}
\]
is a diagonal matrix of the eigenvalues of $\m{X}$.

\subsection{Probability Theory}
\slab{probability-theory}

Let $(\Omega, \mathcal{F}, \probability)$ be a probability space where $\Omega$
is a set of outcomes, $\mathcal{F} \subseteq 2^\Omega$ is a $\sigma$-algebra on
$\Omega$, and $\probability: \mathcal{F} \to [0, 1]$ is a probability measure
\cite{durrett2010}. The $\sigma$-algebra represents a set of events, and the
probability measure assigns probabilities to these events. A real-valued random
variable defined on $(\Omega, \mathcal{F}, \probability)$ is an
$\mathcal{F}$-measurable function $x: \Omega \to \real$. A random variable $x$
is uniquely characterized by its distribution function---which is also known as
the \ac{CDF}---defined by
\begin{equation*}
  F(x^*) = \probability{x \leq x^*} = \probability{\{ \omega \in \Omega: x(\omega) \leq x^* \}}.
\end{equation*}
The expectation of $x$ is given by
\[
  \expectation{x} = \int_\real x^* dF(x^*) = \int_\Omega x(\omega) dP(\omega).
\]
If $\expectation{x} = 0$, $x$ is called centered. The variance of $x$ is given
by
\[
  \variance{x} = \expectation{(x - \expectation{x})^2}.
\]
If $\expectation{x} = 0$ and $\variance{x} = 1$, $x$ is called standardized.

The above quantities are well defined only when the corresponding integrals are
finite. An example of a space of random variables defined on $(\Omega,
\mathcal{F}, \probability)$ whose expectations and variances are finite is
$\L{2}(\Omega, \mathcal{F}, \probability)$, the Hilbert space of
square-integrable random variables \cite{janson1997} with the inner product
defined by
\[
  \innerproduct{x_1}{x_2} = \expectation{(x_1 x_2)}
\]
and the norm defined by
\[
  \norm{x} = \innerproduct{x}{x}^\frac{1}{2}
\]
for any $x$, $x_1$, and $x_2$ in $\L{2}(\Omega, \mathcal{F}, \probability)$.

If the distribution function $F$ of a random variable $x$ is continuous, $x$ is
said to be a continuous random variable. If, moreover, $F$ is absolutely
continuous, $x$ admits a \ac{PDF} defined by
\[
  f(x) = \frac{dF(x)}{dx}.
\]
Other types of random variables such as discrete random variables are of little
interest to this thesis, and, therefore, they are not covered here.

Suppose now that there is a set of $n$ random variables $\{ x_i \}_{i = 1}^n$.
In order to impose an ordering, the variables are often gathered in a vector
$\v{x} = (x_i)_{i = 1}^n: \Omega \to \real^n$, which is called a random vector
and can also be viewed as a single random variable taking values in $\real^n$.
The variables obey a common distribution called a joint distribution, and the
distribution of any subset of the variables is called a marginal distribution.
The individual variables are called mutually independent if their joint
distribution function $F$ factorizes as follows:
\[
  F(\v{x}) = \prod_{i = 1}^n F_i(x_i)
\]
where $F_i$ is the marginal distribution of $x_i$ for $i = \range{1}{n}$.

Analogously to the one-dimensional case, $\v{x}$ has an expectation and a
variance (if the integrals are finite) as well as a \ac{PDF} (if the \ac{CDF} is
absolutely continuous). In addition, the covariance of $x_i$ and $x_j$ is
defined as follows:
\[
  \covariance{x_i}{x_j} = \expectation{((x_i - \expectation{x_i})(x_j - \expectation{x_j}))}.
\]
If $\covariance{x_i}{x_j} = 0$, $x_i$ and $x_j$ are called uncorrelated.
Finally, the covariance matrix of $\v{x}$ is given by
\begin{equation} \elab{covariance-matrix}
  \covariance{\v{x}} = \left(\covariance{x_i}{x_j}\right)_{i = 1, j = 1}^{n, n},
\end{equation}
which is positive semi-definite by definition.

\subsection{Probability Transformation}
\slab{probability-transformation}

The eigendecomposition in \eref{eigendecomposition} applied to
\eref{covariance-matrix} provides a useful transformation, which is known as the
discrete \ac{KL} decomposition \cite{ghanem1991, xiu2010} and is closely related
to principal component analysis \cite{hastie2013}. The decomposition transforms
$n$ potentially correlated variables $\v{x}$ into $n$ linearly uncorrelated
variables \vz whose variances are given by the corresponding eigenvalues (they
are non-negative since the matrix is positive semi-definite). More concretely,
the relationship between the two vectors is as follows:
\[
  \v{x} = \m{U} \m{\Lambda}^\frac{1}{2} \vz.
\]
When $\v{x}$ obeys a multivariate Gaussian distribution, the individual
variables in \vz follow the standard Gaussian distribution and are independent.

In addition, the decomposition provides a means of model order reduction. In
this case, we are to select the smallest subset of the uncorrelated variables
such that its cumulative contribution to the total variance is above a certain
threshold. Formally, assuming that $\{ \lambda_i: i = \range{1}{n} \}$ are
sorted in the descending order and given a threshold $\eta \in (0, 1]$, which is
the fraction of the total variance to be preserved, we identify the smallest
$\nz \leq n$ such that
\[
  \frac{\sum_{i = 1}^\nz \lambda_i}{\sum_{i = 1}^n \lambda_i} \geq \eta.
\]
Then, given a vector \vz in the reduced space $\real^\nz$, the corresponding
vector $\v{x}$ in the original space $\real^n$ can be lossy reconstructed as
follows:
\begin{equation} \elab{model-order-reduction}
  \v{x} = \m{U} \tm{\Lambda}^\frac{1}{2} \vz.
\end{equation}
In the above formula, $\tm{\Lambda} \in \real^{n \times \nz}$ is a truncated
version of $\m{\Lambda} \in \real^{n \times n}$ given in
\eref{eigendecomposition}, that is, the matrix contains only the first \nz
columns of $\m{\Lambda}$.

\subsection{Numerical Integration}
\slab{numerical-integration}

The integral of a function $g: \real \to \real$
\[
  I = \int_\real g(x) dx
\]
is approximated by a summation of the function's values computed at prescribed
points $\{ \hat{x}_i \in \real \}_{i = 1}^\nq$ and multiplied by prescribed
weights $\{ w_i \in \real \}_{i = 1}^\nq$, which we denote as follows:
\[
  I \approx \quadrature{1}{\lq}{g} = \sum_{i = 1}^\nq g(\hat{x}_i) w_i.
\]
Such a pair of a set of points and a set of weights is called a quadrature rule.
In the notation $\quadrature{1}{\lq}$, the superscript $1$ indicates that it is
a one-dimensional rule, and the subscript \lq gives the accuracy level of the
rule, which is the index of the rule in the corresponding family of quadrature
rules with increasing accuracy. In the one-dimensional case, \lq corresponds to
the maximum order of polynomials that the rule integrates exactly
\cite{heiss2008}. Similarly, in multiple dimensions, the integral of a function
$g: \real^n \to \real$
\[
  I = \int_{\real^n} g(\v{x}) d\v{x}
\]
is approximated by
\[
  I \approx \quadrature{n}{\lq}{g} = \sum_{i = 1}^\nq g(\hat{\v{x}}_i) w_i.
\]
An appropriate $n$-dimensional quadrature rule $\quadrature{n}{\lq}$ is
typically formed by computing the tensor product of $n$ potentially distinct
one-dimensional quadrature rules. The number of terms \nq in summations is
dictated by both $n$ and \lq. Lastly, it is worth noting that quadrature rules
are generally precomputed and tabulated since they do not depend the integrand;
see, for instance, \cite{burkardt}.

Consider now the following more general integral:
\[
  I = \int_{\real^n} g(\v{x}) dF(\v{x}).
\]
Here $g$ is integrated with respect to a measure $F: \real^n \to \real$
\cite{durrett2010} that does not necessarily correspond to the usual Lebesgue
measure used in the earlier examples. If $F$ is absolutely continuous, the
integrand can be rewritten as follows:
\[
  I = \int_{\real^n} g(\v{x}) f(\v{x}) d\v{x}
\]
where $g$ is weighted by the derivative $f: \real^n \to \real$ of $F$ and
integrated with respect to the Lebesgue measure. Since integrating in such a way
is a very frequent operation, there are families of quadrature rules that are
designed to automatically take this aspect into account for the most common
scenarios.

There is one more and arguably the most crucial aspect of numerical integration
that we ought to discuss: the algorithm used to construct multidimensional
quadrature rules. In low dimensions, the construction can be based on the direct
tensor product of one-dimensional rules. However, in high dimensions, the
situation changes dramatically as the number of points produced by this approach
can easily explode. For instance \cite{heiss2008}, if a one-dimensional rule has
only 4 points, that is, $\nq = 4$, then in 10 stochastic dimensions, that is, $n
= 10$, the number of multivariate points becomes $\nq = 4^{10} = 1~048~576$,
which is not affordable. Moreover, it can be shown that most of the points
obtained in this way do not contribute to the asymptotic accuracy and, hence,
are a waste of time. In order to tackle this problem, one resides to sparse
integration grids constructed via the Smolyak algorithm \cite{burkardt,
eldred2008, heiss2008}. The algorithm preserves the accuracy of the underlying
one-dimensional rules for complete polynomials while significantly reducing the
number of points. For instance, in the example given earlier, the number of
points computed by the algorithm is only $\nq = 1~581$, which implies a drastic
saving of the computation time.

\subsection{Polynomial Chaos}
\slab{polynomial-chaos}

Due to the inherent complexity, uncertainty-quantification problems are
typically viewed as approximation problems: one usually constructs a
computationally efficient surrogate for the system under consideration and then
studies this light representation instead of the original system. One way to
construct such a surrogate is \ac{PC} \cite{xiu2010}. The technique decomposes
stochastic quantities into infinite series of mutually orthogonal polynomials of
random variables. Such series are especially attractive from the post-processing
perspective as they are nothing more than the familiar polynomials; therefore,
\ac{PC} expansions are easy to interpret and easy to evaluate.

Let now $\{ \psi_i: \real^n \to \real \}_{i = 1}^\infty$ be a set of $n$-variate
polynomials. The polynomials span a weighted Hilbert space with the inner
product
\begin{equation} \elab{polynomial-inner-product}
  \innerproduct{\psi_i}{\psi_j}
  = \int_{\real^n} \psi_i(\v{x}) \psi_j(\v{x}) dF(\v{x})
  = \int_{\real^n} \psi_i(\v{x}) \psi_j(\v{x}) f(\v{x}) d\v{x}
\end{equation}
where $F$ and $f$ are a measure and a weight function, respectively, similarly
to \sref{numerical-integration}. Polynomials $\psi_i$ and $\psi_j$ are called
orthogonal if
\begin{equation} \elab{polynomial-orthogonality}
  \innerproduct{\psi_i}{\psi_j} = \innerproduct{\psi_i}{\psi_i} \delta_{ij}
\end{equation}
where $\delta_{ij}$ is the Kronecker delta function. The set of polynomials
constitutes a basis of the weighted space if all the polynomials are mutually
orthogonal.

Consider now a probability space $(\Omega, \mathcal{F}, \probability)$, which is
defined in \sref{probability-theory}, and let a function $g: \real^n \to \real$
depend on a random vector $\v{x}: \Omega \to \real^n$ with mutually independent
components. Let also $\{ \psi_i: \real^n \to \real \}_{i = 1}^\infty$ be a basis
whose polynomials are mutually orthogonal with respect to the probability
distribution of $\v{x}$, which means that $F$ and $f$ in
\eref{polynomial-inner-product} are the \ac{CDF} and \ac{PDF} of $\v{x}$,
respectively. Then a \ac{PC} expansion of $g$ as a function of $\v{x}$ is
\[
  g(\v{x}) = \sum_{i = 1}^\infty \hat{g}_i \psi_i(\v{x}).
\]
The coefficients $\{ \hat{g}_i \}_{i = 1}^\infty$ of the expansion are found by
multiplying both side of the equation by $\psi_i$ and making use of
\eref{polynomial-orthogonality}, which results in
\begin{equation} \elab{chaos-projection}
  \hat{g}_i = \frac{\innerproduct{g}{\psi_i}}{\innerproduct{\psi_i}{\psi_i}}
\end{equation}
for $i = \range{1}{\infty}$. This operation is referred to as a spectral
projection. For practical calculations, the expansion has be truncated, which we
denote as
\begin{equation} \elab{chaos-expansion}
  g(\v{x}) \approx \chaos{\nz}{\lc}{g}(\v{x}) = \sum_{i = 1}^\nc \hat{g}_i \psi_i(\v{x}).
\end{equation}
In the notation $\chaos{\nz}{\lc}$, the superscript \nz indicates the
dimensionality of the problem, and the subscript \lc indicates the accuracy
level of the expansion. The two parameters dictate the number of terms \nc that
are preserved as follows:
\begin{equation} \elab{chaos-length-total-order}
  \nc = { \lc + \nz \choose \nz } = \frac{(\lc + \nz)!}{\lc! \, \nz!},
\end{equation}
which corresponds to the total-order polynomial space \cite{eldred2008,
beck2011}.

It is worth noting that, assuming that the polynomials are centered, the inner
product in \eref{polynomial-inner-product} corresponds to the covariance of two
polynomials, and orthogonality corresponds to the absence of correlation. In
addition, the inner product $\innerproduct{\psi_i}{\psi_i}$ in
\eref{polynomial-orthogonality} and \eref{chaos-projection} corresponds to the
variance of a polynomial.

Many of the most popular probability distributions directly correspond to
certain families of orthogonal polynomials, which can be found in the Askey
scheme of orthogonal polynomials \cite{xiu2010}. A distribution that does not
have such a correspondence can be transformed into one of those that do using
such techniques as the one shown in \sref{probability-transformation}. Another
solutions is to construct a custom polynomial basis using the Gram--Schmidt
process. Lastly, let us note that the machinery of \ac{PC} expansions is
applicable to discrete distributions as well. The interested reader is referred
to \cite{xiu2010} for further discussions.

\subsection{Bayesian Statistics}
\slab{bayesian-statistics}

Let \u be a set of unknown parameters that we would like to know. In order to
find them, the following information is at our disposal: \one~a set of
observations $H$ of a quantity \h that is related to \u; \two~a data model that
describes the relation between \u and \h; and \three~prior beliefs about what \u
should be. A natural solution is Bayes' theorem \cite{gelman2004}, which is as
follows:
\begin{equation} \elab{bayes-theorem}
  p(\u | H) \propto p(H | \u) p(\u)
\end{equation}
where $p$ denotes a probability density function. In \eref{bayes-theorem}, $p(H
| \u)$ is known as the likelihood function, which accommodates the data model
and yields the probability of observing the data set $H$ given the parameters
\u; $p(\u)$ is called the prior of \u, which represents our knowledge on \u
prior to any observations; and $p(\u | H)$ is known as the posterior of \u given
$H$. The last is an exhaustive solution to our problem: having constructed $p(\u
| H)$, all the needed information about \u can be trivially estimated by drawing
samples from this posterior.

The posterior distribution does not typically belong to any of the common
families of probability distributions, which is primarily due to the data model
involved in the likelihood function, and, therefore, the sampling procedure is
not straightforward. In order to tackle the difficulty, one usually relies on
Markov chain Monte Carlo sampling \cite{gelman2004}. In this case, an ergodic
Markov chain with the stationary distribution equal to the target posterior
distribution is constructed and then utilized for exploring the probability
space. A popular technique in this regard is the Metropolis--Hastings algorithm
where the chain is constructed via sampling from a computationally convenient
distribution known as the proposal distribution. Each sample drawn from the
proposal is passed through the posterior in order to calculates its posterior
probability, which is then used to decide whether the sample should be accepted
or rejected. A rejection means that the sequence of samples advances using the
last accepted sample as if it was drawn once again. The acceptance strategy of
the algorithm pushes the produced chain of samples toward regions of high
posterior probability, which, after a sufficient number of steps, depending on
the starting point of the chain and the efficiency of the moves, results in an
adequate approximation of the target posterior distribution.
