Before we proceed any further, let us first cover a few concepts and notations
that are extensively utilized throughout the thesis.

\subsection{Linear Algebra}
\slab{linear-algebra}

Vectors are denoted by bold lowercase. An $n$-dimensional vector $\v{a}$ in a
vector space such as $\real^n$ can be defined in a number of equivalent ways,
depending on what is important to emphasize in a particular context, as follows:
\begin{align*}
  & \v{a} = (a_i), \\
  & \v{a} = (a_i)_{i = 1}^n, \text{ and} \\
  & \v{a} = (\range{a_1}{a_n}).
\end{align*}
Matrices are denoted by bold uppercase letters. An $n_1 \times n_2$ matrix in a
vector space such as $\real^{n_1 \times n_2}$ can also be defined in several
ways as follows:
\begin{align*}
  & \m{A} = (a_{ij}), \\
  & \m{A} = (a_{ij})_{i = 1, j = 1}^{n_1, n_2}, \text{ and} \\
  & \m{A} = \left(
    \begin{array}{lll}
      a_{1 1}   & \cdots & a_{1 n_2}   \\
      \cdots    & \cdots & \cdots      \\
      a_{n_1 1} & \cdots & a_{n_1 n_2}
    \end{array}
  \right).
\end{align*}

Any symmetric matrix $\m{A} \in \real^{n \times n}$ admits the
eigendecomposition \cite{press2007}, which, in this particular case, takes the
following form:
\begin{equation} \elab{eigendecomposition}
  \m{A} = \m{U} \m{\Lambda} \transpose{\m{U}}
\end{equation}
where $\m{U} \in \real^{n \times n}$ is an orthogonal matrix of the eigenvectors
of $\m{A}$, and
\[
  \m{\Lambda} = \diagonal{\lambda_1}{\lambda_n} = \left(
    \begin{array}{lll}
      \lambda_1 & \cdots & 0         \\
      \cdots    & \cdots & \cdots    \\
      0         & \cdots & \lambda_n
    \end{array}
  \right) \in \real^{n \times n}
\]
is a diagonal matrix of the eigenvalues of $\m{A}$.

The above decomposition provides a means of model order reduction, which is
closely related to principal component analysis \cite{hastie2013}. In this
setting, $\m{A}$ is a covariance matrix of $n$ potentially correlated variables.
The eigendecomposition transforms these correlated variables into $n$ linearly
uncorrelated variables whose variances are given by the corresponding
eigenvalues, which are non-negative since a covariance matrix is positive
semi-definite by definition. The goal of the reduction is to select the smallest
subset of the uncorrelated variables whose cumulative contribution to the total
variance is above a certain threshold. Formally, assuming that $\{ \lambda_i: i
= \range{1}{n} \}$ are sorted in the descending order and given a threshold
$\eta \in (0, 1]$, which is the fraction of the total variance to be preserved,
we identify the smallest $\nz \leq n$ such that
\[
  \frac{\sum_{i = 1}^\nz \lambda_i}{\sum_{i = 1}^n \lambda_i} \geq \eta.
\]
Then, given a vector \vz in the reduced space $\real^\nz$, the corresponding
vector $\v{x}$ in the original space $\real^n$ can be lossy reconstructed as
follows:
\begin{equation} \elab{model-order-reduction}
  \v{x} = \m{L} \vz
\end{equation}
where
\[
  \m{L} = \m{U} \tm{\Lambda}^\frac{1}{2}.
\]
In the above formula, $\tm{\Lambda} \in \real^{n \times \nz}$ is a truncated
version of $\m{\Lambda} \in \real^{n \times n}$ given in
\eref{eigendecomposition}: the matrix contains only the first \nz columns of
$\m{\Lambda}$.

\subsection{Probability Theory}
\slab{probability-theory}

Let $(\Omega, \mathcal{F}, P)$ be a probability space where $\Omega$ is a set of
outcomes, $\mathcal{F} \subseteq 2^\Omega$ is a $\sigma$-algebra on $\Omega$,
which represents a set of events, and $P: \mathcal{F} \to [0, 1]$ is a
probability measure, which assigns probabilities to the events
\cite{durrett2010}. A random variable defined on $(\Omega, \mathcal{F}, P)$ is
an $\mathcal{F}$-measurable function $x: \Omega \to \real$. Such a variable is
uniquely characterized by its distribution function defined by
\begin{equation*}
  F_x(x^*) = \probability{x \leq x^*} = \probability{\{ \omega \in \Omega: x(\omega) \leq x^* \}}.
\end{equation*}
The expectation of $x$ is given by
\[
  \expectation{x} = \int_\real x^* dF_z(x^*) = \int_\Omega x(\omega) dP(\omega),
\]
and the variance of $x$ is given by
\[
  \variance{x} = \expectation{(x - \expectation{x})^2}.
\]
A random vector and a random matrix are a vector and matrix, respectively, whose
elements are random variables. Lastly, denote by $\L{2}(\Omega, \mathcal{F}, P)$
the Hilbert space of square-integrable random variables \cite{janson1997}
defined on $(\Omega, \mathcal{F}, P)$ with the inner product given by
\[
  \innerproduct{x}{y}_\L{2} = \expectation{x y}
\]
and the norm given by
\[
  \norm[\L{2}]{x} = \innerproduct{x}{x}^\frac{1}{2}
\]
for any $x$ and $y$ in $\L{2}(\Omega, \mathcal{F}, P)$.

\subsection{Bayesian Statistics}
\slab{bayesian-statistics}

Let \u be a set of unknown parameters that we would like to know. In order to
find them, the following information is at our disposal: \one~a set of
observations $H$ of a quantity \h that is related to \u; \two~a data model that
describes the relation between \u and \h; and \three~prior beliefs about what \u
should be. A natural solution is Bayes' theorem \cite{gelman2004}, which is as
follows:
\begin{equation} \elab{bayes-theorem}
  p(\u | H) \propto p(H | \u) p(\u)
\end{equation}
where $p$ denotes a probability density function. In \eref{bayes-theorem}, $p(H
| \u)$ is known as the likelihood function, which accommodates the data model
and yields the probability of observing the data set $H$ given the parameters
\u; $p(\u)$ is called the prior of \u, which represents our knowledge on \u
prior to any observations; and $p(\u | H)$ is known as the posterior of \u given
$H$. The last is an exhaustive solution to our problem: having constructed $p(\u
| H)$, all the needed information about \u can be trivially estimated by drawing
samples from this posterior.

The posterior distribution does not typically belong to any of the common
families of probability distributions, which is primarily due to the data model
involved in the likelihood function, and, therefore, the sampling procedure is
not straightforward. In order to tackle the difficulty, one usually relies on
Markov chain Monte Carlo sampling \cite{gelman2004}. In this case, an ergodic
Markov chain with the stationary distribution equal to the target posterior
distribution is constructed and then utilized for exploring the probability
space. A popular technique in this regard is the Metropolis--Hastings algorithm
where the chain is constructed via sampling from a computationally convenient
distribution known as the proposal distribution. Each sample drawn from the
proposal is passed through the posterior in order to calculates its posterior
probability, which is then used to decide whether the sample should be accepted
or rejected. A rejection means that the sequence of samples advances using the
last accepted sample as if it was drawn once again. The acceptance strategy of
the algorithm pushes the produced chain of samples toward regions of high
posterior probability, which, after a sufficient number of steps, depending on
the starting point of the chain and the efficiency of the moves, results in an
adequate approximation of the target posterior distribution.
