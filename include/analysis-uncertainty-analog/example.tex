Consider the distribution of the effective channel length across a silicon
wafer. The effective channel length, which we denote by \g, has one of the
strongest effects on the leakage current and, consequently, on power and
temperature \cite{juan2012}; see also \sref{power-model}. At the same time, \g
is well known to be severely deteriorated by process variation
\cite{chandrakasan2000, srivastava2010}. Therefore, the distribution of \g is
not uniform across the wafer. For concreteness, let this distribution be the one
depicted on the left-hand side of \fref{inference-example}. The gradient from
blue to yellow represents the transition of \g from low to high values, and the
scale is given in terms of the number of standard deviations away from the mean
value (the exact experimental setup will be described in detail in
\sref{inference-results}). Hence, the blue regions have a high level of the
power consumption and heat dissipation. Assume that the technological process
imposes a lower bound $\g_\minimum$ on \g. This bound separates defective dies
($\g < \g_\minimum$) from those that are acceptable ($\g \geq \g_\minimum$). In
order to reduce costs, the manufacturer is interested in detecting the faulty
dies and taking them out of the production process at early stages. The possible
actions with respect to a single die on the wafer are \one~to keep the die if it
conforms to the specification and \two~to recycle it otherwise.

\inputfigure{inference-example}
In order to analyze the variability of the effective channel length \g across
the wafer, one can remove the top layer of (and, consequently, destroy) the dies
and measure \g directly. Alternatively, despite the fact that the knowledge of
\g is more preferable, one can step back and decide to quantify process
variation using some other parameter \h that can be measured without the need
for damaging the dies; an example is the leakage current. It should be noted
that, in this second case, the chosen surrogate is the final product of the
analysis, and \g is left unknown. In either way, adequate test structures have
to be present on the dies in order to take measurements at sufficiently many
locations with a sufficient level of granularity. Such a sophisticated test
structure might not always be readily available, and its deployment might
significantly increase production expenses. Moreover, the first approach implies
that the measured dies have to be recycled afterwards, and the second implies
that the further design decisions will be based on a surrogate quantity \h
instead of the primary source of uncertainty \g, which can compromise these
decisions. The latter concern is particularly prominent in the situations where
the production process is not yet completely stable, and, consequently, design
decisions based on the primary subjects of process variation are preferable.

Our technique operates differently. In this example, in order to characterize
the effective channel length \g, we begin by measuring an auxiliary quantity \h
as well. The quantity is required to depend on \g, and it can be chosen to be
straightforward from the measurement perspective. The distribution of \g across
the whole wafer is then obtained by inferring it from the collected measurements
of \h. Our technique permits these measurements to be taken only at a small
number of locations on the wafer and to be corrupted by noise, which can be due
to the imperfection of the measurement equipment utilized.

\inputfigure{inference-measured-defective}
Let us consider one particular \h that can be used to study the effective
channel length \g; specifically, let \h be temperature (we elaborate further on
this choice in \sref{inference-results}). We can then apply a fixed workload
---for instance, we can run the same application under the same conditions---to
a few dies on the wafer and measure the corresponding temperature profiles.
Since temperature does not require extra equipment to be deployed on the wafer
and can be tracked using infrared cameras \cite{mesa-martinez2007} or built-in
facilities of the dies, our approach can reduce the costs associated with the
analysis of process variation. The results of our framework applied to a set of
noisy temperature profiles measured for only 7\% of the dies are shown on the
right-hand side of \fref{inference-example}, and the locations of the measured
dies are depicted in \fref{inference-measured}. It can be seen that the two maps
in \fref{inference-example} closely match each other, implying that the
distribution of \g is reconstructed with a high level of accuracy.

Another characteristic of the proposed framework is that probabilities of
various events, for instance, $\probability{\g \geq \g_\minimum}$, can readily
be estimated. This is important since, in reality, the true values are
unknown---otherwise, we would not need to infer them---and, therefore, we can
rely on our decisions only up to a certain probability. We can then reformulate
the decision rule given earlier as follows: \one~to keep the die if
$\probability{\g \geq \g_\minimum}$ is larger than a certain threshold and
\two~to recycle it otherwise. An illustration of following this rule is given in
\fref{inference-defective} where $\g_\minimum$ is set to two standard deviations
below the mean value of \g; the probability threshold of the first action is set
to 0.9; the crosses mark both the true and inferred defective dies (they
coincide); and the gradient from white to orange corresponds to the inferred
probability of a die to be defective. It can be seen that the inference
accurately detects faulty dies.

In addition, we can introduce a trade-off action between action \one and action
\two as follows: \three~to expose the die to a thorough inspection (for
instance, via a test structure) if $\probability{\g \geq \g_\minimum}$ is
smaller than the threshold of action \one but larger than another threshold. For
instance, action \three can be taken if $0.1 < \probability{\g \geq \g_\minimum}
< 0.9$. In this case, we can reduce costs by examining only those dies for which
there is no sufficiently strong evidence of their satisfactory and
unsatisfactory conditions. Furthermore, one can take into consideration a
so-called utility function, which, for each combination of an outcome of \g and
a taken action, returns the gain that the decision maker obtains. For example,
such a function can favor a rare omission of malfunctioning dies to a frequent
inspection of correct dies as the latter might involve much higher costs. The
optimal decision is given by the action that maximizes the expected utility with
respect to both the observed data and prior knowledge on \g. Thus, all possible
\g weighted by their probabilities will be taken into account in the final
decision, incorporating also the preferences of the designer via the utility
function.
