In this chapter, we shift our attention from analyzing process variation to
designing with process variation. In other words, instead of analyzing the
original variability, we now study its impact on higher-level characteristics so
that this deteriorating impact can be taken into account in the final design.

\section{Introduction}
\inputsection{introduction}

\section{Motivational Example}
\slab{chaos-example}
\inputsection{example}

\section{Problem Formulation}
\slab{chaos-problem}
\inputsection{problem}

\section{Prior Work}
\slab{chaos-prior}
\inputsection{prior}

\section{Our Solution}
\slab{chaos-solution}
\inputsection{solution}

\section{Illustrative Application}
\slab{chaos-application}
\inputsection{application}

\section{Experimental Results}
\slab{chaos-result}
\inputsection{result}

\section{Conclusion}
\slab{chaos-conclusion}
\inputsection{conclusion}

\section{Abstract}

Electronic system designs that ignore process variation are unreliable and
inefficient. In this work, we propose a system-level framework for the analysis
of temperature-induced failures that takes into account the uncertainty due to
process variation. As an intermediate step, we also develop a probabilistic
technique for dynamic steady-state temperature analysis. Given an electronic
system under a certain workload, our framework delivers the corresponding
survival function, founded on the basis of well-established reliability models,
with a closed-form stochastic parameterization in terms of the quantities that
are uncertain at the design stage. The proposed solution is exemplified
considering systems with periodic workloads that suffer from the thermal-cycling
fatigue. The analysis of this fatigue is a challenging problem as it requires
the availability of detailed temperature profiles, which are uncertain due to
the variability of process parameters. In order to demonstrate the computational
efficiency of our framework, we undertake a design-space exploration procedure
to minimize the expected energy consumption under a set of timing, thermal, and
reliability constraints.

\section{Introduction}

Process variation constitutes one of the major concerns of electronic system
designs \cite{srivastava2010}. A crucial implication of process variation is
that it renders the key parameters of a technological process, for instance, the
effective channel length, gate oxide thickness, and threshold voltage, as random
quantities at the design stage. Therefore, the same workload applied to two
``identical'' dies can lead to two different power and, thus, temperature
profiles since the dissipation of power and heat essentially depends on the
aforementioned stochastic parameters. This concern is especially urgent due to
the interdependence between the leakage power and temperature \cite{liu2007}.
Consequently, process variation leads to performance degradation in the best
case and to severe faults or burnt silicon in the worst scenario. Under these
circumstances, uncertainty quantification \cite{maitre2010} has evolved into an
indispensable asset of temperature-aware design workflows in order to provide
them with guaranties on the efficiency and robustness of products.

Temperature analysis can be broadly classified into two categories: transient
and steady-state. The latter can be further subdivided into static and dynamic.
Transient temperature analysis is concerned with studying the thermal behavior
of a system as a function of time. Intuitively speaking, the analysis takes a
power curve and delivers the corresponding temperature curve. Static
steady-state temperature analysis addresses the hypothetical scenario in which
the power dissipation is constant, and one is interested in the temperature that
the system will attain when it reaches a static steady state. In this case, the
analysis takes a single value for power (or a power curve which is immediately
averaged out) and outputs the corresponding single value for temperature.
Dynamic steady-state temperature analysis is a combination of the previous two:
it is also targeted at a steady state of the system, but this steady state,
referred to as a dynamic steady state, is now a temperature curve rather than a
single value. The considered scenario is that the system is exposed to a
periodic workload or to such a workload that can be approximated as periodic,
and one is interested in the repetitive evolution of temperature over time when
the thermal behavior of the system stabilizes and starts exhibiting the same
pattern over and over again. Prominent examples here are various multimedia
applications. The input to the analysis is a power curve, and the output is the
corresponding periodic temperature curve. In the absence of uncertainty, this
type of analysis can be efficiently undertaken using the technique developed in
\sref{dynamic-steady-state-solution}.

A typical design task, for which temperature analysis is of central importance,
is temperature-aware reliability analysis and optimization. The crucial impact
of temperature on the lifetime of electronic circuits is well known
\cite{jedec2016}. Examples of the commonly considered failure mechanisms include
electromigration, time-dependent dielectric breakdown, and thermal cycling,
which are directly driven by temperature. Among all failure mechanisms, thermal
cycling has arguably the most prominent dependence on temperature: not only the
average and maximum temperature but also the amplitude and frequency of
temperature oscillations have a huge impact on the lifetime of the circuit. In
this context, the availability of detailed temperature profiles is essential,
which can be delivered by means of either transient or dynamic steady-state
temperature analysis.

Due to the urgent concern originating from process variation, deterministic
temperature analysis and, thus, all procedures based on it are no longer a
viable option for the designer. The presence of uncertainty has to be addressed
in order to pursue efficiency and fail-safeness. In this context, probabilistic
techniques are the way to go, which, however, implies a higher level of
complexity. This paper builds upon the state-of-the-art techniques for
deterministic dynamic steady-state temperature analysis proposed in
\cite{ukhov2012} and probabilistic transient temperature analysis proposed in
\cite{ukhov2014b} and presents a computationally efficient framework for
probabilistic dynamic steady-state temperature analysis and the subsequent
reliability analysis and optimization of electronic systems.

The remainder of the paper is organized as follows. \sref{foo} provides an
overview of the prior work. In \sref{foo}, we summarize the contribution of the
present paper. The preliminaries are given in \sref{foo}. The objective of our
study is formulated in \sref{foo}. The proposed frameworks for uncertainty,
temperature, and reliability analyses are presented in \sref{foo}, \sref{foo},
and \sref{foo}, respectively. An application of the proposed techniques in the
context of reliability optimization is given in \sref{foo}. In \sref{foo}, the
experimental results are reported and discussed. \sref{foo} concludes the paper.
The appendix contains a set of supplementary materials with discussions on
certain aspects of our solutions.

\section{Prior Work}

In this section, an overview of the literature related to our work is given.
First, we discuss those studies that focus on probabilistic temperature
analysis, and then we turn to those that focus on temperature-aware reliability
analysis.

The most straightforward approach to analyze a stochastic system is \acf{MC}
sampling \cite{maitre2010}. The technique is general and has had a tremendous
impact since the middle of the twentieth century when it was introduced. The
success of \ac{MC} sampling is due to the ease of implementation, independence
of the stochastic dimensionality, and asymptotic behavior of the quantities
estimated using this approach. The crucial problem with \ac{MC} sampling,
however, is the low rate of convergence: in order to obtain an additional
decimal point of accuracy, one has to draw usually hundred times more samples.
Each sample implies a complete realization of the whole system, which renders
\ac{MC}-based methods slow and often infeasible as the needed number of
simulations can be extremely large \cite{diaz-emparanza2002}.

In order to overcome the limitations of deterministic temperature analysis and,
at the same time, to completely eliminate or, at least, to mitigate the costs
associated with \ac{MC} sampling, a number of alternative probabilistic
techniques have been introduced. The overwhelming majority of the literature
concerned with temperature relies on static steady-state temperature analysis.
Examples include the work in \cite{lee2013}, which employs stochastic
collocation \cite{maitre2010} as a means of uncertainty quantification, and the
work in \cite{juan2012}, which makes use of the linearity property of Gaussian
distributions and time-invariant systems. The omnipresent assumption about
static temperatures, however, can rarely be justified since power profiles are
not invariant in reality. Nevertheless, the other two types of temperature
analysis, that is, transient and dynamic steady-state, are deprived of
attention. Only recently a probabilistic framework for the characterization of
transient temperature profiles was introduced in \cite{ukhov2014b}; the
framework is based on polynomial chaos expansions \cite{maitre2010}. Regarding
the dynamic steady-state case, to the best of our knowledge, it has not been
studied yet in the literature from the stochastic perspective. However, as
mentioned earlier, the knowledge of dynamic steady-state variations is of
practical importance when designing systems whose workloads tend to be periodic.
In particular, the dynamic steady-state analysis allows the designer to address
the thermal-cycling fatigue, which we illustrate in this paper.

Let us now discuss temperature-aware reliability-driven studies. Reliability
analysis is probabilistic by nature. Certain components of a reliability model,
however, can be treated as either stochastic or deterministic, depending on the
phenomena that the model is designed to account for. Temperature is an example
of such a component: it can be considered as deterministic if the effect of
process variation on temperature is neglected and as stochastic otherwise. The
former scenario is the one that is typically addressed in the literature related
to reliability. For instance, the reliability modeling framework proposed in
\cite{xiang2010} has a treatment of process variation, but temperature is
included in the model as a deterministic quantity. Likewise, the
aging-minimization procedure in \cite{ukhov2012} assumes temperature to be
unaffected by process variation. In \cite{das2014a}, a design methodology
minimizing the energy consumption and temperature-related wear-outs of
multiprocessor systems is introduced; yet neither energy nor temperature is
aware of the uncertainty due to process variation. A similar observation can be
made with respect to the work in \cite{das2014c} wherein a reinforcement
learning algorithm is used to improve the lifetime of multiprocessor systems. An
extensive and up-to-date survey on reliability-aware system-level design
techniques given in \cite{das2014b} confirms the trend outlined above: the
widespread device-level models of failure mechanisms generally ignore the impact
of process variation on temperature. However, as motivated in the introduction,
deterministic temperature is a strong assumption that can lead to substantial
yield losses.

An example of a different kind is the work in \cite{lee2013}: it provides a
statistical simulator for reliability analysis under process variations and does
consider temperature as a stochastic parameter. However, as discussed
previously, this study is bound to static steady-state temperatures, and the
presented reliability analysis is essentially an analysis of maximum
temperatures without any relation to the typical failure mechanisms
\cite{jedec2016}.

To conclude, the designer's toolbox in our field does not yet include a tool for
dynamic steady-state temperature analysis under process variation, which is of
high importance for certain classes of applications mentioned previously.
Furthermore, the state-of-the-art reliability models lack a flexible approach
for taking the effect of process variation on temperature into consideration.
This work eliminates the aforementioned concerns.

\section{Our Contributions}

Our work brings the following major contributions:
First, based on the stochastic approach to transient temperature analysis
presented in \cite{ukhov2014b}, we extend the deterministic dynamic steady-state
temperature analysis presented in \cite{ukhov2012} to account for the
uncertainty due to process variation. Second, we develop a framework for the
reliability analysis of electronic systems that enriches the state-of-the-art
reliability models by taking into consideration the effect of process variation
on temperature. Third, we construct a computationally efficient design-space
exploration procedure targeted at the minimization of the energy consumption,
which is \emph{a priori} random, under probabilistic constraints on the thermal
behavior and lifetime of the system.

\section{Preliminaries}

Let $(\Omega, \mathcal{F}, \probability)$ be a complete probability space, where
$\Omega$ is a set of Omega, $\mathcal{F} \subseteq 2^\Omega$ is a
$\sigma$-algebra on $\Omega$, and $\probability: \mathcal{F} \to [0, 1]$ is a
probability measure \cite{durrett2010}. A random variable on $(\Omega,
\mathcal{F}, \probability)$ is an $\mathcal{F}$-measurable function $x: \Omega
\to \real$. A random variable $x$ is uniquely characterized by its (cumulative)
distribution function defined by
\begin{equation*}
  F_x(y) := \probability(\{ \omega \in \Omega: x(\omega) \leq y \}).
\end{equation*}
The expectation and variance of $x$ are given by
\begin{align*}
  & \expectation{x} := \int_\Omega x(\omega) \, \d \probability(\omega) = \int_\real y \, \d F_x(y) \hspace{1em} \text{and} \\
  & \variance{x} := \expectation{(x - \expectation{x})^2},
\end{align*}
respectively. A random vector $\v{x} = (x_i)$ and matrix $\m{X} = (x_{ij})$ are
a vector and matrix whose elements are random variables. Denote by
$\L{2}(\Omega, \mathcal{F}, \probability)$ the Hilbert space of
square-integrable random variables \cite{janson1997} defined on $(\Omega,
\mathcal{F}, \probability)$ with the inner product and norm defined,
respectively, by
\begin{equation*}
  \innerproduct{x_1}{x_2} := \expectation{x_1 x_2} \hspace{1em} \text{and} \hspace{1em}
  \norm{x} := \innerproduct{x}{x}^{1/2}.
\end{equation*}
In what follows, all the random variables will tacitly have $(\Omega,
\mathcal{F}, \probability)$ as the underlying probability space.

\section{Problem Formulation}

Consider a heterogeneous electronic system that consists of \np processing
elements and is equipped with a thermal package. The processing elements are the
active components of the system that are identified at the system level with a
desired level of granularity (for instance, \up{SoC}s, \up{CPU}s, and
\up{ALU}s); the components can be subdivided whenever a finer level of modeling
is required for the problem at hand.

We shall denote by \spec an abstract set containing all information
about the system that is relevant to our analysis, and we shall refer to it as
the system specification. The content of \spec is problem specific,
and it will be gradually detailed when it is needed. For now, \spec
is assumed to include: (a) the floorplan of the chip; (b) the geometry of the
thermal package; and (c) the thermal parameters of the materials that the chip
and package are made of (for instance, silicon thermal conductivity).

A power profile is defined as a matrix $\mp \in \real^{\np \times \ns}$
containing \ns samples of the power dissipation of the processing elements. The
samples correspond to certain moments of time $(t_k)_{k = 1}^\ns$ that partition
a time interval $[0, \period]$ as
\[
  0 = t_0 < t_1 < \dotsc < t_\ns = \period.
\]
Analogously, a temperature profile $\mq \in \real^{\np \times \ns}$ is a matrix
containing samples of temperature. For clarity, power and temperature profiles
are assumed to have a one-to-one correspondence and a constant sampling interval
\dt, that is, $t_k - t_{k - 1} = \dt$, for $k = 1, 2, \dots, \ns$. In what
follows, a power profile that contains only the dynamic component of the (total)
power dissipation will be denoted by $\mp_\dynamic$.

The system depends on a set of parameters that are uncertain at the design stage
due to process variation. We model such parameters using random variables and
denote them by a random vector $\vu = (\u_i)_{i = 1}^\nu: \Omega \to \real^\nu$.
In this work, we are only concerned with those parameters that manifest
themselves in the deviation of the actual power dissipation from nominal values
and, consequently, in the deviation of temperature from the one corresponding to
the nominal power consumption.

Given \spec, we pursue the following major objectives:
\begin{itemize}

  \item Objective~1. Extend probabilistic temperature analysis to include the
  dynamic steady-state scenario under the uncertainty due to process variation
  specified by \vu.

  \item Objective~2. Taking into consideration the effect of process variation
  on temperature, find the survival function of the system at hand under an
  arbitrary workload given as a dynamic power profile $\mp_\dynamic$.

  \item Objective~3. Develop a computationally tractable design-space
  exploration scheme exploiting the proposed framework for
  temperature/reliability analysis.

\end{itemize}

In order to give a better intuition about our solutions, we shall accompany the
development of our framework with the development of a concrete
example/application, which will eventually be utilized for the quantitative
evaluation of the framework given in \sref{experimental-results}. To this end,
we have decided to focus on two process parameters, which are arguably the most
crucial ones, namely, the effective channel length $L$ and the gate-oxide
thickness $T$. In this example,
\[
  \vu = (L, T): \Omega \to \real^2,
\]
which is to be discussed in detail shortly. Regarding reliability, we shall
address the thermal-cycling fatigue as it is naturally connected with dynamic
steady-state temperature analysis that we develop.

\section{Uncertainty Analysis}

The key building block of our solutions developed in the subsequent section is
the uncertainty quantification technique presented in this section. The main
task of this technique is the propagation of uncertainty through the system,
that is, from a set of inputs to a set of outputs. Specifically, the inputs are
the uncertain parameters \vu, and the outputs are the quantities that we are
interested in studying. The latter can be, for instance, the energy consumption,
maximum temperature, or temperature profile of the system over a certain period
of time.

Due to the inherent complexity, uncertainty quantification problems are
typically viewed as approximation problems: one first constructs a
computationally efficient surrogate for the stochastic model under consideration
and then studies this computationally efficient representation instead of the
original model. In order to construct such an approximation, we appeal to
spectral methods \cite{maitre2010, janson1997, eldred2008}.

\subsection{Uncertainty Model}

Before we proceed to the construction of light surrogate models, let us first
refine our definition of $\vu = (\u_i)_{i = 1}^\nu$. Each $\u_i$ is a
characteristic of a single transistor (consider, for instance, the effective
channel length), and, therefore, each device in the electrical circuits at hand
can potentially have a different value of this parameter as, in general, the
variability due to process variation is not uniform. Consequently, each $\u_i$
can be viewed as a random process $\u_i: \Omega \times D \to \real$ defined on
an appropriate spatial domain $D \subset \real^2$. Since this work is
system-level oriented, we model each processing element with one variable for
each such random process. More specifically, we let $\u_{ij} = \u_i(\cdot,
\v{r}_j)$ be the random variable representing the $i$th uncertain parameter at
the $j$th processing element where $\v{r}_j$ stands for the spatial location of
the center of the processing element. Therefore, we redefine the
parameterization \vu of the problem at hand as
\[
  \vu = (\u_i)_{i = 1}^{\nu \np}
\]
such that there is a one-to-one correspondence between $\u_i$, $i = 1, 2, \dots,
\nu \np$, and $\u_{ij}$, $i = 1, 2, \dots, \nu$, $j = 1, 2, \dots, \np$. For
instance, in our illustrative application with two process parameters, the total
number of stochastic dimensions is $2 \np$.

\begin{remark}
Some authors prefer to split the variability of a process parameter at a spatial
location into several parts such as wafer-to-wafer, die-to-die, and within-die;
see, for instance, \cite{juan2012}. However, from the mathematical point of
view, it is sufficient to consider just one random variable per location which
is adequately correlated with the other locations of interest.
\end{remark}

A description of \vu is an input to our analysis given by the user, and we
consider it to be a part of the system specification \spec. A proper (complete,
unambiguous) way to describe a set of random variables is to specify their joint
probability distribution function. In practice, however, such exhaustive
information is often unavailable, in particular, due to the high dimensionality
in the presence of prominent dependencies inherent to the considered problem. A
more realistic assumption is the knowledge of the marginal distributions and
correlation matrix of \vu. Denote by $\{ F_{\u_i} \}_{i = 1}^{\nu \np}$ and
$\correlation{\vu} \in \real^{\nu \np \times \nu \np}$ the marginal distribution
functions and correlation matrix of the uncertain parameters \vu in
\eref{uncertain-parameters}, respectively. Note that the number of distinct
marginals is only \nu since \np components of \vu correspond to the same
uncertain parameter.

\subsection{Parameter Preprocessing}

Our foremost task now is to transform \vu into mutually independent random
variables as independence is essential for the forthcoming mathematical
treatment and practical computations. To this end, an adequate probability
transformation should be undertaken depending on the available information; see
\cite{eldred2008} for an overview. One transformation for which the assumed
knowledge about \vu is sufficient is the Nataf transformation \cite{li2008}.
Denote this transformation by
\[
  \vu = \transform{\vz},
\]
which relates $\nu \np$ dependent random variables, that is, \vu, with $\nz =
\nu \np$ independent random variables
\[
  \vz = (\z_i)_{i = 1}^\nz.
\]
Regardless of the marginals, $\z_i \sim \mathrm{Gaussian}{0, 1}$, $i = 1, 2,
\dots, \nz$, that is, each $\z_i$ has the standard Gaussian distribution. Refer
to \sref{probability-transformation} for further details about the Nataf
transformation.

As we shall discuss later on, the stochastic dimensionality \nz has a
considerable impact on the computational complexity of our framework. Therefore,
an important part of the preprocessing stage is model order reduction. To this
end, we preserve only those stochastic dimensions whose contribution to the
total variance of \vu is the most significant, which is identified by the
eigenvalues of the correlation matrix $\correlation{\vu}$:
\[
  \v{\lambda} = (\lambda_i)_{i = 1}^{\nu \np}, \hspace{1em}
  \norm[1]{\v{\lambda}} = 1,
\]
as it is further discussed in \sref{model-order-reduction}. Without introducing
additional transformations, we let $\transform$ in
\eref{probability-transformation} be augmented with such a reduction procedure
and redefine $\vz \in \real^\nz$ as the reduced independent random variables
where $\nz \leq \nu \np$. We would like to note that this procedure is highly
preferable as it helps to keep \nz moderate, and it is especially advantages
when refining the granularity of the analysis (see \sref{problem-formulation}).

Let us turn to the illustrative application. Recall that we exemplify our
framework considering the effective channel length and gate-oxide thickness with
the notation given in \eref{application-uncertain-parameters}. Both parameters
correspond to Euclidean distances; they take values on bounded intervals of the
positive part of the real line. With this in mind, we model the two process
parameters using the four-parametric family of beta distributions:
\[
  \u_i \sim F_{\u_i} = \mathrm{Beta}(a_i, b_i, c_i, d_i)
\]
where $i = 1, 2, \dots, 2 \np$, $a_i$ and $b_i$ control the shape of the
distributions, and $[c_i, d_i]$ correspond to their supports. Without loss of
generality, we let the two considered process parameters be independent of each
other, and the correlations among those elements of \vu that correspond to the
same process parameter be given by the following correlation function:
\[
  k(\v{r}_i, \v{r}_j) = w \; k_\SE(\v{r}_i, \v{r}_j) + (1 - w) k_\OU(\v{r}_i, \v{r}_j)
\]
where $\v{r}_i \in \real^2$ is the center of the $i$th processing element
relative to the center of the die. The correlation function is a composition of
two kernels:
\begin{align*}
  & k_\SE(\v{r}_i, \v{r}_j) = \exp\left(- \frac{\norm{\v{r}_i - \v{r}_j}^2}{\ell_\SE^2} \right) \text{ and} \\
  & k_\OU(\v{r}_i, \v{r}_j) = \exp\left(- \frac{\absolute{\norm{\v{r}_i} - \norm{\v{r}_j}}}{\ell_\OU} \right),
\end{align*}
which are known as the squared-exponential and Ornstein--Uhlenbeck kernels,
respectively. In the above formulae, $w \in [0, 1]$ is a weight coefficient
balancing the kernels; $\ell_\SE$ and $\ell_\OU > 0$ are so-called length-scale
parameters; and $\norm{\cdot}$ stands for the Euclidean norm in $\real^2$. The
choice of these two kernels is guided by the observations of the correlation
patterns induced by the fabrication process: $k_\SE$ imposes similarities
between those spatial locations that are close to each other, and $k_\OU$
imposes similarities between those locations that are at the same distance from
the center of the die; see, for instance, \cite{friedberg2005} for additional
details. The length-scale parameters $\ell_\SE$ and $\ell_\OU$ control the
extend of these similarities, that is the range wherein the influence of one
point on another is significant.

\subsection{Surrogate Construction}

Let $\g: \Omega \to \real$ be a quantity of interest dependent on \vu. For
convenience, \g is assumed to be one-dimensional, which will be generalized
later on. In order to give a computationally efficient probabilistic
characterization of \g, we utilize nonintrusive spectral decompositions based on
orthogonal polynomials. The corresponding mathematical foundation is outlined in
\sref{spectral-decomposition} and \sref{numerical-integration}, and here we go
directly to the main results obtained in those sections.

\subsubsection{Classical Decomposition}

Assume $\g \in \L{2}(\Omega, \mathcal{F}, \probability)$ (see
\sref{preliminaries}). Then \g can be expanded into the following series:
\[
  \g \approx \chaos{\nz}{\lc}{\g} := \sum_{\multiindex \in
  \multiindices{\lc}} \hat{\g}_{\multiindex} \, \phi_{\multiindex}(\vz)
\]
where \lc is the expansion level; $\multiindex = (\alpha_i) \in \natural^\nz$ is
a multi-index; $\multiindices{\lc}$ is an index set to be discussed shortly; and
$\phi_{\multiindex}(\vz)$ is an \nz-variate Hermite polynomial constructed as a
product of normalized one-dimensional Hermite polynomials of orders specified by
the corresponding elements of \multiindex.

As discussed in \sref{spectral-decomposition}, each coefficient
$\hat{\g}_{\multiindex}$ in \eref{spectral-decomposition} is an
$\nz$-dimensional integral of the product of \g with $\phi_{\multiindex}$, and
this integral should be computed numerically. To this end, we construct a
quadrature rule and calculate $\hat{\g}_{\multiindex}$ as
\begin{equation} \elab{numerical-integration}
  \hat{\g}_{\multiindex} \approx \quadrature{\nz}{\lq}{\g \,
  \phi_{\multiindex}} := \sum_{i = 1}^\nq \g(\transform{\v{x}_i}) \, \phi_{\multiindex}(\v{x}_i) \, w_i
\end{equation}
where $\lq$ is the quadrature level, and $\{ (\v{x}_i \in \real^\nz, w_i \in
\real) \}_{i = 1}^\nq$ are the points and weights of the quadrature. The
multivariate quadrature operator $\quadrature{\nz}{\lq}$ is based on a set of
univariate operators and is constructed as follows:
\begin{equation} \elab{smolyak-sparse-grid}
  \quadrature{\nz}{\lq} = \bigoplus_{\multiindex \in \multiindices{\lq}}
  \Delta_{\alpha_1} \otimes \cdots \otimes \Delta_{\alpha_\nz}.
\end{equation}
The notation used in the above equation is not essential for the present
discussion and is explained in \sref{numerical-integration}. The important
aspect to note is the structure of this operator, namely, the index set
$\multiindices{\lq}$, which we shall come back to shortly.

The standard choice of $\multiindices{\lc}$ in \eref{spectral-decomposition} is
$\{ \multiindex: \norm[1]{\multiindex} \leq \lc \}$, which is called an
isotropic total-order index set. \emph{Isotropic} refers to the fact that all
dimensions are trimmed identically, and \emph{total-order} refers to the
structure of the corresponding polynomial space. In
\eref{numerical-integration}, $\phi_{\multiindex}$ is a polynomial of total
order at most \lc, and \g is modeled as such a polynomial. Hence, the integrand
in \eref{numerical-integration} is a polynomial of total order at most $2 \lc$.
Having this aspect in mind, one usually constructs a quadrature rule such that
it is exact for polynomials of total order $2 \lc$ \cite{eldred2008}. In this
work, we employ Gaussian quadratures for integration, in which case a quadrature
of level $\lq$ is exact for integrating polynomials of total order $2 \lq + 1$
\cite{heiss2008} (see also \sref{numerical-integration}). Therefore, it is
sufficient to keep \lc and \lq equal. More generally, the index sets
$\multiindices{\lc}$ and $\multiindices{\lq}$ should be synchronized; in what
follows, we shall denote both by $\multiindices{l}$.

\subsubsection{Anisotropic Decomposition}

In the context of sparse grids, an important generalization of the construction
in \eref{smolyak-sparse-grid} is the so-called anisotropic Smolyak algorithm
\cite{nobile2008}. The main difference between the isotropic and anisotropic
versions lies in the constraints imposed on $\multiindices{l}$. An anisotropic
total-order index set is defined as follows:
\begin{equation} \elab{anisotropic-total-order-index-set}
  \multiindices{l} = \left\{ \multiindex: \innerproduct{\v{c}}{\multiindex} \leq l \, \min_i c_i \right\}
\end{equation}
where $\v{c} = (c_i) \in \real^\nz$, $c_i \geq 0$, is a vector assigning
importance coefficients to each dimension, and $\innerproduct{\cdot}{\cdot}$ is
the standard inner product on $\real^\nz$. Equation
\eref{anisotropic-total-order-index-set} plugged into \eref{smolyak-sparse-grid}
results in a sparse grid which is exact for the polynomial space that is
tailored using the same index set.

The above approach allows one to leverage the highly anisotropic behaviors
inherent for many practical problems \cite{nobile2008}. It provides a great
control over the computational time associated with the construction of spectral
decompositions: a carefully chosen importance vector $\v{c}$ in
\eref{anisotropic-total-order-index-set} can significantly reduce the number of
polynomial terms in \eref{spectral-decomposition} and the number of quadrature
points needed in \eref{numerical-integration} to compute the coefficients of
those terms. The question to discuss now is the choice of $\v{c}$. In this
regard, we rely on the variance contributions of the dimensions given by
$\v{\lambda}$ in \eref{dimension-contribution}. Specifically, we let
\begin{equation} \elab{dimension-anisotropy}
  \v{c} = \v{\lambda}^\gamma := (\lambda_i^\gamma)_{i = 1}^\nz
\end{equation}
where $\gamma \in [0, 1]$ is a tuning parameter. The isotropic scenario can be
recovered by setting $\gamma = 0$; the other values of $\gamma$ correspond to
various levels of anisotropy with the maximum attained by setting $\gamma = 1$.

Let us sum up what we have achieved at this point. In order to give a
probabilistic characterization of a quantity of interest, we perform polynomial
expansions as shown in \eref{spectral-decomposition}. The coefficients of such
expansions are evaluated by means of Gaussian quadratures as shown in
\eref{numerical-integration}. The quadratures are constructed using the Smolyak
formula given in \eref{smolyak-sparse-grid}. The index sets used in both
\eref{spectral-decomposition} and \eref{smolyak-sparse-grid} are the one given
in \eref{anisotropic-total-order-index-set} wherein the anisotropic weights are
set according to \eref{dimension-contribution} and \eref{dimension-anisotropy}.

\subsubsection{Efficient Implementation}

The pair of \vz and $\v{c}$ uniquely characterizes the uncertainty
quantification problem at hand. Once they have been identified, and the desired
approximation level $l = \lc = \lq$ has been specified, the corresponding
polynomial basis and quadrature stay the same for all quantities that one might
be interested in studying. This observation is of high importance as a lot of
preparatory work can and should be done only once and then stored for future
uses. In particular, the construction in \eref{spectral-decomposition} can be
reduced to one matrix multiplication with a precomputed matrix, which we shall
demonstrate next.

Let $\nc = \cardinality{\multiindices{l}}$ be the cardinality of
$\multiindices{l}$, which is also the number of polynomial terms and, hence,
coefficients in \eref{spectral-decomposition}. Assume the multi-indices
contained in $\multiindices{l}$ are arranged in a vector $(\multiindex_i)_{i =
1}^\nc$, which gives a certain ordering. Now, let
\[
  \Pi = \big( \pi_{ij} = \phi_{\multiindex_i}(\v{x}_j) \, w_j \big)_{i = 1, \, j = 1}^{i = \nc, \, j = \nq},
\]
that is, $\pi_{ij}$ is the polynomial corresponding to the $i$th multi-index
evaluated at the $j$th quadrature point and multiplied by the $j$th quadrature
weight. We refer to $\Pi$ as the projection matrix. The coefficients in
\eref{spectral-decomposition} can now be computed as
\[
  \hat{\v{v}} = \Pi \, \v{v}
\]
where
\[
  \hat{\v{v}} = (\hat{\g}_i)_{i = 1}^\nc \hspace{1em} \text{and} \hspace{1em}
  \v{v} = \big(\g(\transform{\v{x}_i}) \big)_{i = 1}^\nq.
\]
It can be seen that \eref{coefficient-evaluation} is a matrix version of
\eref{numerical-integration}. $\Pi$ is the one that should be precomputed. The
pseudocode of the procedure is given in \aref{surrogate-construction} wherein
Algorithm~X stands for the routine that calculates \g for a given \vu. Needless
to say, Algorithm~X is problem specific and has a crucial impact on the
performance of the whole procedure presented in this section: any modeling
errors inherent to this algorithm can propagate to the output of the uncertainty
analysis. Algorithm~X will be further discussed in
\sref{temperature-analysis}--\sref{reliability-optimization}.

\subsection{Post-Processing}

The function given by \eref{spectral-decomposition} is nothing more than a
polynomial; hence, it is easy to interpret and easy to evaluate. Consequently,
having constructed such an expansion, various statistics about \g can be
estimated with little effort. Moreover, \eref{spectral-decomposition} yields
analytical formulae for the expected value and variance of \g solely based on
the coefficients of \eref{spectral-decomposition}:
\[
  \expectation{\g} = \hat{\g}_{\v{0}} \hspace{1em} \text{and} \hspace{1em}
  \variance{\g} = \sum_{\multiindex \in \multiindices{l} \setminus \{ \v{0} \}} \hat{\g}_{\multiindex}^2
\]
where $\v{0} = (0)$ is a multi-index with all entries equal to zero. Such
quantities as the cumulative distribution and probability density functions can
be estimated by sampling \eref{spectral-decomposition}; each sample is a trivial
evaluation of a polynomial.

\begin{remark}
When \g is multidimensional, we shall consider it as a row vector with an
appropriate number of elements. Then all the operations with respect to \g, such
as those in \eref{spectral-decomposition}, \eref{numerical-integration}, and
\eref{probabilistic-moments}, should be undertaken elementwise. In
\eref{coefficient-evaluation}, \eref{quantity-evaluation}, and
\aref{surrogate-construction}, $\v{v}$ and $\hat{\v{v}}$ are to be treated as
matrices with \nc rows, and $\g_i$ as a row vector. The output of Algorithm~X is
assumed to be automatically reshaped into a row vector.
\end{remark}

In what follows, we shall apply the probabilistic analysis developed in this
section to a number of concrete problems: temperature analysis
(\sref{temperature-analysis}), reliability analysis
(\sref{reliability-analysis}), and reliability optimization
(\sref{reliability-optimization}).

\section{Temperature Analysis}

In this section, we detail our temperature analysis, which is suitable for
system-level studies. We shall cover both the transient and dynamic steady-state
scenarios as the former is a prerequisite for the latter. Since temperature is a
direct consequence of power, we begin with the power model utilized in the
proposed framework.

\subsection{Power Model}

Recall that the system is composed of \np processing elements and depends on the
outcome of the probability space $\omega \in \Omega$ via \vu. The total
dissipation of power is modeled as the following system of \np temporal
stochastic process:
\[
  \vp(t, \vu, \vq) = \vp_\dynamic(t) + \vp_\static(\vu, \vq)
\]
where, for time $t \geq 0$, $\vp_\dynamic \in \real^\np$ and $\vp_\static \in
\real^\np$ are vectors representing the dynamic and static components of the
total power, respectively, and $\vq \in \real^\np$ is the corresponding vector
of temperature. $\vp_\dynamic$ is deterministic, and the rest are random.

\begin{remark}
In \eref{power-model}, \textnormal{$\vp_\dynamic$} has no dependency on \vu as
the influence of process variation on the dynamic power is known to be
negligibly small \cite{srivastava2010}. On the other hand, the variability of
\textnormal{$\vp_\static$} is substantial and is further magnified by the
well-known interdependency between leakage and temperature.
\end{remark}

\subsection{Thermal Model}

Based on the information gathered in \spec (see \sref{problem-formulation}), an
equivalent thermal \up{RC} circuit of the system is constructed
\cite{skadron2003}. The circuit comprises \nn thermal nodes, and its structure
depends on the intended level of granularity that impacts the resulting
accuracy. For clarity, we assume that each processing element is mapped onto one
corresponding node, and the thermal package is represented as a set of
additional nodes.

The thermal dynamics of the system are modeled using the following system of
differential-algebraic equations \cite{ukhov2012, ukhov2014}:
\begin{subnumcases}{\elab{thermal-model}}
  \frac{\d \vs(t, \vu)}{\d t} = \m{A} \: \vs(t, \vu) + \m{B} \: \vp(t, \vu, \vq(t, \vu)) \elab{thermal-model-inner} \\
  \vq(t, \vu) = \m{B}^T \vs(t, \vu) + \vq_\ambient \elab{thermal-model-outer}
\end{subnumcases}
where
\[
  \m{A} = -\m{C}^{-\frac{1}{2}} \m{G} \m{C}^{-\frac{1}{2}} \hspace{1em} \text{and} \hspace{1em}
  \m{B} = \m{C}^{-\frac{1}{2}} \m{M}.
\]
For time $t \geq 0$, $\vp \in \real^\np$, $\vq \in \real^\np$, and $\vs \in
\real^\nn$ are the power, temperature, and state vectors, respectively.
$\vq_\ambient \in \real^\np$ is a vector of the ambient temperature. $\m{M} \in
\real^{\nn \times \np}$ is a matrix that distributes the power dissipation of
the processing elements across the thermal nodes; without loss of generality,
$\m{M}$ is a rectangular diagonal matrix wherein each diagonal element is equal
to unity. $\m{C} \in \real^{\nn \times \nn}$ and $\m{G} \in \real^{\nn \times
\nn}$ are a diagonal matrix of the thermal capacitance and a symmetric,
positive-definite matrix of the thermal conductance, respectively.

\subsection{Our Solution}

Let us fix $\omega \in \Omega$, meaning that \vu is assumed to be known, and
consider the system in \eref{thermal-model} as deterministic. In general,
\eref{thermal-model-inner} is a system of ordinary differential equations which
is nonlinear due to the power term, given in \eref{power-model} as an arbitrary
function. Hence, the system in \eref{thermal-model} does not have a general
closed-form solution. A robust and computationally efficient solution to
\eref{thermal-model} for a given \vu is an essential part of our probabilistic
framework. In order to attain such a solution, we utilize a numerical method
from the family of exponential integrators \cite{hochbruck2010}. The procedure
is described in \aref{temperature-solution}, and here we use the final result.

Recall that we are to analyze a dynamic power profile $\mp_\dynamic$ covering a
time interval $[0, \period]$ with $\ns$ samples that are evenly spaced in
time. The transient solution of \eref{thermal-model-inner} is reduced to the
following recurrence for $k = 1, 2, \dots, \ns$:
\[
  \vs_k = \m{E} \: \vs_{k - 1} + \m{F} \: \vp_k
\]
where the subscript $k$ stands for time $k \dt$, $\vs_0 = \v{0}$,
\[
  \m{E} = e^{\m{A} \dt}, \hspace{1em} \text{and} \hspace{1em}
  \m{F} = \m{A}^{-1} (e^{\m{A} \dt} - \m{I}) \: \m{B}.
\]
For computational efficiency, we perform the eigendecomposition of the state
matrix $\m{A}$:
\[
  \m{A} = \m{V} \m{\Lambda} \m{V}^T
\]
where $\m{V}$ and $\m{\Lambda} = \diagonal{\lambda_i}{}$ are an
orthogonal matrix of the eigenvectors and a diagonal matrix of the eigenvectors
of $\m{A}$, respectively. The matrices $\m{E}$ and $\m{F}$ are then
\begin{align*}
  & \m{E} = \m{V} \; \diagonal{e^{\lambda_i \dt}}{} \m{V}^T \text{ and} \\
  & \m{F} = \m{V} \; \diagonal{\frac{e^{\lambda_i \dt} - 1}{\lambda_i}}{} \m{V}^T \m{B}.
\end{align*}
To sum up, the derivation up to this point is sufficient for transient
temperature analysis via \eref{recurrence} followed by
\eref{thermal-model-outer}.

\begin{remark}
Although the focus of this paper is on temperature, each temperature analysis
developed is accompanied by the corresponding power analysis as the two are
inseparable due to the leakage-temperature interplay. Consequently, when it is
appropriate, one can easily extract only the (temperature-aware) power part of
the presented solutions.
\end{remark}

Let us move on to the dynamic steady-state case. Assume for now that
$\vp_\static$ in \eref{power-model} does not depend on \vq, that is, no
interdependency between leakage and temperature. The dynamic steady-state
boundary condition is $\vs_1 = \vs_{\ns + 1}$. In other words, the system is
required to come back to its original state by the end of the analyzed time
frame. This constraint and \eref{recurrence} yield a block-circulant system of
$\nn \ns$ linear equations with $\nn \ns$ unknowns. As describe in detail in
\cite{ukhov2012}, the system can be efficiently solved by exploiting its
particular structure and the decomposition in \eref{eigendecomposition}. The
pseudocode of this algorithm, which delivers the exact solution under the above
assumptions, is given in \aref{temperature-solution-without-leakage}. Here we
adopt \up{MATLAB}'s \cite{matlab} notations $\m{A}(k, :)$ and $\m{A}(:, k)$ to
refer to the $k$th row and the $k$th column of a matrix $\m{A}$, respectively.
In the pseudocode, auxiliary variables are written with hats, and $\mq_\ambient$
is a matrix of the ambient temperature.

\begin{remark}
The time complexity of  direct dense and sparse solvers (for instance, the LU
decomposition) of the system of linear equations is $\bigo{\ns^3 \nn^3}$ while
the one of \aref{temperature-solution-without-leakage} is only $\bigo{\ns \nn^2
+ \nn^3}$, which the algorithm is able to achieve by exploiting the specific
structure of the system; see \cite{ukhov2012}.
\end{remark}

Let us now bring the leakage-temperature interdependence into the picture. To
this end, we repeat \aref{temperature-solution-without-leakage} for a sequence
of total power profiles $\{ \mp_k = \mp_\dynamic + \mp_{\static, k} \}$ wherein
the static part $\mp_{\static, k}$ is being updated using \eref{power-model}
given the temperature profile $\mq_{k - 1}$ computed at the previous iteration
starting from the ambient temperature. The procedure stops when the sequence of
temperature profiles $\{ \mq_k \}$ converges in an appropriate norm, or some
other stopping condition is satisfied (for instance, a maximum temperature
constraint is violated). This procedure is illustrated in
\aref{temperature-solution}.

In \aref{temperature-solution}, $\mp_\static(\vu, \mq)$ should be understood as
a call to a subroutine that returns an $\np \times \ns$ matrix wherein the $(i,
k)$th element is the static component of the power dissipation of the $i$th
processing element at the $k$th moment of time with respect to \vu and the
temperature given by the $(i, k)$th entry of \mq.

\begin{remark}
A widespread approach to account for leakage is to linearize it with respect to
temperature. As shown in \cite{liu2007}, already one linear segment can deliver
sufficiently accurate results. One notable feature of such a linearization is
that no iterating-until-convergence is needed in this case; see
\cite{ukhov2012}. However, this technique assumes that the only varying
parameter of leakage is temperature, and all other parameters have nominal
values. In that case, it is relatively easy to decide on a representative
temperature range and undertake a one-dimensional curve-fitting procedure with
respect to it. In our case, the power model has multiple parameters stepping far
from their nominal values, which makes it difficult to construct a good linear
fit with respect to temperature. Thus, in order to be accurate, we use a
nonlinear model of leakage.
\end{remark}

So far in this subsection, \vu has been assumed to be deterministic. Now we turn
to the stochastic scenario and let \vu be random. Then we apply
\aref{surrogate-construction} to one particular quantity of interest \g.
Specifically, \g is now the temperature profile \mq corresponding to a given
$\mp_\dynamic$. Since \mq is an $\np \times \ns$ matrix, following
\rref{multiple-dimensions}, \g is viewed as an $\np \ns$-element row vector, in
which case each coefficient $\hat{\g}_{\multiindex}$ in
\eref{spectral-decomposition} is also such a vector. The projection in
\eref{coefficient-evaluation} and, consequently, \aref{surrogate-construction}
should be interpreted as follows: $\v{v}$ is an $\nq \times \np \ns$ matrix, and
the $i$th row of this matrix is the temperature profile computed at the $i$th
quadrature point and reshaped into a row vector. Similarly, $\hat{\v{v}}$ is an
$\nc \times \np \ns$ matrix, and the $i$th row of this matrix is the $i$th
coefficient $\hat{\g}_{\multiindex_i}$ of the spectral decomposition in
\eref{spectral-decomposition} (recall that a fixed ordering is assumed to be
imposed on the multi-indices). Keeping the above in mind, a call to
\aref{surrogate-construction} should be made such that Algorithm~X points at an
auxiliary routine which receives \vu, forwards it to \aref{temperature-solution}
along with $\mp_\dynamic$, and returns the resulting temperature profile to
\aref{surrogate-construction}. The constructed expansion can now be
postprocessed as needed; see \sref{postprocessing}.

To give a concrete example, for a dual-core system ($\np = 2$) with one
independent random variable ($\nz = 1$), a second-level expansion ($\lc = 2$) of
the temperature profile with 100 time steps ($\ns = 100$) can be written as
follows (see \eref{spectral-decomposition}):
\[
  \g \approx \hat{\g}_0 + \hat{\g}_1 \z + \hat{\g}_2(\z^2 - 1),
\]
which is a polynomial in \z, and each coefficient is a vector with $\np \ns =
200$ elements. Then, for any outcome $\z \equiv \z(\o)$, the corresponding
temperature profile \mq can be evaluated by plugging in \z into the above
equation and reshaping the result into an $\np \times \ns = 2 \times 100$
matrix. In this case, the three rows of $\hat{\v{v}}$ are $\hat{\g}_0$,
$\hat{\g}_1$, and $\hat{\g}_2$; the first one is also a flattened version of the
expected value of $\mq$ as shown in \eref{probabilistic-moments}.

\section{Reliability Analysis}

In this section, our primary objective is to build a flexible and
computationally efficient framework for the reliability analysis of electronic
systems affected by process variation. Let us begin with a description of a
generic reliability model and make several observations with respect to it.

\subsection{Reliability Model}

Let $T: \Omega \to \real$ be a random variable representing the lifetime of the
considered system. The lifetime is the time span until the system experiences a
fault after which the system no longer meets the imposed requirements. Let
$F_T(\cdot | \vg)$ be the distribution of $T$ where $\vg = (\g_i)$ is a vector
of parameters. The survival function of the system is
\[
  S_T(t | \vg) = 1 - F_T(t | \vg).
\]

The overall lifetime $T$ is a function of the lifetimes of the processing
elements, which are denoted by a set of random variables $\{ T_i \}_{i =
1}^\np$. Each $T_i$ is characterized by a physical model of wear
\cite{jedec2016} describing the fatigues that the corresponding processing
element is exposed to. Each $T_i$ is also assigned an individual survival
function $S_{T_i}(\cdot | \vg)$ describing the failures due to those fatigues.
The structure of $S_T(\cdot | \vg)$ with respect to $\{ S_{T_i}(\cdot | \vg)
\}_{i = 1}^\np$ is problem specific, and it can be especially diverse in the
context of fault-tolerant systems. $S_T(\cdot | \vg)$ is to be specified by the
designer of the system, and it is assumed to be included in the specification
\spec (see \sref{problem-formulation}). To give an example, suppose the failure
of any of the \np processing elements makes the system fail, and $\{ T_i \}_{i =
1}^\np$ are conditionally independent given the parameters gathered in \vg. In
this scenario,
\[
  T = \min_{i = 1}^\np T_i \hspace{1em} \text{and} \hspace{1em}
  S_T(t | \vg) = \prod_{i = 1}^\np S_{T_i}(t | \vg).
\]

Our work in this context is motivated by the following two observations. First,
temperature is the driving force of the dominant failure mechanisms. The most
prominent examples include electromigration, time-dependent dielectric
breakdown, stress migration, and thermal cycling \cite{xiang2010}; see
\cite{jedec} for an exhaustive overview. All of the aforementioned mechanisms
have strong dependencies on the operating temperature, which is taken into
account by considering the parameters in \vg as adequate functions of
temperature. At the same time, temperature is tightly related to process
parameters, such as the effective channel length and gate-oxide thickness, and
can vary dramatically when those parameters deviate from their nominal values
\cite{ukhov2014, juan2012}. Meanwhile, the state-of-the-art techniques for
reliability analysis of electronic systems lack a systematic treatment of
process variation and, in particular, of the effect of process variation on
temperature.

Second, having determined a probabilistic model $S_T(\cdot | \vg)$ of the
considered system, the major portion of the associated computational time is
ascribed to the evaluation of the parameterization \vg rather than to the model
\emph{per se}, that is, when \vg is known. For instance, \vg often contains
estimates of the mean time to failure of each processing element given for a
range of stress levels. Therefore, \vg typically involves (computationally
intensive) full-system simulations including power analysis paired with
temperature analysis \cite{xiang2010}.

\begin{remark}
It is important to realize that there are two levels of probabilistic modeling
here. First, the reliability model \emph{per se} is a probabilistic model
describing the lifetime of the system. Second, the parameterization \vg is
another probabilistic model characterizing the impact of the uncertainty due to
process variation on the reliability model. Consequently, the overall model can
be thought of as a probability distribution over probability distributions.
Given an outcome of the fabrication process, that is, \vg, the lifetime remains
random.
\end{remark}

\subsection{Our Solution}

Guided by the aforementioned observations, we propose to use the spectral
decompositions developed in \sref{uncertainty-analysis} and
\sref{temperature-analysis} in order to construct a light surrogate for \vg. The
proposed technique is founded on the basis of the state-of-the-art reliability
models by enriching their modeling capabilities with respect to process
variation and by speeding up the associated computational process. This approach
allows one to seamlessly incorporate into reliability analysis the effect of
process variation on process parameters. In particular, the framework allows for
a straightforward propagation of the uncertainty from process parameters through
power and temperature to the lifetime of the system. In contrast to the
straightforward use of \ac{MC} sampling, the spectral representation that we
construct makes the subsequent analysis highly efficient from the computational
perspective.

It is worth noting that $S_T(\cdot | \vg)$ is left intact, meaning that our
approach does not impose any restrictions on $S_T(\cdot | \vg)$. Thus, the user
can take advantage of various reliability models in a straightforward manner.
Naturally, this also implies that the modeling errors associated with the chosen
$S_T(\cdot | \vg)$ can affect the quality of the results delivered by our
technique. Therefore, choosing an adequate reliability model for the problem at
hand is a responsibility of the user.

Let us now apply our general technique to address one of the major concerns of
the designer of electronic systems: the thermal-cycling fatigue
\cite{jedec2016}. This fatigue has a sophisticated dependency on temperature:
apart from average/maximum temperatures, the frequencies and amplitudes of
temperature fluctuations matter in this case. Suppose that the system at hand is
experiencing a periodic workload due to the execution of a periodic or nearly
periodic application with period \period. The power consumption is changing
during the execution of the application, and, thus, the system is inevitably
exposed to the damage from thermal oscillations. The corresponding temperature
profile \mq is then a dynamic steady-state profile, which, for a given \vu, can
be computed using \aref{temperature-solution}.

Assume further that the structure of the reliability model is the one shown in
\eref{reliability-model}. Regarding the individual survival functions, we shall
rely on Weibull distributions. In this case,
\[
  \ln S_{T_i}(t | \vq) = -\left(\frac{t}{\eta_i}\right)^{\beta_i}
\]
and the mean time to failure is
\[
  \mu_i = \expectation{T_i} = \eta_i \, \Gamma\left(1 + \frac{1}{\beta_i}\right)
\]
where $\eta_i$ and $\beta_i$ are the scale and shape parameters of the
distribution, respectively, and $\Gamma$ is the gamma function. At this point,
$\vg = (\eta_1, \dotsc, \eta_\np, \beta_1, \dotsc, \beta_\np)$.

During one iteration of the application, the temperature of the $i$th processing
element exhibits $\nc_i$ cycles. Each cycle generally has different
characteristics and, therefore, causes different damage to the system. This
aspect is taken into account by adjusting $\eta_i$ as follows. Let \mq be the
dynamic steady-state temperature profile of the system under analysis and denote
by $\mq(i, :)$ the $i$th row of \mq, which corresponds to the temperature curve
of the $i$th processing element. First, $\mq(i, :)$ is analyzed using a
peak-detection procedure in order to extract the extrema of this curve. The
found extrema are then fed to the rainflow counting algorithm \cite{xiang2010}
for an adequate identification of thermal cycles. Denote by $\nc_{ij}$ the
expected number of cycles to failure corresponding to the $i$th processing
element and its $j$th cycle (as if it was the only cycle damaging the processing
element). $\nc_{ij}$ is computed using the corresponding physical model of wear
that can be found in \cite{ukhov2012, jedec, xiang2010}. Let $\eta_{ij}$ and
$\mu_{ij}$ be the scale parameter and expectation of the lifetime corresponding
to the $i$th processing element under the stress of the $j$th cycle; the two are
related as shown in \eref{weibull-expectation}. Then \cite{ukhov2012, xiang2010}
\[
  \eta_i = \frac{\period}{\Gamma\left(1 + \frac{1}{\beta_i}\right) \sum_{j = 1}^{\nc_i} \frac{1}{\nc_{ij}}}.
\]
Note that $\eta_i$ accounts for process variation via temperature (in the above
equation, $\nc_{ij}$ is a function of temperature).

\begin{remark}
A cycle need not be formed by adjacent extrema; cycles can overlap. In this
regard, the rainflow counting method is known to be the best as it efficiently
mitigates overestimation. A cycle can be a half cycle, meaning that only an
upward or downward temperature swing is present in the time series, which is
assumed to be taken into account in $\nc_{ij}$.
\end{remark}

The shape parameter $\beta_i$ is known to be indifferent to temperature. For
simplicity, we also assume that $\beta_i$ does not depend on process parameters
and $\beta_i = \beta$ for $i = 1, 2, \dotsc, \np$. However, we would like to
emphasize that these assumptions are not a limitation of the proposed
techniques. Then it can be shown that the compositional survival function
$S_T(\cdot | \vg)$ corresponds to a Weibull distribution, and the shape
parameter of this distribution is $\beta$ whereas the scale parameter is given
by the following equation:
\[
  \eta = \left(\sum\left(\frac{1}{\eta_i}\right)^\beta\right)^{-\frac{1}{\beta}}
\]
where $\eta_i$ is as in \eref{weibull-eta}. Consequently, the parameterization
of the reliability model has boiled down to two parameters, $\eta$ and $\beta$,
among which only $\eta$ is random.

Now we let the scale parameter $\eta$ be our quantity of interest \g and apply
the technique in \sref{uncertainty-analysis} to this quantity. In this case,
Algorithm~X in \aref{surrogate-construction} is an auxiliary function that makes
a call to \aref{temperature-solution}, processes the resulting temperature
profile as it was described earlier in this subsection, and returns $\eta$
computed according to the formula in \eref{compound-weibull-eta}. Consequently,
we obtain a light polynomial surrogate of the parameterization of the
reliability model, which can be then studied from various perspectives. The
example for a dual-core system given at the end of \sref{temperature-solution}
can be considered in this context as well with the only change that the
dimensionality of the polynomial coefficients would be two here (since $\eta \in
\real^\np$ and $\np = 2$).

\section{Reliability Optimization}

In this section, the proposed analysis techniques are applied in the context of
design-space exploration.

\subsection{Problem Formulation}

Consider a periodic application which is composed of a number of tasks and is
given as a directed acyclic graph. The graph has \nt vertices representing the
tasks and a number of edges specifying data dependencies between those tasks.
Any processing element can execute any task, and each pair of a processing
element and a task is characterized by an execution time and dynamic power.
Since the proposed techniques are orientated towards the design stage, static
scheduling is considered, which is typically done offline. More specifically,
the application is scheduled using a static cyclic scheduler, and schedules are
generated using the list scheduling policy \cite{adam1974}. A schedule is
defined as a mapping of the tasks onto the processing elements and the
corresponding starting times; we shall denote it by \schedule. The goal of our
optimization is to find such a schedule \schedule that minimizes the energy
consumption while satisfying certain constraints.

Since energy is a function of power, and power depends on a set of uncertain
parameters, the energy consumption is a random variable at the design stage,
which we denote by $E$. Our objective is to minimize the expected value of $E$:
\[
  \min_{\schedule} \expectation{E(\schedule)}
\]
where
\[
  E(\schedule) = \dt \sum \mp(\schedule),
\]
\dt is the sampling interval of the power profile \mp, and $\sum \mp$ denotes
the summation over all elements of \mp. Hereafter, we also emphasize the
dependency on \schedule. Our constraints are (i) time, (ii) temperature, and
(iii) reliability as follows. (i) The period of the application is constrained
by $t_\maximum$ (a deadline). (ii) The maximum temperature that the system can
tolerate is constrained by $\q_\maximum$, and $\rho_\burn$ is an acceptable
probability of burning the chip. (iii) The minimum time that the system should
survive is constrained by $T_\minimum$, and $\rho_\wear$ is an acceptable
probability of having a premature fault due to wear. The three constraints are
formalized as follows:
\begin{align}
  & \period(\schedule) \leq t_\maximum, \elab{timing-constraint} \\
  & \probability\left(Q(\schedule) \geq \q_\maximum\right) \leq \rho_\burn, \text{ and} \elab{thermal-constraint} \\
  & \probability\left(T(\schedule) \leq T_\minimum\right) \leq \rho_\wear. \elab{reliability-constraint}
\end{align}
In \eref{timing-constraint}--\eref{reliability-constraint}, $\period$ is the
period of the application according to the schedule,
\begin{align*}
  & Q(\schedule) = \norm[\infty]{\mq(\schedule)}, \\
  & T(\schedule) = \expectation{T(\schedule) \, | \, \eta} = \eta(\schedule) \, \Gamma\left(1 + \frac{1}{\beta}\right), \text{ and}
\end{align*}
$\norm[\infty]{\mq}$ denotes the extraction of the maximum value from the
temperature profile \mq. The last two constraints, that is,
\eref{thermal-constraint} and \eref{reliability-constraint}, are probabilistic
as the quantities under consideration are random. In
\eref{reliability-constraint}, we set an upper bound on the probability of the
expected value of $T$, and it is important to realize that this expectation is a
random variable itself due to the nested structure of the reliability model
described in \rref{two-level-probabilistic-modeling}.

\subsection{Our Solution}

In order to evaluate \eref{objective}--\eref{reliability-constraint}, we utilize
the uncertainty analysis technique presented in \sref{uncertainty-analysis}. In
this case, the quantity of interest is a vector with three elements:
\[
  \vq = (E, Q, T).
\]
Although it is not spelled out, each quantity depends on \schedule. The first
element corresponds to the energy consumption used in \eref{objective}, the
second element is the maximum temperature used in \eref{thermal-constraint}, and
the last one is the scale parameter of the reliability model (see
\sref{reliability-analysis}) used in \eref{reliability-constraint}. The
uncertainty analysis in \sref{uncertainty-analysis} should be applied as
explained in \rref{multiple-dimensions}. In \aref{surrogate-construction},
Algorithm~X is an intermediate procedure that makes a call to
\aref{temperature-solution} and processes the resulting power and temperature
profiles as required by \eref{quantity-of-interest}.

We use a genetic algorithm for optimization. Each chromosome is a $2
\nt$-element vector (twice the number of tasks) concatenating a pair of two
vectors. The first is a vector in $\{ 1, 2, \dotsc, \np\}^\nt$ that maps the
tasks onto the processing elements (that is, a mapping). The second is a vector
in $\{ 1, 2, \dotsc, \nt \}^\nt$ that orders the tasks according to their
priorities (that is, a ranking). Since we rely on a static cyclic scheduler and
the list scheduling policy \cite{adam1974}, such a pair of vectors uniquely
encodes a schedule \schedule. The population contains $4 \nt$ individuals which
are initialized using uniform distributions. The parents for the next generation
are chosen by a tournament selection with the number of competitors equal to
20\% of \nt. A one-point crossover is then applied to 80\% of the parents. Each
parent undergoes a uniform mutation wherein each gene is altered with
probability 0.01. The top five-percent individuals always survive. The stopping
condition is the absence of improvement within 10 successive generations.

Let us turn to the evaluation of a chromosome's fitness. We begin by checking
the timing constraint given in \eref{timing-constraint} as it does not require
any probabilistic analysis; the constraint is purely deterministic. If
\eref{timing-constraint} is violated, we set the fitness to the amount of this
violation relative to the constraint---that is, to the difference between the
actual application period and the deadline $t_\maximum$ divided by
$t_\maximum$---and add a large constant, say, $C$, on top. If
\eref{timing-constraint} is satisfied, we perform our probabilistic analysis and
proceed to checking the constraints in \eref{thermal-constraint} and
\eref{reliability-constraint}. If any of the two is violated, we set the fitness
to the total relative amount of violation plus $C / 2$. If all the constraints
are satisfied, the fitness value of the chromosome is set to the expected
consumption of energy, as in shown in \eref{objective}.

In order to speed up the optimization, we make use of caching and parallel
computing. Specifically, the fitness value of each evaluated chromosome is
stored in memory and pulled out when a chromosome with the same set of genes is
encountered, and unseen (not cached) individuals are assessed in parallel.

\section{Experimental Results}

In this section, we evaluate the performance of the proposed techniques. All the
experiments are conducted on a \up{GNU}/Linux machine equipped with 16
processors Intel Xeon E5520 2.27~\up{GH}z and 24~\up{GB} of \up{RAM}. Parallel
computing is utilized only in the experiments reported in
\sref{experimental-results-optimization}.

\subsection{Configuration}

We consider a 45-nm technological process and rely on the 45-nm standard-cell
library published and maintained by NanGate \cite{nangate}. The effective
channel length and gate-oxide thickness are assumed to have nominal values equal
to 22.5~nm and 1~nm, respectively. Following the information about process
variation reported by \up{ITRS} \cite{itrs}, we assume that each process
parameter can deviate up to 12\% of its nominal value, and this percentage is
treated as three standard deviations. The corresponding probabilistic model is
the one described in \sref{preprocessing}. Regarding the correlation function in
\eref{correlation-function}, the weight coefficient $w$ is set to 0.5, and the
length-scale parameters $\ell_\SE$ and $\ell_\OU$ are set to half the size of
the die (see the next paragraph). The model order reduction is set to preserve
95\% of the variance of the problem (see also \sref{model-order-reduction}). The
tuning parameter $\v{c}$ in \eref{dimension-anisotropy} is set to 0.25.

Heterogeneous platforms and periodic applications are generated randomly using
\up{TGFF} \cite{dick1998} in such a way that the execution time of each task is
uniformly distributed between 10 and 30~ms, and its dynamic power between 6 and
20~W. The floorplans of the platforms are regular grids wherein each processing
element occupies $2 \times 2\,\text{mm}^2$. Thermal RC circuits---which are
essentially pairs of a thermal capacitance matrix $\m{C}$ and a thermal
conductance $\m{G}$ matrix needed in the equations given in
\sref{thermal-model}---are constructed using the HotSpot thermal model
\cite{skadron2004}. The granularity of power and temperature profiles, that is,
\dt in \sref{temperature-solution} and \sref{temperature-solution}, is set to
1~ms; in practice, \dt should be set to a value that is reasonable for the
problem at hand. The stopping condition in \aref{temperature-solution} is a
decrease of the normalized root-mean-square error between two successive
temperature profiles smaller than 1\%, which typically requires 3--5 iterations.

The leakage model needed for the calculation of $\mp_\static(\vu, \mq)$ in
\aref{temperature-solution} is based on \up{SPICE} simulations of a series of
\up{CMOS} invertors taken from the NanGate cell library and configured according
to the high-performance 45-nm \up{PTM} \cite{ptm}. The simulations are performed
on a fine-grained and sufficiently broad three-dimensional grid comprising the
effective channel length, gate-oxide thickness, and temperature; the results are
tabulated. The interpolation facilities of \up{MATLAB} \cite{matlab} are then
utilized whenever we need to evaluate the leakage power for a particular point
within the range of the grid. The output of the constructed leakage model is
scaled up to account for about 40\% of the total power dissipation
\cite{liu2007}.

\subsection{Probabilistic Analysis}

Our objective here is to study the accuracy and speed of the proposed solutions.
Since the optimization procedure described in \sref{reliability-optimization}
embraces all the techniques developed throughout the paper, we shall perform the
assessment directly in the design-space-exploration context. In other words, we
do not consider temperature analysis or reliability analysis as a separate
uncertainty quantification problem in our experiments and shall focus on the
quantity of interest given in \eref{quantity-of-interest}. This quantity plays
the key role as the objective function in \eref{objective} and the constraints
in \eref{thermal-constraint} and \eref{reliability-constraint} are entirely
based on it.

We shall compare our performance with the performance of \ac{MC} sampling. The
operations performed by the \ac{MC}-based approach for one sample are exactly
the same as those performed by our technique for one quadrature point. The only
difference is that no reduction of any kind is undertaken prior to \ac{MC}
sampling. In other words, the \ac{MC}-based approach samples the \emph{original}
model and, hence, does not compromise any resulting accuracy. The number of
\ac{MC} samples is set to $10^4$, which is a practical assumption that conforms
to the experience from the literature \cite{ukhov2014, lee2013, juan2012,
xiang2010} and to the theoretical estimates given in \cite{diaz-emparanza2002}.
Hence, we consider this setup of \ac{MC} sampling to be a paragon of accuracy.

The results concerning accuracy are displayed in \tref{accuracy} where we
consider a quad-core platform, that is, $\np = 4$, with ten randomly generated
applications and vary the level of polynomial expansions \lc from one to five.
The errors for the three components of $\vq = (E, Q, T)$ are denoted by
$\epsilon_E$, $\epsilon_Q$, and $\epsilon_T$, respectively. Each error indicator
shows the distance between the empirical probability distributions produced by
our approach and the ones produced by \ac{MC}\ sampling, and the measure of this
distance is the popular Kullback--Leibler divergence (\up{KLD}) wherein the
results of \ac{MC}\ sampling are treaded as the ``true'' ones. The \up{KLD}
takes nonnegative values and attains zero only when two distributions are equal
almost everywhere \cite{durrett2010}. In general, the errors decrease as \lc
increases. This trend, however, is not monotonic for expansions of high levels
(see $\epsilon_Q$ and $\epsilon_T$ for $\lc = 5$). The observation can be
ascribed to the random nature of sampling and the fact that the reduction
procedures, which we undertake to gain speed, might impose limitations on the
accuracy that can be attained by polynomial expansions. \tref{accuracy} also
contains the numbers of polynomial terms \nc and quadrature points \nq
corresponding to each value of \lc. We also performed the above experiment for
platforms with fewer/more processing elements; the observations were similar to
the ones in \tref{accuracy}.

Based on \tref{accuracy}, we consider the results delivered by third-level
polynomial expansions, where the \up{KLD} drops to the third decimal place for
all quantities, to be sufficiently accurate, and, therefore, we fix $\lc = \lq =
l = 3$ (recall the notation in the last paragraph of
\sref{classical-decomposition}) for the rest of the experiments.

\tref{speed} displays the time needed to perform one characterization of \vq for
the number of processing elements \np swept from 2 to 32. It can be seen that
the computational time ranges from a fraction of a second to around two seconds.
More importantly, \tref{speed} provides information about a number of
complementary quantities that are of high interest for the user of the proposed
techniques, which we discuss below.

The primary quantity to pay attention to is the number of random variables \nz
preserved after the reduction procedure described in \sref{preprocessing} and
\sref{model-order-reduction}. Without this reduction, \nz would be $2 \np$ as
there are two process parameters per processing element. It can be seen that
there is no reduction for the dual-core platform while around 80\% of the
stochastic dimensions have been eliminated for the platform with 32 cores. In
addition, one can note that \nz is the same for the last two platforms. The
magnitude of reduction is solely determined by the correlation patterns assumed
(see \sref{preprocessing}) and the floorplans of the considered platforms.

Another important quantity displayed in \tref{speed} is the number of quadrature
nodes \nq. This number is the main indicator of the computational complexity of
our probabilistic analysis: it equals to the number of times Algorithm~X in
\aref{surrogate-construction} is executed to construct a polynomial expansion of
\eref{quantity-of-interest} needed for the evaluation of the fitness function.
It can be seen that \nq is very low. To illustrate this, the last column of
\tref{speed} shows the speedup of our approach with respect to $10^4$ \ac{MC}.
Our solution is faster by approximately 100--200 times while delivering highly
accurate results as discussed earlier. It should be noted that the comparison
has been drawn based on the number of evaluation points rather than on the
actual time since the relative cost of other computations is negligible.

To conclude, the proposed solutions to temperature and reliability analyses
under process variation have been assessed using the compositional quantity of
interest given in \eref{quantity-of-interest}. The results shown in
\tref{accuracy} and \tref{speed} allow us to conclude that our approach is both
accurate and computationally efficient.

\subsection{Probabilistic Optimization}

In this subsection, we report the results of the optimization procedure
formulated in \sref{reliability-optimization}. To reiterate, the objective is to
minimize energy as shown in \eref{objective} while satisfying a set of
constraints on the application period, maximum temperature, and minimum lifetime
as shown in \eref{timing-constraint}, \eref{thermal-constraint}, and
\eref{reliability-constraint}, respectively. We employ a genetic algorithm for
optimization. The population is evaluated in parallel using 16 processors; this
job is delegated to the parallel computing toolbox of \up{MATLAB} \cite{matlab}.

The goal of this experiment is to justify the following assertion: reliability
analysis has to account for the effect of process variation on temperature. To
this end, for each problem (a pair of a platform and an application), we shall
run the optimization procedure twice: once using the setup that has been
described so far and once making the objective in \eref{objective} and the
constraints in \eref{thermal-constraint} and \eref{reliability-constraint}
deterministic. To elaborate, the second run assumes that temperature is
deterministic and can be computed using the nominal values of the process
parameters. Consequently, only one simulation of the system is needed in the
deterministic case to evaluate the fitness function, and \eref{objective},
\eref{thermal-constraint}, and \eref{reliability-constraint} become,
respectively,
\[
  \min_{\schedule} E(\schedule), \hspace{0.7em}
  Q(\schedule) \geq \q_\maximum, \hspace{0.7em} \text{and} \hspace{0.7em}
  T(\schedule) \leq T_\minimum.
\]

We consider platforms with $\np = 2$, 4, 8, 16, and 32 cores. Ten applications
with the number of tasks $\nt = 20 \, \np$ (that is, 40 tasks for 2 cores up to
640 tasks for 32 cores) are randomly generated for each platform; thus, 50
problems in total. The floorplans of the platforms and the task graphs of the
applications, including the execution time and dynamic power consumption of each
task on each core, are available online at \cite{sources}. $\rho_\burn$ and
$\rho_\wear$ in \eref{thermal-constraint} and \eref{reliability-constraint},
respectively, are set to 0.01. Due to the diversity of the problems,
$t_\maximum$, $\q_\maximum$, and $T_\minimum$ are found individually for each
problem, ensuring that they make sense for the subsequent optimization. For
instance, $\q_\maximum$ was found within the range 90--120${}^\circ{}C$. Note,
however, that these three parameters stay the same for both the probabilistic
and deterministic variants of the optimization.

The obtained results are reported in \tref{optimization}, and the most important
message is in the last column. \emph{Failure rate} refers to the ratio of the
solutions produced by the deterministic optimization that, after being
reevaluated using the probabilistic approach (that is, after taking process
variation into account), have been found to be violating the probabilistic
constraints given in \eref{thermal-constraint} and/or
\eref{reliability-constraint}. To give an example, for the quad-core platform,
six out of ten schedules proposed by the deterministic approach violate the
constraints on the maximum temperature and/or minimum lifetime when evaluated
considering process variation. The more complex the problem becomes, the higher
values the failure rate attains: with 16 and 32 processing elements (320 and 640
tasks, respectively), all deterministic solutions violate the imposed
constraints. Moreover, the difference between the acceptable one percent of
burn/wear ($\rho_\burn = \rho_\wear = 0.01$) and the actual probability of
burn/wear was found to be as high as 80\% in some cases, which is unacceptable.

In addition, we inspected those few deterministic solutions that had passed the
probabilistic reevaluation and observed that the reported reduction of the
energy consumption and maximum temperature as well as the reported increase of
the lifetime were overoptimistic. More precisely, the predictions produced by
the deterministic optimization, which ignores variations, were compared with the
expected values obtained when process variation was taken into account. The
comparison showed that the expected energy and temperature were up to 5\% higher
while the expected lifetime was up to 20\% shorter than the ones estimated by
the deterministic approach. This aspect of the deterministic optimization can
mislead the designer.

Consequently, when studying those aspects of electronic systems that are
concerned with power, temperature, and reliability, the ignorance of the
deteriorating effect of process variation can severely compromise the associated
design decisions making them less profitable in the best case and dangerous,
harmful in the worst scenario.

Let us now comment on the optimization time shown in \tref{optimization}. It can
be seen that the prototype of the proposed framework takes from about one minute
to six hours (utilizing 16 \up{CPU}s) in order to perform optimization, and
the deterministic optimization is approximately 2--40 times faster. However, the
price to pay when relying on the deterministic approach is considerably high as
we discussed in the previous paragraphs. It can be summarized as ``blind
guessing with highly unfavorable odds of succeeding." Consequently, we consider
the computational time of our framework to be reasonable and affordable,
especially in an industrial setting.

Lastly, we performed experiments also to investigate the impact of the lifetime
constraint in \eref{reliability-constraint} on the reduction of the expected
energy consumption. To this end, we ran our probabilistic optimization (all 50
problems) without the constraint in \eref{reliability-constraint} and compared
the corresponding results with those obtained considering the lifetime
constraint. We observed that the expected energy consumption was higher when
\eref{reliability-constraint} was taken into account, but the difference
vanishes when the complexity of the problems increases. On average, the cost of
\eref{reliability-constraint} was below 5\% of the expected energy consumption.
Without \eref{reliability-constraint}, however, no (probabilistic) guarantees on
the lifetime of the considered systems can be given.

\section{Conclusion}

We have presented a number of techniques for uncertainty quantification of
electronic systems subjected to process variation. First, we developed a
process-variation-aware approach to dynamic steady-state temperature analysis.
Second, we proposed a framework for reliability analysis that seamlessly takes
into account the variability of process parameters and, in particular, the
effect of process variation on temperature. We drew a comparison with \ac{MC}
sampling, which confirmed the efficiency of our solutions in terms of both
accuracy and speed. The low computational demand of our techniques implies that
they are readily applicable for practical instantiations inside design-space
exploration loops, which was also demonstrated in this work considering an
energy-driven probabilistic optimization procedure under reliability-related
constraints. We have shown that temperature is to be treated as a stochastic
quantity in order to pursue robustness of electronic system designs.

\section{Appendix}

\subsection{Spectral Decomposition}

Let $H \subset \L{2}(\Omega, \mathcal{F}, \probability)$ be the Gaussian Hilbert
space \cite{janson1997} spanned by the random variables $\{ \z_i \}_{i = 1}^\nz$
contained in \vz as defined in \eref{independent-random-variables}. Since these
variables are independent and standard, they form an orthonormal basis in $H$,
and the dimensionality of $H$ is $\nz$. Let $\Psi_\lc(H)$ be the space of
$\nz$-variate polynomials over $H$ such that the total order of each polynomial
is less or equal to $\lc$. $\Psi_\lc(H)$ can be constructed as a span of
$\nz$-variate Hermite polynomials \cite{maitre2010, eldred2008}:
\[
  \Psi_\lc(H) = \mathrm{span}\left(\left\{ \psi_{\multiindex}(\vh): \: \multiindex \in \multiindices{\lc}, \: \vh \in H^\nz \right\}\right)
\]
where $\multiindex = (\alpha_i) \in \natural^\nz$ is a multi-index,
\[
  \multiindices{\lc} = \left\{ \multiindex: \: \norm[1]{\multiindex} := \sum_{i = 1}^\nz |\alpha_i| \leq \lc \right\},
\]
\[
  \psi_{\multiindex}(\vh) = \prod_{i = 1}^\nz \psi_{\alpha_i}(\h_i), \hspace{1em} \text{and}
\]
$\psi_{\alpha_i}$ is a one-dimensional Hermite polynomial of order $\alpha_i$,
which is assumed to be normalized for convenience. Define $\mathcal{H}_0 :=
\Psi_0(H)$ (the space of constants) and, for $i \geq 1$,
\[
  \mathcal{H}_i := \Psi_i(H) \, \cap \, \Psi_{i - 1}(H)^\perp.
\]
The spaces $\mathcal{H}_i$, $i \geq 0$, are mutually orthogonal, closed
subspaces of $\L{2}(\Omega, \mathcal{F}, \probability)$. Since our scope of
interest is restricted to functions of $\vz$ (via
\eref{probability-transformation}), $\mathcal{F}$ is assumed to be generated by
$\{ \z_i \}_{i = 1}^\nz$. Then, by the Cameron--Martin theorem,
\[
  \L{2}(\Omega, \mathcal{F}, \probability) = \bigoplus_{i = 0}^\infty \mathcal{H}_i,
\]
which is known as the Wiener chaos decomposition. Thus, any $\g \in
\L{2}(\Omega, \mathcal{F}, \probability)$ admits an expansion with
respect to the polynomial basis. Define the associated linear operator by
\[
  \chaos{\nz}{\lc}{\g} := \sum_{\multiindex \in
  \multiindices{\lc}} \innerproduct{\g}{\psi_{\multiindex}} \, \psi_{\multiindex}(\vz).
\]
\rref{multiple-dimensions} is relevant for the present discussion. The spectral
decomposition in \eref{spectral-decomposition} converges in mean square to \g as
$\lc \to \infty$. We shall refer to $\lc$ as the level of the chaotic expansion.
The cardinality of $\multiindices{\lc}$ in
\eref{isotropic-total-order-index-set} is
\[
  \cardinality{\multiindices{\lc}} := { \lc + \nz \choose \nz }.
\]
Denote by $\d F_\z$ the probability measure induced on $\real^\nz$
by \vz, which is standard Gaussian given by
\[
  \d F_\z(\v{x}) = (2 \pi)^{-\nz/2} \, e^{-\norm[2]{\v{x}}^2 / 2} \, \d \v{x}.
\]
The inner product in \eref{spectral-decomposition} projects \g onto the
corresponding polynomial subspaces and is given by
\[
  \hat{\g}_{\multiindex}
  = \innerproduct{\g}{\psi_{\multiindex}}
  = \int_{\real^\nz} \g(\transform{\v{x}}) \, \psi_{\multiindex}(\v{x}) \, \d F_\z(\v{x}).
\]
Recall that \g is a function of \vu, and $\transform$, defined in
\eref{probability-transformation}, bridges \vu with \vz. Lastly, we note that
$\{ \psi_{\multiindex}: \multiindex \in \natural^\nz \}$ are orthonormal with
respect to $\d F_\z$:
$\innerproduct{\psi_{\multiindex}}{\psi_{\multiindex[\beta]}} =
\delta_{\multiindex \multiindex[\beta]}$ where $\multiindex = (\alpha_i)$ and
$\multiindex[\beta] = (\beta_i)$ are two arbitrary multi-indices,
$\delta_{\multiindex \multiindex[\beta]} := \prod_i \delta_{\alpha_i \beta_i}$,
and $\delta_{ij}$ is the Kronecker delta.

\subsection{Numerical Integration}

As shown in \eref{orthogonal-projection}, each $\hat{\g}_{\multiindex}$ is an
\nz-dimensional integral, which, in general, should be computed numerically.
This task is accomplished by virtue of an adequate \nz-dimensional quadrature,
which is essentially a set of \nz-dimensional points accompanied by scalar
weights. Since we are interested in integration with respect to the standard
Gaussian measure over $\real^\nz$ (see \eref{standard-gaussian-measure}), we
shall rely on the Gauss--Hermite family of quadrature rules \cite{maitre2010},
which is a subset of a broader family known as Gaussian quadratures. The
construction of high-dimensional rules should be undertaken with a great care
as, without special treatments, the number of points grows exponentially. In
what follows, we address this crucial aspect.

Let $f: \real^\nz \to \real$ and define the quadrature-based approximation of
the integral of $f$ by the linear functional
\[
  \quadrature{\nz}{\lq}{f} := \sum_{i = 1}^\nq f(\v{x}_i) \, w_i
\]
where $\{ (\v{x}_i \in \real^\nz, w_i \in \real) \}_{i = 1}^\nq$ are the points
and weights of the chosen quadrature. \rref{multiple-dimensions} applies in this
context as well. The subscript $\lq \in \natural$ denotes the level of the rule,
which is its index in the corresponding family of rules with increasing
precision. The precision refers to the maximal total order of polynomials that
the quadrature integrates exactly. The number of points $\nq$ can be deduced
from the pair $(\nz, \lq)$, which we shall occasionally emphasize by writing
$\nq(\nz, \lq)$. For the Gauss--Hermite quadrature rules in one dimension, we
have that $\nq = \lq + 1$ and the precision is $2 \nq - 1$ \cite{heiss2008} or,
equivalently, $2 \lq + 1$, which is a remarkable property of Gaussian
quadratures.

The foundation of a multidimensional rule $\quadrature{\nz}{\lq}$ is a set of
one-dimensional counterparts $\{ \quadrature{1}{i} \}_{i = 0}^\lq$. A
straightforward construction is the tensor product of $\nz$ copies of
$\quadrature{1}{\lq}$:
\[
  \quadrature{\nz}{\lq} = \bigotimes_{i = 1}^\nz \quadrature{1}{\lq},
\]
which is referred to as the full-tensor product. In this case, $\nq(\nz,
\lq) = \nq(1, \lq)^\nz$, that is, the growth of the number of points is
exponential. Moreover, it can be shown that most of the points obtained via this
construction are an excess as the full-tensor product does not take into account
the fact that the integrands under consideration are polynomials whose total
order is constrained according to a certain strategy.

An alternative construction is the Smolyak algorithm \cite{maitre2010,
eldred2008}. Intuitively, the algorithm combines $\{ \quadrature{1}{i} \}_{i =
0}^\lq$ such that $\quadrature{\nz}{\lq}$ is tailored to be exact only for a
specific polynomial subspace. Define $\Delta_0 := \quadrature{1}{0}$ and
$\Delta_i := \quadrature{1}{i} - \quadrature{1}{i - 1}$ for $i \geq 1$. Then
Smolyak's approximating formula is
\[
  \quadrature{\nz}{\lq} = \bigoplus_{\multiindex \in \multiindices{\lq}} \Delta_{\alpha_1} \otimes \cdots \otimes \Delta_{\alpha_\nz}.
\]
In the original (isotropic) formulation of the Smolyak algorithm,
$\multiindices{\lq}$ is the same as the one defined in
\eref{isotropic-total-order-index-set}; the resulting sparse grid is exact for
polynomials of total order $2 \lq + 1$ (analogous to the integration in one
dimension). Note that, although we use the same notation in
\eref{full-tensor-product-grid} and \eref{smolyak-sparse-grid}, the two
constructions are generally different. (The latter reduces to the former if
$\multiindices{\lq}$ is set to $\{ \multiindex: \max_i \alpha_i
\leq \lq \}$.) It can be seen that the construction in
\eref{smolyak-sparse-grid} is a summation of cherry-picked tensor products of
one-dimensional quadrature rules. Equation \eref{smolyak-sparse-grid} is well
suited for grasping the structure of the resulting sparse grids; more
implementation-oriented versions of the Smolyak formula can be found in the
cited literature.
