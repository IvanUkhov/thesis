At Stage~3, an interpolant of the quantity of interest is to be constructed
using the algorithm presented in this section. The algorithm constitutes the
core of the framework proposed in this chapter, and it features a sparse
structure, hierarchical construction, and hybrid adaptivity. The benefits of
these features are interconnected and can be summarized as follows: the ability
to efficiently tackle multidimensional problems, the ability to perform gradual
refinement of the approximation with a natural error control, and the ability to
make the refinement fine-grained and, therefore, gain further efficiency.

Hierarchical interpolation is introduced in \sref{sparse-interpolation}, and
here we rely heavily on the results given in that section. The mathematics
presented in \sref{sparse-interpolation} as well as here is based on the
development in \cite{klimke2006, ma2009, jakeman2012}.

Let \g as a function of \vz via $\transform$ be in $\continuous([0, 1]^\nz)$,
the space of continuous functions on $[0, 1]^\nz$; this assumption is not
limiting in practice. As shown in \sref{sparse-interpolation}, \g can be
approximated using the following hierarchical interpolant:
\begin{equation} \elab{interpolant-sparse}
  \g \approx \interpolant{\nz}{\ls}(\g)
  = \interpolant{\nz}{\ls - 1}(\g) + \sum_{\vi \in \Delta\sparseindex{\nz}{\ls}}
  \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} \Delta(\g \circ \transform)(\vx_{\vi \vj}) e_{\vi \vj} \end{equation}
where $\vi \in \natural^\nz$ is referred to as a level index; $\vj \in
\natural^\nz$ is referred to as an order index; $\{ \vx_{\vi \vj} \}$ and $\{
e_{\vi \vj} \}$ are collocation nodes and basis functions, respectively; $\{
\Delta(\g \circ \transform)(\vx_{\vi \vj}) \}$ are hierarchical surpluses
defined in \eref{interpolant-sparse-surplus}; and $\Delta\sparseindex{\nz}{\ls}$
and $\Delta\tensorindex{\nz}{\vi}$ are index sets defined in
\eref{interpolant-sparse-index-delta} and \eref{interpolant-tensor-index-delta},
respectively.

Due to the reason to be discussed in \sref{interpolant-adaptivity}, the
interpretation of \eref{interpolant-sparse} is different from the one used in
\sref{sparse-interpolation}. Specifically, $\ls \in \natural$ no longer
represents the current interpolation level but rather the current interpolation
step, and $\interpolant{\nz}{\ls}(\g)$ is the interpolant at that step. From now
on, all index sets are generally subsets of their full-fledged counterparts
defined in \sref{sparse-interpolation}.

Let us now turn to the choice of collocation nodes and basis functions.

\subsection{Collocation Nodes}
\slab{interpolant-grid}

\inputfigure{interpolant-grid}
As explained in \sref{sparse-interpolation}, the integration grid should
preferably be fully nested in order to gain additional computational efficiency.
Such a grid can be constructed using the family of Newton--Cotes rules
\cite{ma2009}, which, moreover, is well disposed to adaptivity as we shall see
later on. In one dimension, a Newton--Cotes rule is merely a set of equidistant
nodes on $[0, 1]$.

There are two types of Newton--Cotes rules: closed and open. The only difference
between the two is that the former includes the endpoints, that is, 0 and 1,
while the latter does not. Now, in \sref{sparse-interpolation}, we postulate
that the condition in \eref{interpolant-tensor-exactness} has to be fulfilled in
order to proceed. The closed type satisfies this condition, and it is the one
used in the original version of local adaptivity presented in \cite{ma2009}. The
open type, on the other hand, does not fulfill the condition close to the
boundaries of the interval. However, according to our experience, open
Newton--Cotes rules are a viable option since they performs well in practice,
which is also noted in \cite{klimke2006}. In fact, we are able to obtain better
results with open rules and, therefore, present them here.

The open Newton--Cotes rule of level $i \in \natural$ is
\[
  \X^1_i = \left\{ \x_{ij}: j \in \tensorindex{1}{i} \right\}
\]
where
\begin{align*}
  & \x_{ij} = \frac{j + 1}{n_i + 1}, \\
  & \tensorindex{1}{i} = \left\{ i - 1 \right\}_{i = 1}^{n_i}, \text{ and} \\
  & n_i = 2^{i + 1} - 1.
\end{align*}
\fref{interpolant-grid} depicts the first three levels of this rule. It can be
seen that the number of nodes grows as 1, 3, 7, and so on, and that the rule is
fully nested. In multiple dimensions, the nodes are formed as shown in
\eref{interpolant-tensor-grid} and \eref{quadrature-tensor-index}.

\subsection{Basis Functions}
\slab{interpolant-basis}

\inputfigure{interpolant-basis}
The basis functions that correspond to open Newton--Cotes rules are the
following piece-wise linear functions. For $i = 0$ and $j = 0$, we have that
$e_{00}(\x) = 1$. For $i > 0$ and $j = 0$ (close to the left endpoint),
\[
  e_{i0}(\x) =
  \begin{cases}
    2 - \left(n_i + 1\right) \x, & \text{if } \x < \frac{2}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
For $i > 0$ and $j = n_i - 1$ (close to the right endpoint),
\[
  e_{i, n_i - 1}(\x) =
  \begin{cases}
    \left(n_i + 1\right) \x - n_i + 1, & \text{if } \x > \frac{n_i - 1}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
In other cases,
\[
  e_{ij}(\x) =
  \begin{cases}
    1 - \left(n_i + 1\right)|\x - \x_{ij}|, & \text{if } |\x - \x_{ij}| < \frac{1}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
The basis functions corresponding to the first three levels of one-dimensional
interpolation are depicted in \fref{interpolant-basis}. In multiple dimensions,
the basis functions are formed as shown in \eref{interpolant-tensor-basis} and
\eref{quadrature-tensor-index}.

Lastly, let us calculate the volumes (integrals over the whole domain) of the
basis functions, which we denote by $w_{ij}$; they will be needed in the
continuation. We have that $w_{00} = 1$ and, for $i > 0$,
\begin{equation} \elab{interpolant-basis-volume}
  w_{ij} = \int_0^1 e_{ij}(\x) \d \x =
  \begin{cases}
    \frac{2}{n_i + 1}, & \text{if } j \in \{ 0, n_i - 1 \}, \\
    \frac{1}{n_i + 1}, & \text{otherwise}.
  \end{cases}
\end{equation}
In multiple dimensions, the volumes of basis functions are products of
their one-dimensional counterparts shown above.

Imagine now a function that is nearly flat on the first half of $[0, 1]$ and
rather irregular on the other. Under these circumstances, it is natural to
expect that, in order to attain the same accuracy, the first half should require
much fewer collocation nodes than the other one; recall
\fref{interpolant-example}. However, if we followed the usual construction
procedure described in \sref{sparse-interpolation}, we would not be able to
benefit from this idiosyncrasy: both sides would be treated equally, and all the
nodes of each level would be added. The solution to the above problem is to make
the interpolation algorithm adaptive, which we discuss next.

\subsection{Hybrid Adaptivity}
\slab{interpolant-adaptivity}

In order to make the algorithm adaptive, we first need to decide on a measure of
how good the constructed interpolant $\interpolant{\nz}{\ls}(\g)$ is at any
given point in $[0, 1]^\nz$. Then, when refining the interpolant, instead of
evaluating the quantity of interest at all possible nodes, we shall choose only
those that are located in the regions with poor accuracy as indicated by the
yet-to-be-found criterion.

We already have a good foundation for building such a criterion. Specifically,
hierarchical surpluses given in \eref{interpolant-sparse-surplus} are natural
indicators of the interpolation error: they are the difference between the
values of the true function and those of an approximation at the nodes of the
underlying sparse grid. Hence, surpluses can be recycled in order to effectively
identify problematic regions. We proceed as follows. First a score is assigned
to each node $\vx_{\vi \vj}$ or, equivalently, to each pair of level and order
indices $(\vi, \vj)$ as follows:
\begin{equation} \elab{interpolant-score}
  s_{\vi \vj} = \absolute{\Delta(\g \circ \transform)(\vx_{\vi \vj}) w_{\vi \vj}}
\end{equation}
where $\Delta(\g \circ \transform)(\vx_{\vi \vj})$ and $w_{\vi\vj}$ are given by
\eref{interpolant-sparse-surplus} and \eref{interpolant-basis-volume},
respectively. Then this score is used in order to guide the algorithm as we
explain now.

Each $\interpolant{\nz}{\ls}$ is characterized by a set of level indices
$\sparseindex{\nz}{\ls}$, and each $\vi \in \sparseindex{\nz}{\ls}$ is
characterized by a set of order indices $\Delta\tensorindex{\nz}{\vi}$. At each
interpolation step $\ls \in \natural$, a single index denoted by $\vi_{\ls}$ is
chosen from $\sparseindex{\nz}{\ls - 1}$ starting from $\sparseindex{\nz}{-1} =
\{ \v{0} \}$. The chosen index subsequently gives birth to
$\Delta\sparseindex{\nz}{\ls}$ and $\{ \Delta\tensorindex{\nz}{\vi}: \vi \in
\Delta\sparseindex{\nz}{\ls} \}$, which form the increment given on the
right-hand side of \eref{interpolant-sparse}.

The level index set $\Delta\sparseindex{\nz}{\ls}$ contains the so-called
admissible forward neighbors of the chosen $\vi_{\ls}$. The forward neighbors of
an index \vi are given by
\[
  \{ \vi + \v{1}_k \}_{k = 1}^\nz
\]
where $\v{1}_k \in \{ 0, 1 \}^\nz$ is a vector whose elements are zero except
for element $k$ equal to unity. Further, an index \vi is called admissible if
its inclusion into the index set $\sparseindex{\nz}{\ls}$ in question keeps the
set admissible. Finally, $\sparseindex{\nz}{\ls}$ is admissible if it satisfies
the following condition \cite{klimke2006}:
\[
  \vi - \v{1}_k \in \sparseindex{\nz}{\ls}
\]
for $\vi \in \sparseindex{\nz}{\ls}$ and $k = \range{1}{\nz}$ where the cases
with $i_k = 0$ need no check.

Let us now turn to the content of $\Delta\tensorindex{\nz}{\vi}$ where $\vi =
\vi_{\ls} + \v{1}_k$ for some $k$. It also contains admissible forward
neighbors; however, they are order indices, and their construction is different
from the one used in the case of $\Delta\sparseindex{\nz}{\ls}$. Concretely,
these indices are identified by inspecting the backward neighborhood of
\vi---analogous to the forward one but in the opposite direction. For each
backward neighbor $\vi - \v{1}_l$ and each $\vj \in \Delta\tensorindex{n}{\vi -
\v{1}_l}$, we check first the condition
\[
  s_{\vi - \v{1}_l, \vj} \geq \error{s}
\]
where \error{s} is a user-defined constant referred to as the score error. If
the condition holds, the forward neighbors of \vj in dimension $k$ are added to
$\Delta\tensorindex{\nz}{\vi}$. This procedure is illustrated in
\fref{interpolant-grid} for open Newton--Cotes rules in one dimension. The
arrows emerging from a node connect the node with its forward neighbors. In
general, each node has two forward neighbors in each dimension. The order
indices of these nodes are
\begin{align*}
  & (j_1, \dots, 2 j_k \phantom{{} + 2}, \dots, j_{\nz}) \text{ and} \\
  & (j_1, \dots, 2 j_k + 2,              \dots, j_{\nz}).
\end{align*}
The above refinement procedure is to be undertaken for each level index $\vi \in
\Delta\sparseindex{\nz}{\ls}$ with respect to each dimension $k =
\range{1}{\nz}$.

The choice of $\vi_{\ls}$ from $\sparseindex{\nz}{\ls - 1}$ at each step \ls of
\eref{interpolant-sparse} is made as follows. First of all, each index can be
picked at most once. The rest is resolved by prioritizing the candidates. It is
reasonable to assign a priority to a level index \vi based on the scores of the
order indices associated with this level index, that is, based on the scores of
$\tensorindex{\nz}{\vi}$. The priority can be computed as the average score
\[
  s_{\vi} = \frac{1}{\cardinality{\Delta\tensorindex{\nz}{\vi}}} \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} s_{\vi \vj}
\]
Thus, at each step \ls, the index \vi with the highest $s_{\vi}$ is promoted to
$\vi_{\ls}$.

The final question to answer is the stopping condition of the approximation
process in \eref{interpolant-sparse}. Apart from the natural constraints on the
maximum number of function evaluations and the maximum interpolation level (the
original \ls in \sref{sparse-interpolation}), we rely on the following
criterion. Assume there are two additional constants: \error{a} and \error{r},
which are referred to as the absolute and relative error, respective. Then the
process is terminated as soon as
\begin{equation} \elab{interpolant-stop}
  \max_{\vi \vj} \absolute{\Delta(\g \circ \transform)(\vx_{\vi \vj})} \leq \max \left\{ \error{a}, \error{r} (\g_\maximum - \g_\minimum) \right\}
\end{equation}
where $\g_\minimum$ and $\g_\maximum$ are the minimum and maximum observed value
of \g, respectively, and the left-hand side is the maximum surplus whose level
index has not been refined yet (considered as $\vi_{\ls}$ at some step \ls). The
above criterion is adequate for curtailing the process since it is based on the
actual progress.

The adaptivity presented in this subsection is referred to as hybrid since it
combines features of global and local adaptivity; the combination was proposed
in \cite{jakeman2012}. Local adaptivity, which has already been sufficiently
motivated, is due to \cite{ma2009}, and it operates on the level of individual
nodes. Global adaptivity is due to \cite{klimke2006}, and it operates on the
level of individual dimensions. The intuition behind global adaptivity is that,
in general, the input variables manifest themselves (impact \g) differently, and
the interpolation algorithm is likely to benefit by prioritizing those variables
that are the most influential.

To summarize, we have obtained an efficient algorithm for adaptive hierarchical
interpolation in multiple dimensions. The main equation is
\eref{interpolant-sparse} where $\Delta(\g \circ \transform)(\vx_{\vi \vj})$,
$\vx_{\vi \vj}$, and $e_{\vi \vj}$ are the ones given in
\eref{interpolant-sparse-surplus}, \sref{interpolant-grid}, and
\sref{interpolant-basis}, respectively, and the interpolation is undertaken
according to the rules given in \sref{interpolant-adaptivity}. Now we discuss
the implementation.

\subsection{Implementation}

The life cycle of interpolation has roughly two stages: construction and usage.
The construction stage invokes \g at a set of collocation nodes and produces
certain artifacts. The usage stage estimates the values of \g at a set of
arbitrary points by manipulating the artifacts. In this subsection, we provide
the pseudocodes of the two stages in order to give the big picture of the
technique.

Let us first make a general note. According to our experience, it is beneficial
to the clarity and ease of implementation to collapse the two sums in
\eref{interpolant-sparse} into one. This requires storing a level index $\vi =
(i_k) \in \natural^\nz$ and an order index $\vj = (j_k) \in \natural^\nz$ for
each interpolation element. It is also advantageous to encode each pair $(i_k,
j_k)$ as a single unsigned integer, which, in particular, eliminates excessive
memory usage. In multiple dimensions, this results in a vector $\v{\iota} =
(\iota_k) \in \natural^\nz$, which we simply call an index. Our encoding is as
follows:
\[
  \iota_k = i_k \lor (j_k \ll \n{\mathrm{bits}})
\]
where $\lor$ and $\ll$ are the bitwise \up{OR} and logical left shift,
respectively, and \n{\mathrm{bits}} is the number of bits reserved for storing
level indices, which can be adjusted according to the maximum permitted depth of
interpolation.

\inputalgorithm{interpolant-construction}
The pseudocode of the construction stage is given in
\aref{interpolant-construction}. The input is a subroutine called Algorithm~G
that evaluates $\g \circ \transform$. The output is a structure
\texttt{interpolant} that contains artifacts of interpolation. These artifacts
are a set of tuples $\{ (\v{\iota}_k, \Delta\g(\vx_{\v{\iota}_k})) \}$, which is
a comprehensive description of a hierarchical interpolant. The pseudocode works
as follows.

Line 1: Each iteration of the loop corresponds to an interpolation step \ls in
\eref{interpolant-sparse}. The progress is captured by a structure
\texttt{state}. The \texttt{strategy} object represents the adaptation strategy
utilized, and it operates in accordance with \sref{interpolant-adaptivity}. The
\texttt{Continue?} method of \texttt{strategy} checks if any of the stopping
conditions is satisfied, in which case the process is terminated.

Line 2: The \texttt{Next} method of \texttt{strategy} consumes the previous
state and returns the initial state of the ongoing interpolation step. In
particular, it populates the \texttt{indices} field of \texttt{state} with the
indices of the step. The rest of the loop's body populates the rest of
\texttt{state}'s fields so that \texttt{strategy} can adequately execute its
functions at the beginning of the next iteration.

Line 3: The \texttt{grid} object represents the utilized interpolation grid, and
its \texttt{Compute} method calculates the collocation nodes that correspond to
given indices, that is, $\{ \vx_{\v{\iota}_k} \}$ based on $\{ \v{\iota}_k \}$;
see \sref{interpolant-grid} for more detail.

Line 4: Algorithm~G evaluates $\g \circ \transform$ at the collocation nodes.
This is by far the most time consuming operation of the algorithm as \g is
generally expensive to execute. This operation is also a prominent candidate for
parallelization since the algorithm does not impose any particular evaluation
order.

Line 5: \aref{interpolant-evaluation} is a subroutine that exercises the
interpolant constructed so far at the collocation nodes and, thereby,
approximates the values obtained on line~4. This algorithm is to be discussed
separately.

Line 6: The \texttt{Subtract} subroutine computes the difference between the
actual and approximated values of \g, which yields the hierarchical surpluses
$\{ \Delta(\g \circ \transform)(\vx_{\v{\iota}_k}) \}$ of the current
interpolation step.

Line 7: The \texttt{Score} method of \texttt{strategy} calculates the scores of
the new collocation nodes based on their surpluses as described in
\sref{interpolant-adaptivity}.

Line 8: The \texttt{Append} method of \texttt{interpolant} refines the
interpolant by extending it with the indices and surpluses of the undertaken
iteration.

\inputalgorithm{interpolant-evaluation}
We now turn to the usage stage of an interpolant. The pseudocode is given in
\aref{interpolant-evaluation}. This subroutine is also involved in
\aref{interpolant-construction}; see line~5. Let us make a couple of
observations regarding this subroutine.

Line 3: The inner loop corresponds to an unfolded version of
\eref{interpolant-sparse}; that is, there is no separation into the individual
interpolation steps taken.

Line 4: The \texttt{basis} object represents the utilized interpolation basis,
and its \texttt{Compute} method evaluates the basis functions that correspond to
given indices, that is, $\{ e_{\v{\iota}_k} \}$ based on $\{ \v{\iota}_k \}$, at
arbitrary points; see \sref{interpolant-basis}.

It is worth noting that the \texttt{strategy}, \texttt{grid}, and \texttt{basis}
objects conform to certain interfaces and can be easily swapped out. This makes
the two algorithms very general and reusable with different configurations. In
particular, the adaptation strategy can be fine-tuned for each particular
problem.

To recapitulate, in this section, the approximation engine of our framework for
probabilistic analysis of computer systems has been presented. The technique is
consolidated in \aref{interpolant-construction} and
\aref{interpolant-evaluation}.
