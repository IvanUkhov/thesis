The post-processing of hierarchical interpolants is similar to the one of
\ac{PC} expansions discussed in \sref{chaos-processing}. An interpolant
$\interpolant{\nz}{\ls}(\g)$ is a lightweight representation of the heavy
quantity of interest \g.

Since the expected value and variance, which are defined in \eref{expectation}
and \eref{variance}, respectively, usually draw particular attention, we would
like to elaborate on them separately.

As shown in \sref{parameters}, \g can be reparameterized in terms of independent
variables that are uniformly distributed on $[0, 1]^\nz$. This means that the
probability density function of \vz simply equals to one. Therefore, using
\eref{expectation} and \eref{approximation}, we have
\[
  \expectation{\g} \approx \expectation{\interpolant{n}{l}(\f)}
  = \int_{[0, 1]^\nz} \interpolant{n}{l}(\f)(\vz) \d \vz
  = \sum_{\vi \in \sparseindex{n}{l}} \sum_{\vj \in \Delta\tensorindex{n}{\vi}} \Delta\f(\vx_{\vi \vj}) w_{\vi \vj}
\]
where
\[
  w_{\vi \vj}
  = \int_{[0, 1]^\nz} e_{\vi \vj}(\vz) \d \vz
  = \prod_{k = 1}^\nz \int_0^1 e_{i_k j_k}(\z_k) \d \z_k
  = \prod_{k = 1}^\nz w_{i_k j_k}.
\]
In the above equation, $w_{ij}$ is as shown in \eref{volume}. Consequently, we
have obtained an analytical formula for the expected value of \g, which does not
require any additional sampling.

Regarding the variance of \g, it can be seen in \eref{variance} that the
variance can be assembled from two components: the expected value of \g, which
we already have, and the expected value of $\g^2$, which we are missing. The
solution is to let $\h = (\g, \g^2)$ be the metric instead of \g. Then the
expected values of both \g and $\g^2$ will be available in analytical forms, and
the variance of \g can be computed using \eref{variance}. This approach can be
generalized to probabilistic moments of higher orders.

The careful reader has noted a problem with the calculation of variance in the
previous subsection: \h is vector valued. More generally, the metric \g in
\sref{modeling} and the function \f in \sref{interpolation} have been depicted
as having one-dimensional codomains. This, however, has been done only for the
sake of clarity. All the mathematics and pseudocodes stay the same for
vector-valued functions. The only except is that, since a surplus
$\Delta\f(\vx_{\vi \vj})$ naturally inherits the output dimensionality of \f,
the operations that involve $\Delta\f(\vx_{\vi \vj})$ should be adequately
adjusted. If the outputs are on different scales and/or have different accuracy
requirements, one might want to have different $\epsilon_a$ and $\epsilon_r$ in
\eref{stopping-condition} for different outputs. In that case, one also needs to
device a more sensible strategy for scoring collocation nodes in \eref{score}
such as rescaling individual outputs and then calculating the uniform norm
$\norm[\infty]{\cdot}$ or $\L{2}$ norm $\norm[2]{\cdot}$. Our code
\cite{sources} has been written with multiple outputs in mind.

To summarize, once an interpolant of \g has been constructed, the distribution
of \g is estimated using versatile sampling methods applied to the interpolant.
The framework extends naturally to metrics with multiple outputs, and it
provides analytical formulae for expectations and variances.

Let us remind that the evaluation of \g is an extensive operation. Our technique
is designed to keep this expense as low as possible by choosing the evaluation
points adaptively, which is unlike traditional sampling methods. Moreover, in
contrast to \up{PC} expansions and similar techniques, the proposed framework is
well suited for the nonsmooth response surfaces.
