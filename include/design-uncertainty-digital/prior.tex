Sampling methods would be a reasonable solution to probabilistic analysis of
computer systems if computer systems were inexpensive (with respect to the
computation time) to simulate. In order to eliminate or reduce the costs
associated with direct sampling, a number of techniques have been introduced.

Let us first discuss physical sources of uncertainty. Since process variation is
the most prominent example in this case, our work in
\cref{design-uncertainty-analog}---which leverages \ac{PC} expansions
\cite{xiu2010} introduced in \sref{polynomial-chaos}---is very much relevant in
this regard, and the discussion here is a brief recapitulation of the one in
\sref{chaos-prior}. Circuit-level timing and power analysis under process
variation are undertaken in \cite{bhardwaj2008} by means of \ac{PC} expansions.
The work in \cite{juan2012} models static steady-state temperature and accounts
for process variation by leveraging the linearity of Gaussian distributions and
time-invariant systems. A stochastic collocation \cite{xiu2010} approach to
static steady-state temperature analysis is given in \cite{lee2013}, which
relies on global interpolation using Newton polynomials.

Let us now turn to what we refer to as digital sources of uncertainty. In this
context, timing analysis has drawn the major attention \cite{quinton2012}. A
seminal work on response time analysis of periodic tasks with random execution
times on uniprocessor systems is reported in \cite{diaz2002}. A novel analytical
solution to this problem is given in \cite{tanasa2015}, which makes milder
assumptions and allows for addressing larger, previously unsolvable problems.
The framework proposed in \cite{santinelli2011} facilitates task scheduling by
providing probabilistic bounds on the resource given to a task flow and the
resource needed by that task flow; the approach is based on real-time calculus
and applicable to computer systems.

Studying the literature on probabilistic analysis of computer systems related to
\cref{design-uncertainty-analog} and this chapter, one can note a pronounced
trend: the generality and straightforwardness of sampling methods tend to be
lost. To elaborate, a technique typically \one~requires restrictive assumptions
to be fulfilled such as the absence of correlation, \two~is tailored to one
concrete quantity of interest such as the response time, and \three~requires
substantial effort in order to be deployed. However, one should keep in mind
what is practical. First of all, although additional assumptions might make the
mathematics analytically solvable, they often do not hold in reality and
oversimplify the model. An exact analytical solution might also be extremely
complex, requiring a lot of computational resources upon evaluation.
Furthermore, it is often the case that there has been developed a robust
simulator evaluating the quantity under consideration for the deterministic
scenario. Switching to probabilistic analysis based on analytical approaches
means discarding this battle-tested code and implementing something else from
scratch, which is wasteful.

Some of the techniques mentioned earlier, in fact, preserve the generality and
straightforwardness of sampling methods. An example is our uncertainty analysis
presented in \cref{design-uncertainty-analog}. The reason is that our
construction of \ac{PC} expansions is undertaken by means of nonintrusive
spectral projections \cite{xiu2010}, which, similar to sampling methods, do not
need to look inside the ``black box.'' However, as motivated in
\sref{interpolant-example}, nonsmoothness is a serious problem for global
approximation based on polynomials. The convergence of, for instance, \ac{PC}
expansions deteriorates substantially in such cases, requiring partitioning the
stochastic space in order to alleviate the problem. Therefore, it is not
straightforward to apply such techniques as the one given in
\cref{design-uncertainty-analog} in the context of digital sources of
uncertainty exhibiting nonsmoothness.

To the best of our knowledge, in the context of probabilistic analysis of
computer systems, nonsmooth problems have not been systematically and
efficiently addressed yet. However, such problems are inherent to the digital
world, which computer systems are denizens of, as we discuss in
\sref{interpolant-example}.

To conclude, the available techniques for probabilistic analysis of computer
systems are restricted in use. A flexible and easy-to-deploy framework capable
of tackling nonsmooth uncertainty-quantification problems is needed.
