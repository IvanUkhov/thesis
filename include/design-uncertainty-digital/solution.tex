We develop a framework for probabilistic analysis of computer systems that is
efficient in quantifying the deteriorating impact of digital sources of
uncertainty on a design, and that is straightforward to use
in practice. The effectiveness of our approach is due to the powerful
approximation engine that the framework features. Specifically, we make use of
the hierarchical interpolation with hybrid adaptivity developed in
\cite{klimke2006, ma2009, jakeman2012}, which enables tackling diverse design
problems while keeping the associated computational costs low. The usage of the
framework is streamlined---which is also the case with the framework presented
in \cref{design-uncertainty-analog}---because it has the same low entrance
requirements as sampling techniques: one only has to be able to evaluate the
quantity of interest given a set of deterministic parameters. Moreover, it can
be utilized in scenarios with limited knowledge of the joint probability
distribution of the uncertain parameters, which are common in practice.

The general solution strategy here is similar to the one outlined in
\sref{chaos-solution}. First, we note that making use of a sampling method is a
compelling approach to uncertainty quantification, and we would readily apply
such a method to study the quantity \g if only evaluating \g had a small cost,
which it does not. Our solution to this quandary is to construct a light
representation of the heavy \g and study this representation instead of \g.
Given the setting of this chapter, the surrogate that we build is based on
adaptive interpolation: \g is evaluated at a number of strategically chosen
collocation nodes, and any other values of \g are reconstructed on demand
(without involving \g) using a set of basis functions mediating between the
collected values of \g. The benefit of this approach is in the number of
invocations of the quantity \g: only a few evaluations of \g are needed, and the
rest of probabilistic analysis is powered by the constructed interpolant, which,
in contrast to \g, has a negligible cost.

Let us adumbrate the stages of the solution process. They reflect the ones
depicted in \fref{chaos-overview}. At Stage~1, the quantity of interest \g and
the uncertain parameters \vu are decided upon by the designer. At Stage~2, \g is
reparameterized in terms of an auxiliary random vector \vz extracted from \vu.
At Stage~3, an interpolant of \g is constructed by considering \g as a
deterministic function of \vz and evaluating \g at a small set of carefully
chosen points. At Stage~4, the constructed interpolant of \g is post-processed
in order to calculate the desired statistics about \g. In particular, the
probability distribution of \g is estimated by applying an arbitrary sampling
method to the interpolant.

The first stage is problem specific, and it is to be elaborated on in
\sref{interpolant-application}. In what follow, we proceed directly to the
second stage, which together with the third should be approached with a great
care since interpolation of multivariate functions is a challenging undertaking.

\section{Probability Transformation}
\slab{interpolant-transformation}

The foremost step of our framework is to change the parameterization of the
problem from the random vector $\vu: \Omega \to \real^\nu$ to an auxiliary
random vector $\vz: \Omega \to \real^\nz$ such that \one~the support of the
\ac{PDF} of \vz is the unit hypercube $[0, 1]^\nz$, and \two~$\nz \leq \nu$ has
the smallest value that is needed to retain the desired level of accuracy. The
first is standardization, which is done primarily for convenience. The second is
model order reduction, which identifies and eliminates excessive complexity and,
hence, speeds up the subsequent solution process. The overall transformation is
denoted by
\[
  \vu = \transform{\vz}
\]
where $\transform: [0, 1]^\nz \to \real^\nu$. For any point $\vz \in [0,
1]^\nz$, we are now able to compute the corresponding \vu and, consequently, the
quantity \g as
\[
  \g(\vu) = (\g \circ \transform)(\vz) = \g(\transform(\vz)).
\]

The attentive reader might already have a suitable candidate for $\transform$;
it is the one in \eref{probability-transformation} used throughout the thesis,
which is to be discussed in \sref{interpolant-application}.

\section{Surrogate Construction}
\slab{interpolant-construction}

In this section, we present the algorithm that constitutes the core of the
framework proposed in this chapter. The algorithm features a sparse structure,
hierarchical construction, and hybrid adaptivity. The benefits of these features
are interconnected and can be summarized as follows: the ability to efficiently
tackle multidimensional problems, the ability to perform gradual refinement of
the approximation with a natural error control, and the ability to make the
refinement fine-grained and, therefore, gain further efficiency.

Hierarchical interpolation is introduced in \sref{sparse-interpolation}, and
here we rely heavily on the results given in that section. The mathematics
presented in \sref{sparse-interpolation} as well as here is based on the
development in \cite{klimke2006, ma2009, jakeman2012}.

Consider \g as a function of \vz as shown in the previous section. Let \g be in
$\continuous([0, 1]^\nz)$, the space of continuous functions on $[0, 1]^\nz$;
this assumption is a formality and is not limiting in practice. As shown in
\sref{sparse-interpolation}, \g can be approximated using the following
hierarchical interpolant:
\begin{equation} \elab{interpolant-sparse}
  \g \approx \interpolant{\nz}{\ls}(\g)
  = \interpolant{\nz}{\ls - 1}(\g) + \sum_{\vi \in \Delta\sparseindex{\nz}{\ls}} \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} \Delta\g(\vx_{\vi \vj}) e_{\vi \vj}
\end{equation}
where $\ls \in \natural$ is the level of the interpolant; $\interpolant{\nz}{-1}
= 0$; $\{ \vx_{\vi \vj} \}$ and $\{ e_{\vi \vj} \}$ are collocation nodes and
basis functions, respectively; $\Delta\g(\vx_{\vi \vj})$ is a hierarchical
surplus defined in \eref{interpolant-sparse-surplus}; and
$\Delta\sparseindex{\nz}{\ls}$ and $\Delta\tensorindex{\nz}{\vi}$ are index sets
defined in \eref{interpolant-sparse-index-delta} and
\eref{interpolant-tensor-index-delta}, respectively. In this context of
hierarchical interpolation, $\vi \in \natural^\nz$ is referred to as a level
index, and $\vj \in \natural^\nz$ is referred to as an order index.

Let us now turn to the choice of collocation nodes and basis functions.

\subsection{Collocation Nodes}
\slab{interpolant-grid}

\inputfigure{interpolant-grid}
As explained in \sref{sparse-interpolation}, the integration grid should
preferably be fully nested in order to gain additional computational efficiency.
Such a grid can be constructed using the family of Newton--Cotes rules
\cite{ma2009}, which, moreover, is well disposed to adaptivity as we shall see
later on. In one dimension, a Newton--Cotes rule is merely a set of equidistant
nodes on $[0, 1]$.

There are two types of Newton--Cotes rules: closed and open. The only difference
between the two is that the former includes the endpoints, that is, 0 and 1,
while the latter does not. Now, in \sref{sparse-interpolation}, we postulate
that the condition in \eref{interpolant-tensor-exactness} has to be fulfilled in
order to proceed. The closed type satisfies this condition, and it is the one
used in the original version of local adaptivity presented in \cite{ma2009}. The
open type, on the other hand, does not fulfill the condition close to the
boundaries of the interval. However, according to our experience, open
Newton--Cotes rules are a viable option since they performs well in practice,
which is also noted in \cite{klimke2006}. In fact, we are able to obtain better
results with open rules and, therefore, present them here.

The open Newton--Cotes rule of level $i \in \natural$ is
\[
  \X^1_i = \left\{ \x_{ij}: j \in \tensorindex{1}{i} \right\}
\]
where
\begin{align*}
  & \x_{ij} = \frac{j + 1}{n_i + 1}, \\
  & \tensorindex{1}{i} = \left\{ i - 1 \right\}_{i = 1}^{n_i}, \text{ and} \\
  & n_i = 2^{i + 1} - 1.
\end{align*}
\fref{interpolant-grid} depicts the first three levels of this rule. It can be
seen that the number of nodes grows as 1, 3, 7, and so on, and that the rule is
fully nested. In multiple dimensions, the nodes are formed as shown in
\eref{interpolant-tensor-grid} and \eref{quadrature-tensor-index}.

\subsection{Basis Functions}
\slab{interpolant-basis}

\inputfigure{interpolant-basis}
The basis functions that correspond to open Newton--Cotes rules are the
following piece-wise linear functions. For $i = 0$ and $j = 0$, we have that
$e_{00}(\x) = 1$. For $i > 0$ and $j = 0$ (close to the left endpoint),
\[
  e_{i0}(\x) =
  \begin{cases}
    2 - \left(n_i + 1\right) \x, & \text{if } \x < \frac{2}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
For $i > 0$ and $j = n_i - 1$ (close to the right endpoint),
\[
  e_{i, n_i - 1}(\x) =
  \begin{cases}
    \left(n_i + 1\right) \x - n_i + 1, & \text{if } \x > \frac{n_i - 1}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
In other cases,
\[
  e_{ij}(\x) =
  \begin{cases}
    1 - \left(n_i + 1\right)|\x - \x_{ij}|, & \text{if } |\x - \x_{ij}| < \frac{1}{n_i + 1}, \\
    0, & \text{otherwise}.
  \end{cases}
\]
The basis functions corresponding to the first three levels of one-dimensional
interpolation are depicted in \fref{interpolant-basis}. In multiple dimensions,
the basis functions are formed as shown in \eref{interpolant-tensor-basis} and
\eref{quadrature-tensor-index}.

Lastly, let us calculate the volumes (integrals over the whole domain) of the
basis functions, which we denote by $w_{ij}$; they will be needed in the
continuation. We have that $w_{00} = 1$ and, for $i > 0$,
\begin{equation} \elab{interpolant-basis-volume}
  w_{ij} = \int_0^1 e_{ij}(\x) \d \x =
  \begin{cases}
    \frac{2}{n_i + 1}, & \text{if } j \in \{ 0, n_i - 1 \}, \\
    \frac{1}{n_i + 1}, & \text{otherwise}.
  \end{cases}
\end{equation}
In multiple dimensions, the volumes of basis functions are products of
their one-dimensional counterparts shown above.

Imagine now a function that is nearly flat on the first half of $[0, 1]$ and
rather irregular on the other. Under these circumstances, it is natural to
expect that, in order to attain the same accuracy, the first half should require
much fewer collocation nodes than the other one; recall
\fref{interpolant-example}. However, if we followed the usual construction
procedure described in \sref{sparse-interpolation}, we would not be able to
benefit from this idiosyncrasy: both sides would be treated equally, and all the
nodes of each level would be added. The solution to the above problem is to make
the interpolation algorithm adaptive, which we discuss next.

\subsection{Hybrid Adaptivity}
\slab{interpolant-adaptivity}

In order to make the algorithm adaptive, we first need to decide on a measure of
how good the constructed interpolant $\interpolant{\nz}{\ls}(\g)$ is at any
given point in $[0, 1]^\nz$. Then, when refining the interpolant, instead of
evaluating the quantity of interest at all possible nodes, we shall choose only
those that are located in the regions with poor accuracy as indicated by the
yet-to-be-found criterion.

We already have a good foundation for building such a criterion. Specifically,
hierarchical surpluses given in \eref{interpolant-sparse-surplus} are natural
indicators of the interpolation error: they are the difference between the
values of the true function and those of an approximation at the nodes of the
underlying sparse grid. Hence, surpluses can be recycled in order to effectively
identify problematic regions. We proceed as follows. First a score is assigned
to each node $\vx_{\vi \vj}$ or, equivalently, to each pair of level and order
indices $(\vi, \vj)$ as follows:
\[
  s_{\vi \vj} = \absolute{\Delta\g(\vx_{\vi \vj}) w_{\vi \vj}}
\]
where $\Delta\f(\vx_{\vi \vj})$ and $w_{\vi\vj}$ are given by
\eref{interpolant-sparse-surplus} and \eref{interpolant-basis-volume},
respectively. Then this score is used in order to guide the algorithm as we
explain now.

The formula in \eref{interpolant-sparse} is reinterpreted as follows. The number
\ls in \eref{interpolant-sparse} no longer represents the current interpolation
level but rather the current interpolation step, and $\interpolant{\nz}{\ls}$ is
the interpolant at that step. From now on, all index sets are generally subsets
of their full-fledged counterparts defined in \sref{sparse-interpolation}. Each
$\interpolant{\nz}{\ls}$ is characterized by a set of level indices
$\sparseindex{\nz}{\ls}$, and each $\vi \in \sparseindex{\nz}{\ls}$ is
characterized by a set of order indices $\Delta\tensorindex{\nz}{\vi}$. At each
interpolation step $\ls \in \natural$, a single index denoted by $\vi_{\ls}$ is
chosen from $\sparseindex{\nz}{\ls - 1}$ starting from $\sparseindex{\nz}{-1} =
\{ \v{0} \}$. The chosen index subsequently gives birth to
$\Delta\sparseindex{\nz}{\ls}$ and $\{ \Delta\tensorindex{\nz}{\vi}: \vi \in
\Delta\sparseindex{\nz}{\ls} \}$, which form the increment given on the
right-hand side of \eref{interpolant-sparse}.

The level index set $\Delta\sparseindex{\nz}{\ls}$ contains the so-called
admissible forward neighbors of the chosen $\vi_{\ls}$. The forward neighbors of
an index \vi are given by
\[
  \{ \vi + \v{1}_k \}_{k = 1}^\nz
\]
where $\v{1}_k \in \{ 0, 1 \}^\nz$ is a vector whose elements are zero except
for element $k$ equal to unity. Further, an index \vi is called admissible if
its inclusion into the index set $\sparseindex{\nz}{\ls}$ in question keeps the
set admissible. Finally, $\sparseindex{\nz}{\ls}$ is admissible if it satisfies
the following condition \cite{klimke2006}:
\[
  \vi - \v{1}_k \in \sparseindex{\nz}{\ls}
\]
for $\vi \in \sparseindex{\nz}{\ls}$ and $k = \range{1}{\nz}$ where the cases
with $i_k = 0$ need no check.

Let us now turn to the content of $\Delta\tensorindex{\nz}{\vi}$ where $\vi =
\vi_{\ls} + \v{1}_k$ for some $k$. It also contains admissible forward
neighbors; however, they are order indices, and their construction is different
from the one used in the case of $\Delta\sparseindex{\nz}{\ls}$. Concretely,
these indices are identified by inspecting the backward neighborhood of
\vi---analogous to the forward one but in the opposite direction. For each
backward neighbor $\vi - \v{1}_l$ and each $\vj \in \Delta\tensorindex{n}{\vi -
\v{1}_l}$, we check first the condition
\[
  s_{\vi - \v{1}_l, \vj} \geq \error{s}
\]
where \error{s} is a user-defined constant referred to as the score error. If
the condition holds, the forward neighbors of \vj in dimension $k$ are added to
$\Delta\tensorindex{\nz}{\vi}$. This procedure is illustrated in
\fref{interpolant-grid} for open Newton--Cotes rules in one dimension. The
arrows emerging from a node connect the node with its forward neighbors. In
general, each node has two forward neighbors in each dimension. The order
indices of these nodes are
\begin{align*}
  & (j_1, \dots, 2 j_k \phantom{{} + 2}, \dots, j_{\nz}) \text{ and} \\
  & (j_1, \dots, 2 j_k + 2,              \dots, j_{\nz}).
\end{align*}
The above refinement procedure is to be undertaken for each level index $\vi \in
\Delta\sparseindex{\nz}{\ls}$ with respect to each dimension $k =
\range{1}{\nz}$.

The choice of $\vi_{\ls}$ from $\sparseindex{\nz}{\ls - 1}$ at each step \ls of
\eref{interpolant-sparse} is made as follows. First of all, each index can be
picked at most once. The rest is resolved by prioritizing the candidates. It is
reasonable to assign a priority to a level index \vi based on the scores of the
order indices associated with this level index, that is, based on the scores of
$\tensorindex{\nz}{\vi}$. The priority can be computed as the average score
\[
  s_{\vi} = \frac{1}{\cardinality{\Delta\tensorindex{\nz}{\vi}}} \sum_{\vj \in \Delta\tensorindex{\nz}{\vi}} s_{\vi \vj}
\]
Thus, at each step \ls, the index \vi with the highest $s_{\vi}$ is promoted to
$\vi_{\ls}$.

The final question to answer is the stopping condition of the approximation
process in \eref{interpolant-sparse}. Apart from the natural constraints on the
maximum number of function evaluations and the maximum interpolation level (the
original \ls in \sref{sparse-interpolation}), we rely on the following
criterion. Assume there are two additional constants: \error{a} and \error{r},
which are referred to as the absolute and relative error, respective. Then the
process is terminated as soon as
\[
  \max_{(\vi, \vj)} \absolute{\Delta\g(\vx_{\vi \vj})} \leq \max \left\{ \error{a}, \error{r} (\g_\maximum - \g_\minimum) \right\}
\]
where $\g_\minimum$ and $\g_\maximum$ are the minimum and maximum observed value
of \g, respectively, and the left-hand side is the maximum surplus whose level
index has not been refined yet (considered as $\vi_{\ls}$ at some step \ls). The
above criterion is adequate for curtailing the process since it is based on the
actual progress.

The adaptivity presented in this subsection is referred to as hybrid since it
combines features of global and local adaptivity; the combination was proposed
in \cite{jakeman2012}. Local adaptivity, which has already been sufficiently
motivated, is due to \cite{ma2009}, and it operates on the level of individual
nodes. Global adaptivity is due to \cite{klimke2006}, and it operates on the
level of individual dimensions. The intuition behind global adaptivity is that,
in general, the input variables manifest themselves (impact \g) differently, and
the interpolation algorithm is likely to benefit by prioritizing those variables
that are the most influential.

To summarize, we have obtained an efficient algorithm for adaptive hierarchical
interpolation in multiple dimensions. The main equation is
\eref{interpolant-sparse} where $\Delta\g(\vx_{\vi \vj})$, $\vx_{\vi \vj}$, and
$e_{\vi \vj}$ are the ones given in \eref{interpolant-sparse-surplus},
\sref{interpolant-grid}, and \sref{interpolant-basis}, respectively, and the
interpolation is undertaken according to the rules given in
\sref{interpolant-adaptivity}. Now we discuss the implementation.

\subsection{Implementation}

The life cycle of interpolation has roughly two stages: construction and usage.
The construction stage invokes \g at a set of collocation nodes and produces
certain artifacts. The usage stage estimates the values of \g at a set of
arbitrary points by manipulating the artifacts. In this subsection, we provide
the pseudocodes of the two stages in order to give the big picture of the
technique.

Let us first make a general note. According to our experience, it is beneficial
to the clarity and ease of implementation to collapse the two sums in
\eref{interpolant-sparse} into one. This requires storing a level index $\vi =
(i_k) \in \natural^\nz$ and an order index $\vj = (j_k) \in \natural^\nz$ for
each interpolation element. It is also advantageous to encode each pair $(i_k,
j_k)$ as a single unsigned integer, which, in particular, eliminates excessive
memory usage. In multiple dimensions, this results in a vector $\v{\iota} =
(\iota_k) \in \natural^\nz$, which we simply call an index. Our encoding is as
follows:
\[
  \iota_k = i_k \lor (j_k \ll \n{\mathrm{bits}})
\]
where $\lor$ and $\ll$ are the bitwise \up{OR} and logical left shift,
respectively, and \n{\mathrm{bits}} is the number of bits reserved for storing
level indices, which can be adjusted according to the maximum permitted depth of
interpolation.

The pseudocode of the construction stage is given in \aref{construct} called
\texttt{Construct}. The \texttt{target} input is a function $\f$ to be
approximated. The \texttt{surrogate} output is a structure containing the
artifacts of interpolation, which are a set of tuples $\{ (\v{\iota}_k,
\Delta\f(\vx_{\v{\iota}_k}) \}_k$, giving a comprehensive description of an
interpolant. The routine works as follows.

\begin{itemize}

\item[L2:] Each iteration is an interpolation step in \eref{approximation}. It
has a state captured by a structure denoted by \texttt{s}. The \texttt{strategy}
object represents an adaptation strategy utilized and works as described in
\sref{adaptivity}. The \texttt{First} method of \texttt{strategy} returns the
initial state of the first step so that the \texttt{indices} field of \texttt{s}
is initialized with the indices of that step. The body of the loop populates the
rest of the fields of \texttt{s} so that \texttt{strategy.Next} can adequately
produce the initial state of the next iteration. The process terminates when a
stopping condition is satisfied, in which case \texttt{Next} returns a null
state.

\item[L3:] The \texttt{grid} object represents the interpolation grid utilized
(see \sref{grid}), and its \texttt{Compute} method converts the step's indices
into the coordinates of the corresponding collocation nodes, that is, $\{
\v{\iota}_k \}_k$ into $\{ \vx_{\v{\iota}_k} \}_k$.

\item[L4:] \texttt{Invoke} evaluates \texttt{target} at the collocation nodes.
This is by far the most time consuming function of the algorithm as
\texttt{target} is generally expensive to evaluate. This function is also a
prominent candidate for parallelization since the algorithm does not impose any
evaluation order.

\item[L5:] \texttt{Evaluate} exercises the interpolant constructed so far at the
collocation nodes, approximating the values obtained on line~4. This function
will be discussed separately.

\item[6:] \texttt{Subtract} computes the difference between the true and
approximated values of \texttt{target}, which yields the step's hierarchical
surpluses $\{ \Delta\f(\vx_{\v{\iota}_k}) \}_k$, similar to \eref{surplus}.

\item[L7:] \texttt{strategy.Score} calculates the scores of the new collocation
nodes based on their surpluses; see \eref{score}.

\item[L8:] \texttt{Append} improves the interpolant by extending it with the
indices and surpluses of the current iteration.

\end{itemize}

We now turn to the usage stage of an interpolant. The pseudocode is given in
\aref{evaluate} called \texttt{Evaluate}. This algorithm is also involved in
\aref{construct}; see line~5. Let us make a couple of observations regarding
\texttt{Evaluate}.

\begin{itemize}

\item[L4:] The inner loop is an unfolded version of \eref{approximation} (there
is no separation between individual interpolation steps taken).

\item[L5:] The \texttt{basis} object represents the interpolation basis utilized
(see \sref{basis}), and its \texttt{Compute} method evaluates a single
(multidimensional) basis function at a single point.

\end{itemize}

It is worth noting that the \texttt{basis}, \texttt{grid}, and \texttt{strategy}
objects conform to certain interfaces and can be easily swapped out. This makes
the two algorithms very general and reusable with different configurations. In
particular, the adaptation strategy can be fine-tuned for each particular
problem.

To recapitulate, we have presented the key component of our framework for
probabilistic analysis of electronic systems: an efficient approach to
multidimensional interpolation. The overall technique has been consolidated in
\aref{construct} and \ref{alg:evaluate}.
