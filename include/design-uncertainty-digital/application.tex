The agenda for this section is as follows. First, we introduce a number of
quantities of interest \g in order to illustrate the broad applicability of the
proposed framework and, thereby, give the reader a better intuition about the
utility of the framework. It should be understood that, in practice, \g stands
for an adequate simulator of the system under consideration, and the modeling
capabilities of this simulator are naturally inherited by our technique.

\subsection{Problem Formulation}

Assume the system, power, and temperature models given in \sref{system-model},
\sref{power-model}, \sref{temperature-model}, respectively. Assume also the
application model utilized in \sref{thermal-cycling-problem} except for the
requirement about being periodic.

Let us first discuss timing. Each task of the application in question has a
start time and a finish time, which are denoted by $b_i$ and $d_i$,
respectively. Let also $\v{b} = (b_i)_{i = 1}^\nt$ and $\v{d} = (d_i)_{i =
1}^\nt$. Then other timing characteristics of the application can be derived
from $(\v{b}, \v{d})$. For example, the end-to-end delay---which is the
difference between the finish time of the latest task and the start time of the
earliest task---is as follows:
\begin{equation} \elab{interpolant-delay}
  \text{End-to-end delay}
  = \max_{i = 1}^\nt d_i - \min_{i = 1}^\nt b_i.
\end{equation}
Suppose now that the execution times of the tasks depend on the uncertain
parameters \vu; see \sref{interpolant-problem}. Then the tuple $(\v{b}, \v{d})$
depends on \vu. Hence, the end-to-end delay given in \eref{interpolant-delay}
does so too, and it constitutes an example of a quantity \g that the designer
might be interested in studying. Note that this \g is nondifferentiable since
the $\max$ and $\min$ functions are such. Therefore, \g is nonsmooth, which
renders \up{PC} expansions and similar techniques inadequate for this problem,
which is elaborated on in \sref{interpolant-example}.

\begin{remark} \rlab{interpolant-nonsmoothness}
In general, the behavior of \g with respect to continuity, differentiability,
and smoothness cannot be inferred from the behavior of \vu. Even when the
parameters behave perfectly, \g might still exhibit nondifferentiability or even
discontinuity, which depends on how \g functions internally. For example, as
shown in \cite{tanasa2015}, even if execution times of tasks are continuous, due
to the actual scheduling policy, end-to-end delays are very often discontinuous.
\end{remark}

Let us move on to power. The total energy consumed by the system during an
application run can be estimated using a power profile \mp as follows:
\begin{equation} \elab{interpolant-energy}
  \text{Total energy}
  = \sum_{i = 1}^\np \int \p_i(t) \d t
  \approx \dt \norm[1]{\mp}
\end{equation}
where $\p_i$ stands for the power consumption of processing element $i$, and \dt
is the sampling interval, which is assumed to be sufficiently small. Since the
tuple $(\v{b}, \v{d})$ depends on \vu, the power consumption of the system is
dependent on \vu as well. Consequently, the total energy given in
\eref{interpolant-energy} depends on \vu and is a candidate for \g.
\rref{interpolant-nonsmoothness} applies in this context to the full extent.

Let us now turn to temperature. The maximum temperature of the system can be
estimated using a temperature profile \mq as follows:
\begin{equation} \elab{interpolant-temperature}
  \text{Maximum temperature}
  = \max_{i = 1}^\np \sup_{t} \q_i(t)
  \approx \norm[\infty]{\mq}
\end{equation}
Since the power consumption of the system is affected by \vu, the heat
dissipation is affected by \vu as well. Therefore, the temperature in
\eref{interpolant-temperature} is a potential quantity of interest \g. Note
that, due to the maximization involved, the quantity is nondifferentiable and,
hence, cannot be adequately addressed using polynomial approximations; recall
also the concern in \rref{interpolant-nonsmoothness}.

To summarize, we have covered three aspects of computer systems, namely, timing,
power, and temperature, and introduced a number of quantities that the designer
is typically interested in analyzing. These quantities are to be discussed
further in the section on experimental results, \sref{interpolant-results}.

\subsection{Probability Transformation}

Let us consider an example of $\transform$ in order to understand the concept
better. Assume that \vu is specified by a set of marginal distribution functions
$\{ F_i \}_{i = 1}^\nu$ and a Gaussian copula whose correlation matrix is
$\correlation{\vu}$; such a copula can be constructed as it is outlined in
\sref{chaos-formulation}. The transformation $\transform$ is
\[
  \vu = F^{-1} \left(\Phi\left(\m{U} \tm{\Lambda}^\frac{1}{2} \Phi^{-1}(\vz)\right)\right)
\]
where the random variables $\vz: \Omega \to \real^\nz$ are independent and
uniformly distributed on $[0, 1]^\nz$; $\Phi$ and $\Phi^{-1}$ are the
distribution function of the standard Gaussian distribution and its inverse,
respectively, which are applied element-wise; and $F_{\vu}^{-1} = F_{\u_1}^{-1}
\times \cdots \times F_{\u_\nz}^{-1}$ is the Cartesian product of the inverse
marginal distributions of \vu, which are applied to the corresponding element of
the vector yielded by $\Phi$. In the absence of correlations,
\eref{transformation-concrete} is simply $\vu = F_{\vu}^{-1}(\vz)$, and no
model-order reduction is possible ($\nu = \nz$).

To summarize, we have found such a transformation $\transform$ and the
corresponding random vector $\vz \sim F_{\vz}$ that: 1) $F_{\vz}$ is supported
by $[0, 1]^\nz$, and 2) \vz has the smallest number of dimensions \nz needed to
preserve $\eta$ portion of the variance. Let us emphasize that this $\transform$
is an example; the framework works with any $\transform$ that yields $\vz \sim
F_{\vz}$ for some \nz.

\subsection{Example}

In \sref{modeling}, we formalized the uncertainty affecting electronic systems
and discussed several aspects of such systems along with metrics \g, which the
designer is interested in evaluating. In \sref{interpolation}, we obtained an
efficient interpolation algorithm for approximating hypothetical
multidimensional functions \f. We shall now amalgamate the ideas developed in
the aforementioned two sections.

Given an electronic system dependent on a number of uncertain parameters $\vu:
\Omega \to \real^\nu$, the goal is to analyze a metric \g representing a certain
aspect of the system. For instance, \vu can correspond to the execution times of
the tasks, and \g can correspond the total energy consumed by the processing
elements, as we exemplify in \sref{time} and \sref{power}. The goal is attained
as follows; recall \fref{example}.

1) The parameterization of \g is changed from \vu to random variables $\vz:
\Omega \to [0, 1]^\nz$ via a suitable transformation $\transform$; this stage is
described in \sref{parameters}. 2) An interpolant of the resulting composition
$\g \circ \transform$ is constructed by treating the composition as a
deterministic function \f of \vz; this stage is detailed in
\sref{interpolation}. 3) An estimation of the probability distribution of \g is
undertaken in the usual sampling-based manner but relying solely on the
constructed interpolant; \g is no longer involved. This last stage boils down to
drawing independent samples from $F_{\vz}$ and evaluating the interpolant
$\interpolant{n}{l}(\f) \equiv \interpolant{n}{l}(\g \circ \transform)$ at those
points. Having collected samples of \g, other statistics about \g, such as
probabilities of particular events, can be straightforwardly estimated. We do
not discuss this estimation stage any further as it is standard.

There are two aspects concerning the usage of the proposed framework that we
would like to cover in what follows.

In this section, we apply our framework to a small problem in order to get a
better understanding of the workflow of the framework. A detailed description of
our experimental setup is given in \sref{configuration}; here we give only the
bare minimum.

The addressed problem is depicted in \fref{example}. We consider a platform with
two processing elements, PE1 and PE2, and an application with four tasks,
T1--T4. The data dependencies between T1--T4 and their mapping onto PE1 and PE2
can be seen in \fref{example}. The metric \g is the end-to-end delay of the
application. The uncertain parameters \vu are the execution times of T2 and T4
denoted by $\u_1$ and $\u_2$, respectively.

The leftmost box in \fref{example} represents a simulator of the system at hand,
and it could involve such tools as Sniper \cite{carlson2011}. It takes an
assignment of the execution times of T2 and T3, $\u_1$ and $\u_2$, and outputs
the calculated end-to-end delay \g. The second box corresponds to the
reparameterization mentioned in \sref{solution} (to be discussed in
\sref{parameters}). It converts the auxiliary variables $\z_1$ and $\z_2$ into
$\u_1$ and $\u_2$ in accordance with $\u_1$ and $\u_2$'s joint distribution. The
third box is our interpolation engine (to be discussed in \sref{interpolation}).
Using a number of strategic invocations of the simulator, the interpolation
engine yields a light surrogate for the simulator; the surrogate corresponds to
the slim box with rounded corners. Having obtained such a surrogate, one
proceeds to sampling extensively the surrogate via a sampling method of choice
(the rightmost box). The surrogate takes $\z_1$ and $\z_2$ and returns an
approximation of \g at that point. Recall that the computational cost of this
extensive sampling is negligible as \g is not involved. The samples are then
used to compute an estimate of the distribution of \g.

In the graph on the right-hand side of \fref{example}, the blue line shows the
probability density function of \g computed by applying kernel density
estimation to the samples obtained from our surrogate. The yellow line (barely
visible behind the blue line) shows the true density of \g; its calculation is
explained in \sref{experiments}. It can be seen that our solution closely
matches the exact one. In addition, the orange line shows the estimation that
one would get if one sampled \g directly 156 times and used only those samples
in order to calculate the density of \g. We see that, for the same budget of
simulations, the solution delivered by our framework is substantially closer to
the true one than the one delivered by na\"{i}ve sampling.

At this point, we are ready to present to the proposed framework. We begin by
elaborating on the modeling of uncertain parameters and metrics of interest. We
shall then proceed to the interpolation engine (\sref{interpolation}).
