Consider a computer system composed of two major components: a platform and an
application. The platform is a collection of heterogeneous processing elements
as defined in \sref{system-model}, and the application is a collection of
interdependent tasks. The designer is interested in studying a quantity \g that
characterizes the system under consideration from a certain perspective.
Examples of \g include the execution delay of the application or a specific
task, energy consumption of the platform or a specific processing element, and
maximum temperature of the platform or a specific processing element.

The quantity of interest \g depends on a set of parameters \vu that are
uncertain at the design stage. Examples of \vu include the amount of data that
the application needs to process, execution times of the tasks, and properties
of the environment. The parameters \vu are given as a random vector $\vu: \Omega
\to \real^\nu$---which is defined on a suitable probability space $(\Omega, \F,
\probability)$ as in \sref{probability-theory}---with an arbitrary but known
distribution whose \ac{CDF} is denoted by $F$.

The dependency of \g on \vu implies that \g is random to the designer. For a
given outcome of \vu, however, the evaluation of \g is purely deterministic.
This operation is typically undertaken by an adequate simulator of the system at
hand, and it is assumed to be doable but computationally expensive.

Our objective is to develop an efficient framework for estimating the
probability distribution of the quantity of interest \g dependent on the
uncertain parameters \vu. The framework is required to be able to handle
nondifferentiable and even discontinuous dependencies between \g and \vu as they
constitute an important class of problems for computer-system design; see
\sref{interpolant-example}.
